[["index.html", "Data Wrangling with R Preface Structure of the book Software information", " Data Wrangling with R Aravind Hebbali 2021-06-02 Preface This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. Structure of the book Chapters ?? and ?? focus on reading data from flat/delimited files and spreadsheets. Chapters ??, ?? and ?? focus on wrangling data using the dplyr package. Chapter 6.2 introduces the pipe operator from the magrittr package. Chapter ?? explores tibble(), an alternative for data.frame(). Chapters ??, ?? and 10 explore ways to handle text, date/time and categorical data. Software information The R session information when compiling this book is shown below: sessionInfo() ## R version 4.1.0 (2021-05-18) ## Platform: x86_64-w64-mingw32/x64 (64-bit) ## Running under: Windows 10 x64 (build 19042) ## ## Matrix products: default ## ## locale: ## [1] LC_COLLATE=English_United States.1252 ## [2] LC_CTYPE=English_United States.1252 ## [3] LC_MONETARY=English_United States.1252 ## [4] LC_NUMERIC=C ## [5] LC_TIME=English_United States.1252 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## loaded via a namespace (and not attached): ## [1] bookdown_0.22 digest_0.6.27 R6_2.5.0 jsonlite_1.7.2 ## [5] magrittr_2.0.1 evaluate_0.14 highr_0.9 stringi_1.6.2 ## [9] rlang_0.4.11 rstudioapi_0.13 jquerylib_0.1.4 bslib_0.2.5.1 ## [13] rmarkdown_2.8 tools_4.1.0 stringr_1.4.0 xfun_0.23 ## [17] yaml_2.2.1 compiler_4.1.0 htmltools_0.5.1.1 knitr_1.33 ## [21] sass_0.4.0 We do not add prompts (&gt; and +) to R source code in this book, and we comment out the text output with two hashes ## by default, as you can see from the R session information above. This is for your convenience when you want to copy and run the code (the text output will be ignored since it is commented out). Package names are in bold text (e.g., rmarkdown), and function names are followed by parentheses (e.g., bookdown::render_book()). The double-colon operator :: means accessing an object from a package. "],["about-the-author.html", "About the Author", " About the Author Aravind Hebbali is the founder of Rsquared Academy. He earned his Masters in Economics from Madras School of Economics. As an active R user, he has authored several R packages such as olsrr rfm descriptr blorr xplorerr In 2015, he founded Rsquared Academy, a free and open source education initiative with focus on data science and analytics. Apart from self paced online courses, Rsquared Academy offers customized learning modules for corporates and universities. You can find him on GitHub. "],["import-data-in-r-basics.html", "Chapter 1 Import Data - Basics 1.1 Introduction 1.2 Delimiters 1.3 Read Data 1.4 Column Names 1.5 Skip Lines 1.6 Maximum Lines 1.7 Column Types 1.8 Select Columns 1.9 Summary", " Chapter 1 Import Data - Basics 1.1 Introduction In this chapter, we will learn to: read data from flat or delimited files handle column names/header skip text/info present before data specify column/variable types read specific columns/variables We will use the following R packages: library(readr) 1.2 Delimiters Before we start reading data from files, let us take a quick look at the different types of delimiters we have to deal with while reading or importing data. In general, it is a good practice to take a quick look at as you will clearly know the delimiter used in the file. 1.2.1 Comma Separated Values 1.2.2 Semi Colon Separated Values 1.2.3 Space Separated Values 1.2.4 Tab Separated Values 1.3 Read Data Let us begin by reading data from a csv file using read_csv(). read_csv(&#39;hsb2.csv&#39;) ## ## -- Column specification -------------------------------------------------------- ## cols( ## id = col_double(), ## female = col_double(), ## race = col_double(), ## ses = col_double(), ## schtyp = col_double(), ## prog = col_double(), ## read = col_double(), ## write = col_double(), ## math = col_double(), ## science = col_double(), ## socst = col_double() ## ) ## # A tibble: 200 x 11 ## id female race ses schtyp prog read write math science socst ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 70 0 4 1 1 1 57 52 41 47 57 ## 2 121 1 4 2 1 3 68 59 53 63 61 ## 3 86 0 4 3 1 1 44 33 54 58 31 ## 4 141 0 4 3 1 3 63 44 47 53 56 ## 5 172 0 4 2 1 2 47 52 57 53 61 ## 6 113 0 4 2 1 2 44 52 51 63 61 ## 7 50 0 3 2 1 1 50 59 42 53 61 ## 8 11 0 1 2 1 2 34 46 45 39 36 ## 9 84 0 4 2 1 1 63 57 54 58 51 ## 10 48 0 3 2 1 2 57 55 52 50 51 ## # ... with 190 more rows Great! If you see the above output, you have successfully read data into R. If you see an error message (which most of us see when we are trying to read data for the first time), follow the below instructions: check the separator in the file and ensure it is a comma check the file name check the file path i.e. location of the file ensure that the file name or path is enclosed in single or double quotes When you read data using readr, it will display the data type detected for each column/variable in the data set. If you want to check the data types before reading the data, use spec_csv(). We will learn to specify the column types in the next section. spec_csv(&#39;hsb2.csv&#39;) ## cols( ## id = col_double(), ## female = col_double(), ## race = col_double(), ## ses = col_double(), ## schtyp = col_double(), ## prog = col_double(), ## read = col_double(), ## write = col_double(), ## math = col_double(), ## science = col_double(), ## socst = col_double() ## ) 1.4 Column Names In some cases, files do not include column names or headers. If we do not indicate the absence of column names, readr will treat the first row from the data as the column name. Like we said before, it is a good practice to take a quick look at the data to check for the presence/absence of column names. We will first read the data set without indicating the presence or absence of column names. read_csv(&#39;hsb3.csv&#39;) ## Warning: Duplicated column names deduplicated: &#39;1&#39; =&gt; &#39;1_1&#39; [5], &#39;1&#39; =&gt; ## &#39;1_2&#39; [6], &#39;57&#39; =&gt; &#39;57_1&#39; [11] ## # A tibble: 199 x 11 ## `70` `0` `4` `1` `1_1` `1_2` `57` `52` `41` `47` `57_1` ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 121 1 4 2 1 3 68 59 53 63 61 ## 2 86 0 4 3 1 1 44 33 54 58 31 ## 3 141 0 4 3 1 3 63 44 47 53 56 ## 4 172 0 4 2 1 2 47 52 57 53 61 ## 5 113 0 4 2 1 2 44 52 51 63 61 ## 6 50 0 3 2 1 1 50 59 42 53 61 ## 7 11 0 1 2 1 2 34 46 45 39 36 ## 8 84 0 4 2 1 1 63 57 54 58 51 ## 9 48 0 3 2 1 2 57 55 52 50 51 ## 10 75 0 4 2 1 3 60 46 51 53 61 ## # ... with 189 more rows As you can see, in the absence of column names, readr has converted the first row of the data into the column names. As a result, the data is not read properly and there are lots of missing values and warnings. If the column names are absent (i.e. the column names are provided in a separate file), use the col_names argument and set it to FALSE. Now readr will not convert the first row of data into column name and instead it will generate new column names. read_csv(&#39;hsb3.csv&#39;, col_names = FALSE) ## # A tibble: 200 x 11 ## X1 X2 X3 X4 X5 X6 X7 X8 X9 X10 X11 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 70 0 4 1 1 1 57 52 41 47 57 ## 2 121 1 4 2 1 3 68 59 53 63 61 ## 3 86 0 4 3 1 1 44 33 54 58 31 ## 4 141 0 4 3 1 3 63 44 47 53 56 ## 5 172 0 4 2 1 2 47 52 57 53 61 ## 6 113 0 4 2 1 2 44 52 51 63 61 ## 7 50 0 3 2 1 1 50 59 42 53 61 ## 8 11 0 1 2 1 2 34 46 45 39 36 ## 9 84 0 4 2 1 1 63 57 54 58 51 ## 10 48 0 3 2 1 2 57 55 52 50 51 ## # ... with 190 more rows We may not always want to use the column names generated by readr and instead specify new column names. In such cases, we can use col_names to supply column names as shown in the below example. Let us reread hsb3 and specify column names. cnames &lt;- c(&quot;id&quot;, &quot;gender&quot;, &quot;race&quot;, &quot;socio_economic_status&quot;, &quot;school_type&quot;, &quot;program&quot;, &quot;read&quot;, &quot;write&quot;, &quot;math&quot;, &quot;science&quot;, &quot;socst&quot;) read_csv(&#39;hsb3.csv&#39;, col_names = cnames) ## # A tibble: 200 x 11 ## id gender race socio_economic_stat~ school_type program read write math ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 70 0 4 1 1 1 57 52 41 ## 2 121 1 4 2 1 3 68 59 53 ## 3 86 0 4 3 1 1 44 33 54 ## 4 141 0 4 3 1 3 63 44 47 ## 5 172 0 4 2 1 2 47 52 57 ## 6 113 0 4 2 1 2 44 52 51 ## 7 50 0 3 2 1 1 50 59 42 ## 8 11 0 1 2 1 2 34 46 45 ## 9 84 0 4 2 1 1 63 57 54 ## 10 48 0 3 2 1 2 57 55 52 ## # ... with 190 more rows, and 2 more variables: science &lt;dbl&gt;, socst &lt;dbl&gt; 1.5 Skip Lines In certain files, you will find information related to the data such as: the data source column names column description copyright etc. The data will appear after/below such text/information. While reading data from such files, we need to skip all the rows where the text is present. If we do not skip them, readr will consider them as part of the data. Let us read the data without skipping any lines/rows and observe the result. read_csv(&#39;hsb4.csv&#39;) ## Warning: 201 parsing failures. ## row col expected actual file ## 3 -- 1 columns 11 columns &#39;hsb4.csv&#39; ## 4 -- 1 columns 11 columns &#39;hsb4.csv&#39; ## 5 -- 1 columns 11 columns &#39;hsb4.csv&#39; ## 6 -- 1 columns 11 columns &#39;hsb4.csv&#39; ## 7 -- 1 columns 11 columns &#39;hsb4.csv&#39; ## ... ... ......... .......... .......... ## See problems(...) for more details. ## # A tibble: 203 x 1 ## `# A dataset containing demographic information and standardized` ## &lt;chr&gt; ## 1 # test scores of high school students. ## 2 # http://www.ats.ucla.edu/stat/spss/whatstat/whatstat.htm ## 3 id ## 4 70 ## 5 121 ## 6 86 ## 7 141 ## 8 172 ## 9 113 ## 10 50 ## # ... with 193 more rows Use skip argument to indicate the number of lines/rows to be skipped while reading data from a file. For example, if the file has contents other than data in the first few lines, we need to skip them before reading the data. In the below example, we will skip the first 3 lines as they contain information about the data set which we do not need. read_csv(&#39;hsb4.csv&#39;, skip = 3) ## # A tibble: 200 x 11 ## id female race ses schtyp prog read write math science socst ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 70 0 4 1 1 1 57 52 41 47 57 ## 2 121 1 4 2 1 3 68 59 53 63 61 ## 3 86 0 4 3 1 1 44 33 54 58 31 ## 4 141 0 4 3 1 3 63 44 47 53 56 ## 5 172 0 4 2 1 2 47 52 57 53 61 ## 6 113 0 4 2 1 2 44 52 51 63 61 ## 7 50 0 3 2 1 1 50 59 42 53 61 ## 8 11 0 1 2 1 2 34 46 45 39 36 ## 9 84 0 4 2 1 1 63 57 54 58 51 ## 10 48 0 3 2 1 2 57 55 52 50 51 ## # ... with 190 more rows 1.6 Maximum Lines Suppose the data file contains several thousands of rows of data and we do not want to read all of it. What can we do in such cases? readr allows us to specify the maximum number of rows to be read using the n_max argument. Suppose we want to read only 100 rows of data from a file, we can set n_max equal to 100. In the next example, we will read the first 120 rows from the hsb2 file. If you observe the last row in the output, it says # ... with 110 more rows, indicating that only 120 rows of data has been read from the file. read_csv(&#39;hsb2.csv&#39;, n_max = 120) ## # A tibble: 120 x 11 ## id female race ses schtyp prog read write math science socst ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 70 0 4 1 1 1 57 52 41 47 57 ## 2 121 1 4 2 1 3 68 59 53 63 61 ## 3 86 0 4 3 1 1 44 33 54 58 31 ## 4 141 0 4 3 1 3 63 44 47 53 56 ## 5 172 0 4 2 1 2 47 52 57 53 61 ## 6 113 0 4 2 1 2 44 52 51 63 61 ## 7 50 0 3 2 1 1 50 59 42 53 61 ## 8 11 0 1 2 1 2 34 46 45 39 36 ## 9 84 0 4 2 1 1 63 57 54 58 51 ## 10 48 0 3 2 1 2 57 55 52 50 51 ## # ... with 110 more rows 1.7 Column Types If you have observed carefully, when you read data using readr, it displays the column names and column types followed by the first 10 rows of data. readr determines the data type for each column based on the first 1000 rows of data. The data can be of the following types: integer double (decimal point) logical (TRUE/FALSE) character (text/string) factor (categorical/qualitative) date/time Before you read data from a file, use spec_csv() to see the data types as determined by readr. If it determines the data types correctly, you can go ahead and read the data else we will have to specify the data types and we will have to do that for all the columns we want to read and not just for those columns whose data type was wrongly determined by readr. To specify the data types, we will use the col_types argument and supply it a list of data types. The data types can be specified using: col_integer() col_double() col_factor() col_logical() col_character() col_date() col_time() col_datetime() While specifying the data types we also need to specify the categories of the categorical/qualitative variable. To do that, we use the levels argument within col_factor(). Let us read data from the hsb2.csv file to understand data type specification. read_csv(&#39;hsb2.csv&#39;, col_types = list( col_integer(), col_factor(levels = c(&quot;0&quot;, &quot;1&quot;)), col_factor(levels = c(&quot;1&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4&quot;)), col_factor(levels = c(&quot;1&quot;, &quot;2&quot;, &quot;3&quot;)), col_factor(levels = c(&quot;1&quot;, &quot;2&quot;)), col_factor(levels = c(&quot;1&quot;, &quot;2&quot;, &quot;3&quot;)), col_integer(), col_integer(), col_integer(), col_integer(), col_integer()) ) ## # A tibble: 200 x 11 ## id female race ses schtyp prog read write math science socst ## &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 70 0 4 1 1 1 57 52 41 47 57 ## 2 121 1 4 2 1 3 68 59 53 63 61 ## 3 86 0 4 3 1 1 44 33 54 58 31 ## 4 141 0 4 3 1 3 63 44 47 53 56 ## 5 172 0 4 2 1 2 47 52 57 53 61 ## 6 113 0 4 2 1 2 44 52 51 63 61 ## 7 50 0 3 2 1 1 50 59 42 53 61 ## 8 11 0 1 2 1 2 34 46 45 39 36 ## 9 84 0 4 2 1 1 63 57 54 58 51 ## 10 48 0 3 2 1 2 57 55 52 50 51 ## # ... with 190 more rows If we do not specify the data type for all columns, readr will return an error which leads to the following questions: What if I want to skip a few columns? What if I want to read certain columns only? 1.8 Select Columns For the first scenario, we can use col_skip() i.e. instead of specifying the data type, we indicate to readr to skip that particular column while reading the data. In case of the second scenario, we will use cols_only() to specify the columns to be read i.e. instead of using list() to supply the data types, we will use cols_only() and provide the following details: column name column type using col_types argument read_csv(&#39;hsb2.csv&#39;, col_types = cols_only(id = col_integer(), prog = col_factor(levels = c(&quot;1&quot;, &quot;2&quot;, &quot;3&quot;)), read = col_integer()) ) ## # A tibble: 200 x 3 ## id prog read ## &lt;int&gt; &lt;fct&gt; &lt;int&gt; ## 1 70 1 57 ## 2 121 3 68 ## 3 86 1 44 ## 4 141 3 63 ## 5 172 2 47 ## 6 113 2 44 ## 7 50 1 50 ## 8 11 2 34 ## 9 84 1 63 ## 10 48 2 57 ## # ... with 190 more rows If you have a data set with 10 columns and plan to skip only a couple of columns, use col_skip() instead if you plan to read only a couple of columns, use cols_only(). 1.9 Summary "],["import-data-in-r-advanced.html", "Chapter 2 Import Data - Advanced 2.1 Introduction 2.2 List Sheets 2.3 Read Sheet 2.4 Read Specific Cells 2.5 Read Specific Rows 2.6 Read Single Column 2.7 Read Multiple Columns 2.8 Statistical Softwares 2.9 Summary", " Chapter 2 Import Data - Advanced 2.1 Introduction In this chapter, we will: list sheets in an excel file read data from an excel sheet read specific cells from an excel sheet read specific rows read specific columns read data from - SAS - SPSS - STATA We will use the following R packages: library(readxl) library(haven) 2.2 List Sheets An excel file may contain several sheets. Let us see how many sheets are present in sample.xls file and their respective names using excel_sheets(). excel_sheets(&#39;sample.xls&#39;) ## [1] &quot;ecom&quot; 2.3 Read Sheet Now that we know the number of sheets and their names, let us read data from the ecom sheet of the sample.xls file using read_excel(). We will specify the file name, and the sheet name or sheet number. 2.3.1 Case 1: Specify the sheet number read_excel(&#39;sample.xls&#39;, sheet = 1) ## # A tibble: 7 x 5 ## channel users new_users sessions bounce_rate ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Organic Search 43296 40238 50810 48.72% ## 2 Direct 12916 12311 16419 49.27% ## 3 Referral 10983 7636 18105 22.26% ## 4 Social 10346 10029 11101 61.92% ## 5 Display 5564 4790 7220 83.30% ## 6 Paid Search 2687 2205 3438 38.02% ## 7 Affiliates 1773 1585 2167 55.75% 2.3.2 Case 2: Specify the sheet name read_excel(&#39;sample.xls&#39;, sheet = &#39;ecom&#39;) ## # A tibble: 7 x 5 ## channel users new_users sessions bounce_rate ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Organic Search 43296 40238 50810 48.72% ## 2 Direct 12916 12311 16419 49.27% ## 3 Referral 10983 7636 18105 22.26% ## 4 Social 10346 10029 11101 61.92% ## 5 Display 5564 4790 7220 83.30% ## 6 Paid Search 2687 2205 3438 38.02% ## 7 Affiliates 1773 1585 2167 55.75% Notice when you use the sheet name, the name should be enclosed in single/double quotes. 2.4 Read Specific Cells You may not always want to read all the columns or rows from the excel sheet. In such cases, you can specify the cells from which the data must be read which can be achieved using the range argument. So how do we specify the cells from which to read data? There are different ways of specifying the cell range and we will look at them one by one: 2.4.1 Method 1 The first method uses the cell names along with : to specify the cell range. For example, to read data from first 4 rows of columns B and C, we will specify the range as \"B1:C4\". read_excel(&#39;sample.xls&#39;, sheet = 1, range = &quot;B1:C4&quot;) ## # A tibble: 3 x 2 ## users new_users ## &lt;dbl&gt; &lt;dbl&gt; ## 1 43296 40238 ## 2 12916 12311 ## 3 10983 7636 To read data from first 5 rows of columns A, B and C, we will specify the range as \"A1:C5\". read_excel(&#39;sample.xls&#39;, sheet = 1, range = &quot;A1:C5&quot;) ## # A tibble: 4 x 3 ## channel users new_users ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Organic Search 43296 40238 ## 2 Direct 12916 12311 ## 3 Referral 10983 7636 ## 4 Social 10346 10029 2.4.2 Method 2 In the second method, we start from a particular cell and specify the number of rows and columns to be covered keeping the initial cell as anchorage. In the below example, we want to read 3 rows and 2 columns starting from the cell A4. read_excel(&#39;sample.xls&#39;, sheet = 1, col_names = FALSE, range = anchored(&quot;A4&quot;, dim = c(3, 2))) ## # A tibble: 3 x 2 ## ...1 ...2 ## &lt;chr&gt; &lt;dbl&gt; ## 1 Referral 10983 ## 2 Social 10346 ## 3 Display 5564 2.4.3 Method 3 In this method, we use the cell_limit() and specify the location of two ends of a rectangle covering the cells we want to read. For example, to read data from the first 6 rows and 4 columns, we will specify the range as following: start from the first row of the first column cover all cells upto the 6th row of the 4th column read_excel(&#39;sample.xls&#39;, sheet = 1, range = cell_limits(c(1, 1), c(6, 4))) ## # A tibble: 5 x 4 ## channel users new_users sessions ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Organic Search 43296 40238 50810 ## 2 Direct 12916 12311 16419 ## 3 Referral 10983 7636 18105 ## 4 Social 10346 10029 11101 ## 5 Display 5564 4790 7220 You can use NA to indicate the first and last row/column. For example, to read data from all the rows from the second column onwards: read_excel(&#39;sample.xls&#39;, sheet = 1, range = cell_limits(c(1, 2), c(NA, NA))) ## # A tibble: 7 x 4 ## users new_users sessions bounce_rate ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 43296 40238 50810 48.72% ## 2 12916 12311 16419 49.27% ## 3 10983 7636 18105 22.26% ## 4 10346 10029 11101 61.92% ## 5 5564 4790 7220 83.30% ## 6 2687 2205 3438 38.02% ## 7 1773 1585 2167 55.75% Let us quickly look at how we will specify range of cells using the above 3 methods when we want to read data from the first 4 rows of columns B and C: 2.4.4 Method 1 read_excel(&#39;sample.xls&#39;, sheet = 1, range = &quot;B1:C4&quot;) ## # A tibble: 3 x 2 ## users new_users ## &lt;dbl&gt; &lt;dbl&gt; ## 1 43296 40238 ## 2 12916 12311 ## 3 10983 7636 2.4.5 Method 2 read_excel(&#39;sample.xls&#39;, sheet = 1, range = anchored(&quot;B1&quot;, dim = c(4, 2))) ## # A tibble: 3 x 2 ## users new_users ## &lt;dbl&gt; &lt;dbl&gt; ## 1 43296 40238 ## 2 12916 12311 ## 3 10983 7636 2.4.6 Method 3 read_excel(&#39;sample.xls&#39;, sheet = 1, range = cell_limits(c(1, 2), c(4, 3))) ## # A tibble: 3 x 2 ## users new_users ## &lt;dbl&gt; &lt;dbl&gt; ## 1 43296 40238 ## 2 12916 12311 ## 3 10983 7636 2.5 Read Specific Rows When you want to read a subset of rows from the data, use cell_rows() and specify the row numbers or the range. In the below example, we want to read the first 4 rows of data from the file. read_excel(&#39;sample.xls&#39;, sheet = 1, range = cell_rows(1:4)) ## # A tibble: 3 x 5 ## channel users new_users sessions bounce_rate ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Organic Search 43296 40238 50810 48.72% ## 2 Direct 12916 12311 16419 49.27% ## 3 Referral 10983 7636 18105 22.26% 2.6 Read Single Column If you want to read a single column from the data, use cell_cols() and specify the column number. In the below example, we read the second column from the sample.xls file. read_excel(&#39;sample.xls&#39;, sheet = 1, range = cell_cols(2)) ## # A tibble: 7 x 1 ## users ## &lt;dbl&gt; ## 1 43296 ## 2 12916 ## 3 10983 ## 4 10346 ## 5 5564 ## 6 2687 ## 7 1773 2.7 Read Multiple Columns In case of multiple columns, we need to specify the column numbers or the column range. In the below example, we want to read the 2nd, 4th and 6th column from the sample.xls file. read_excel(&#39;sample.xls&#39;, sheet = 1, range = cell_cols(c(2, 4, 6))) ## # A tibble: 7 x 5 ## users new_users sessions bounce_rate ...5 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;lgl&gt; ## 1 43296 40238 50810 48.72% NA ## 2 12916 12311 16419 49.27% NA ## 3 10983 7636 18105 22.26% NA ## 4 10346 10029 11101 61.92% NA ## 5 5564 4790 7220 83.30% NA ## 6 2687 2205 3438 38.02% NA ## 7 1773 1585 2167 55.75% NA In the next example, we want to read data from the 2nd column upto and including the 6th column. read_excel(&#39;sample.xls&#39;, sheet = 1, range = cell_cols(c(2:6))) ## # A tibble: 7 x 5 ## users new_users sessions bounce_rate ...5 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;lgl&gt; ## 1 43296 40238 50810 48.72% NA ## 2 12916 12311 16419 49.27% NA ## 3 10983 7636 18105 22.26% NA ## 4 10346 10029 11101 61.92% NA ## 5 5564 4790 7220 83.30% NA ## 6 2687 2205 3438 38.02% NA ## 7 1773 1585 2167 55.75% NA 2.7.1 Summary 2.8 Statistical Softwares We will use the haven package to read data from files of other statistical softwares such as: SAS SPSS STATA 2.8.1 STATA read_stata(&#39;airline.dta&#39;) ## # A tibble: 32 x 6 ## year y w r l k ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1948 1.21 0.243 0.145 1.41 0.612 ## 2 1949 1.35 0.260 0.218 1.38 0.559 ## 3 1950 1.57 0.278 0.316 1.39 0.573 ## 4 1951 1.95 0.297 0.394 1.55 0.564 ## 5 1952 2.27 0.310 0.356 1.80 0.574 ## 6 1953 2.73 0.322 0.359 1.93 0.711 ## 7 1954 3.03 0.335 0.403 1.96 0.776 ## 8 1955 3.56 0.350 0.396 2.12 0.827 ## 9 1956 3.98 0.361 0.382 2.43 0.800 ## 10 1957 4.42 0.379 0.305 2.71 0.921 ## # ... with 22 more rows 2.8.2 SPSS read_spss(&#39;employee.sav&#39;) ## # A tibble: 474 x 9 ## id gender educ jobcat salary salbegin jobtime prevexp minority ## &lt;dbl&gt; &lt;chr+lb&gt; &lt;dbl+lb&gt; &lt;dbl+lbl&gt; &lt;dbl+l&gt; &lt;dbl+lb&gt; &lt;dbl+l&gt; &lt;dbl+lbl&gt; &lt;dbl+lb&gt; ## 1 1 m [Male] 15 [15] 3 [Manag~ 57000 27000 98 144 0 [No] ## 2 2 m [Male] 16 [16] 1 [Cleri~ 40200 18750 98 36 0 [No] ## 3 3 f [Fema~ 12 [12] 1 [Cleri~ 21450 12000 98 381 0 [No] ## 4 4 f [Fema~ 8 [8] 1 [Cleri~ 21900 13200 98 190 0 [No] ## 5 5 m [Male] 15 [15] 1 [Cleri~ 45000 21000 98 138 0 [No] ## 6 6 m [Male] 15 [15] 1 [Cleri~ 32100 13500 98 67 0 [No] ## 7 7 m [Male] 15 [15] 1 [Cleri~ 36000 18750 98 114 0 [No] ## 8 8 f [Fema~ 12 [12] 1 [Cleri~ 21900 9750 98 0 [mis~ 0 [No] ## 9 9 f [Fema~ 15 [15] 1 [Cleri~ 27900 12750 98 115 0 [No] ## 10 10 f [Fema~ 12 [12] 1 [Cleri~ 24000 13500 98 244 0 [No] ## # ... with 464 more rows 2.8.3 SAS read_sas(&#39;airline.sas7bdat&#39;) ## # A tibble: 32 x 6 ## YEAR Y W R L K ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1948 1.21 0.243 0.145 1.41 0.612 ## 2 1949 1.35 0.260 0.218 1.38 0.559 ## 3 1950 1.57 0.278 0.316 1.39 0.573 ## 4 1951 1.95 0.297 0.394 1.55 0.564 ## 5 1952 2.27 0.310 0.356 1.80 0.574 ## 6 1953 2.73 0.322 0.359 1.93 0.711 ## 7 1954 3.03 0.335 0.403 1.96 0.776 ## 8 1955 3.56 0.350 0.396 2.12 0.827 ## 9 1956 3.98 0.361 0.382 2.43 0.800 ## 10 1957 4.42 0.379 0.305 2.71 0.921 ## # ... with 22 more rows 2.9 Summary "],["dplyr-basics.html", "Chapter 3 dplyr Basics 3.1 Introduction 3.2 dplyr Verbs 3.3 Data 3.4 Case Study 3.5 Average Order Value 3.6 AOV by Devices 3.7 Syntax 3.8 Filter Rows 3.9 Select Columns 3.10 Grouping Data 3.11 Summarise Data 3.12 Create Columns 3.13 Arrange Data 3.14 AOV by Devices 3.15 Your Turn", " Chapter 3 dplyr Basics 3.1 Introduction According to a survey by CrowdFlower, data scientists spend most of their time cleaning and manipulating data rather than mining or modeling them for insights. As such, it becomes important to have tools that make data manipulation faster and easier. In todays chapter, we introduce you to dplyr, a grammar of data manipulation. We will use the following R packages: library(dplyr) library(readr) 3.2 dplyr Verbs dplyr provides a set of verbs that help us solve the most common data manipulation challenges while working with tabular data (dataframes, tibbles): select filter arrange mutate summarise 3.3 Data ecom &lt;- read_csv(&#39;https://raw.githubusercontent.com/rsquaredacademy/datasets/master/web.csv&#39;, col_types = cols_only(device = col_factor(levels = c(&quot;laptop&quot;, &quot;tablet&quot;, &quot;mobile&quot;)), referrer = col_factor(levels = c(&quot;bing&quot;, &quot;direct&quot;, &quot;social&quot;, &quot;yahoo&quot;, &quot;google&quot;)), purchase = col_logical(), n_pages = col_double(), n_visit = col_double(), duration = col_double(), order_value = col_double(), order_items = col_double() ) ) ecom ## # A tibble: 1,000 x 8 ## referrer device n_visit n_pages duration purchase order_items order_value ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 google laptop 10 1 693 FALSE 0 0 ## 2 yahoo tablet 9 1 459 FALSE 0 0 ## 3 direct laptop 0 1 996 FALSE 0 0 ## 4 bing tablet 3 18 468 TRUE 6 434 ## 5 yahoo mobile 9 1 955 FALSE 0 0 ## 6 yahoo laptop 5 5 135 FALSE 0 0 ## 7 yahoo mobile 10 1 75 FALSE 0 0 ## 8 direct mobile 10 1 908 FALSE 0 0 ## 9 bing mobile 3 19 209 FALSE 0 0 ## 10 google mobile 6 1 208 FALSE 0 0 ## # ... with 990 more rows 3.3.1 Data Dictionary Below is the description of the data set: referrer: referrer website/search engine device: device used to visit the website n_pages: number of pages visited duration: time spent on the website (in seconds) purchase: whether visitor purchased order_value: order value of visitor (in dollars) n_visit: number of visits 3.4 Case Study We will use dplyr to answer the following: what is the average order value by device types? what is the average number of pages visited by purchasers and non-purchasers? what is the average time on site for purchasers vs non-purchasers? what is the average number of pages visited by purchasers and non-purchasers using mobile? 3.5 Average Order Value 3.6 AOV by Devices ecom %&gt;% filter(purchase) %&gt;% select(device, order_value) %&gt;% group_by(device) %&gt;% summarise_all(funs(revenue = sum, orders = n())) %&gt;% mutate( aov = revenue / orders ) %&gt;% select(device, aov) %&gt;% arrange(aov) ## Warning: `funs()` was deprecated in dplyr 0.8.0. ## Please use a list of either functions or lambdas: ## ## # Simple named list: ## list(mean = mean, median = median) ## ## # Auto named with `tibble::lst()`: ## tibble::lst(mean, median) ## ## # Using lambdas ## list(~ mean(., trim = .2), ~ median(., na.rm = TRUE)) ## # A tibble: 3 x 2 ## device aov ## &lt;fct&gt; &lt;dbl&gt; ## 1 tablet 1426. ## 2 mobile 1431. ## 3 laptop 1824. 3.7 Syntax Before we start exploring the dplyr verbs, let us look at their syntax: the first argument is always a data.frame or tibble the subsequent arguments provide the information required for the verbs to take action the name of columns in the data need not be surrounded by quotes 3.8 Filter Rows In order to compute the AOV, we must first separate the purchasers from non-purchasers. We will do this by filtering the data related to purchasers using the filter() function. It allows us to filter rows that meet a specific criteria/condition. The first argument is the name of the data frame and the rest of the arguments are expressions for filtering the data. Let us look at a few examples: The first example we will look at filters all visits from device mobile. As we had learnt in the previous section, the first argument is our data set ecom and the next argument is the condition for filtering rows. filter(ecom, device == &quot;mobile&quot;) ## # A tibble: 344 x 8 ## referrer device n_visit n_pages duration purchase order_items order_value ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 yahoo mobile 9 1 955 FALSE 0 0 ## 2 yahoo mobile 10 1 75 FALSE 0 0 ## 3 direct mobile 10 1 908 FALSE 0 0 ## 4 bing mobile 3 19 209 FALSE 0 0 ## 5 google mobile 6 1 208 FALSE 0 0 ## 6 direct mobile 9 14 406 TRUE 3 651 ## 7 yahoo mobile 7 1 19 FALSE 7 2423 ## 8 google mobile 5 1 147 FALSE 0 0 ## 9 bing mobile 0 7 196 FALSE 4 237 ## 10 google mobile 10 1 338 FALSE 0 0 ## # ... with 334 more rows We can specify multiple filtering conditions as well. In the below example, we specify two filter conditions: visit from device mobile resulted in a purchase or conversion filter(ecom, device == &quot;mobile&quot;, purchase) ## # A tibble: 36 x 8 ## referrer device n_visit n_pages duration purchase order_items order_value ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 direct mobile 9 14 406 TRUE 3 651 ## 2 bing mobile 4 20 440 TRUE 3 184 ## 3 bing mobile 3 18 288 TRUE 6 764 ## 4 social mobile 10 11 242 TRUE 4 287 ## 5 yahoo mobile 6 14 322 TRUE 3 1443 ## 6 google mobile 1 18 252 TRUE 3 2449 ## 7 social mobile 7 16 352 TRUE 10 2824 ## 8 direct mobile 4 18 324 TRUE 3 1670 ## 9 social mobile 1 20 520 TRUE 5 1021 ## 10 yahoo mobile 0 13 351 TRUE 10 288 ## # ... with 26 more rows Here is another example where we specify multiple conditions: visit from device tablet made a purchase browsed less than 15 pages filter(ecom, device == &quot;tablet&quot;, purchase, n_pages &lt; 15) ## # A tibble: 12 x 8 ## referrer device n_visit n_pages duration purchase order_items order_value ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 social tablet 7 10 290 TRUE 9 1304 ## 2 yahoo tablet 2 14 364 TRUE 6 1667 ## 3 google tablet 7 12 324 TRUE 2 1358 ## 4 direct tablet 3 12 324 TRUE 10 1257 ## 5 yahoo tablet 0 13 390 TRUE 5 1748 ## 6 social tablet 2 12 300 TRUE 2 2754 ## 7 direct tablet 6 13 338 TRUE 5 683 ## 8 yahoo tablet 2 10 280 TRUE 4 293 ## 9 social tablet 10 10 290 TRUE 9 37 ## 10 direct tablet 3 10 260 TRUE 7 980 ## 11 google tablet 9 14 308 TRUE 7 2436 ## 12 social tablet 10 11 330 TRUE 1 2171 3.8.1 Case Study Let us apply what we have learnt to the case study. We want to filter all visits that resulted in a purchase. filter(ecom, purchase) ## # A tibble: 103 x 8 ## referrer device n_visit n_pages duration purchase order_items order_value ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 bing tablet 3 18 468 TRUE 6 434 ## 2 direct mobile 9 14 406 TRUE 3 651 ## 3 bing tablet 5 16 368 TRUE 6 1049 ## 4 social tablet 7 10 290 TRUE 9 1304 ## 5 direct tablet 2 19 342 TRUE 5 622 ## 6 social tablet 9 20 420 TRUE 7 1613 ## 7 bing mobile 4 20 440 TRUE 3 184 ## 8 yahoo tablet 2 16 480 TRUE 9 286 ## 9 bing mobile 3 18 288 TRUE 6 764 ## 10 yahoo tablet 2 14 364 TRUE 6 1667 ## # ... with 93 more rows 3.9 Select Columns After filtering the data, we need to select relevent variables to compute the AOV. Remember, we do not need all the columns in the data to compute a required metric (in our case, AOV). The select() function allows us to select a subset of columns. The first argument is the name of the data frame and the subsequent arguments specify the columns by name or position. To select the device and duration columns, we specify the data set i.e.  ecom followed by the name of the columns. select(ecom, device, duration) ## # A tibble: 1,000 x 2 ## device duration ## &lt;fct&gt; &lt;dbl&gt; ## 1 laptop 693 ## 2 tablet 459 ## 3 laptop 996 ## 4 tablet 468 ## 5 mobile 955 ## 6 laptop 135 ## 7 mobile 75 ## 8 mobile 908 ## 9 mobile 209 ## 10 mobile 208 ## # ... with 990 more rows We can select a set of columns using :. In the below example, we select all the columns starting from referrer up to order_items. Remember that we can use : only when the columns are adjacent to each other in the data set. select(ecom, referrer:order_items) ## # A tibble: 1,000 x 7 ## referrer device n_visit n_pages duration purchase order_items ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;dbl&gt; ## 1 google laptop 10 1 693 FALSE 0 ## 2 yahoo tablet 9 1 459 FALSE 0 ## 3 direct laptop 0 1 996 FALSE 0 ## 4 bing tablet 3 18 468 TRUE 6 ## 5 yahoo mobile 9 1 955 FALSE 0 ## 6 yahoo laptop 5 5 135 FALSE 0 ## 7 yahoo mobile 10 1 75 FALSE 0 ## 8 direct mobile 10 1 908 FALSE 0 ## 9 bing mobile 3 19 209 FALSE 0 ## 10 google mobile 6 1 208 FALSE 0 ## # ... with 990 more rows What if you want to select all columns except a few? Typing the name of many columns can be cumbersome and may also result in spelling errors. We may use : only if the columns are adjacent to each other but that may not always be the case. dplyr allows us to specify columns that need not be selected using -. In the below example, we select all columns except n_pages and duration. Notice the - before both of them. select(ecom, -n_pages, -duration) ## # A tibble: 1,000 x 6 ## referrer device n_visit purchase order_items order_value ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 google laptop 10 FALSE 0 0 ## 2 yahoo tablet 9 FALSE 0 0 ## 3 direct laptop 0 FALSE 0 0 ## 4 bing tablet 3 TRUE 6 434 ## 5 yahoo mobile 9 FALSE 0 0 ## 6 yahoo laptop 5 FALSE 0 0 ## 7 yahoo mobile 10 FALSE 0 0 ## 8 direct mobile 10 FALSE 0 0 ## 9 bing mobile 3 FALSE 0 0 ## 10 google mobile 6 FALSE 0 0 ## # ... with 990 more rows 3.9.1 Case Study For our case study, we need to select the column order_value to calculate the AOV. We also need to select the device column as we are computing the AOV for each device type. select(ecom, device, order_value) ## # A tibble: 1,000 x 2 ## device order_value ## &lt;fct&gt; &lt;dbl&gt; ## 1 laptop 0 ## 2 tablet 0 ## 3 laptop 0 ## 4 tablet 434 ## 5 mobile 0 ## 6 laptop 0 ## 7 mobile 0 ## 8 mobile 0 ## 9 mobile 0 ## 10 mobile 0 ## # ... with 990 more rows But we want the above data only for purchasers. Let us combine filter() and select() functions to extract order_value and order_items only for those visis that resulted in a purchase. # filter all visits that resulted in a purchase ecom1 &lt;- filter(ecom, purchase) # select the relevant columns ecom2 &lt;- select(ecom1, device, order_value) # view data ecom2 ## # A tibble: 103 x 2 ## device order_value ## &lt;fct&gt; &lt;dbl&gt; ## 1 tablet 434 ## 2 mobile 651 ## 3 tablet 1049 ## 4 tablet 1304 ## 5 tablet 622 ## 6 tablet 1613 ## 7 mobile 184 ## 8 tablet 286 ## 9 mobile 764 ## 10 tablet 1667 ## # ... with 93 more rows 3.10 Grouping Data We need to compute the total order value and total order items for each device in order to compute their AOV. To achieve this, we need to group the selected order_value and order_items by device type. group_by() allows us to group or split data based on particular (discrete) variable. The first argument is the name of the data set and the second argument is the name of the column based on which the data will be split. To split the data by referrer type, we use group_by and specify the data set i.e. ecom and the column based on which to split the data i.e. referrer. group_by(ecom, referrer) ## # A tibble: 1,000 x 8 ## # Groups: referrer [5] ## referrer device n_visit n_pages duration purchase order_items order_value ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 google laptop 10 1 693 FALSE 0 0 ## 2 yahoo tablet 9 1 459 FALSE 0 0 ## 3 direct laptop 0 1 996 FALSE 0 0 ## 4 bing tablet 3 18 468 TRUE 6 434 ## 5 yahoo mobile 9 1 955 FALSE 0 0 ## 6 yahoo laptop 5 5 135 FALSE 0 0 ## 7 yahoo mobile 10 1 75 FALSE 0 0 ## 8 direct mobile 10 1 908 FALSE 0 0 ## 9 bing mobile 3 19 209 FALSE 0 0 ## 10 google mobile 6 1 208 FALSE 0 0 ## # ... with 990 more rows 3.10.1 Case Study In the second line in the previous output, you can observe Groups: referrer [5] . The data is split into 5 groups as the referrer variable has 5 distinct values. For our case study, we need to group the data by device type. # split ecom2 by device type ecom3 &lt;- group_by(ecom2, device) ecom3 ## # A tibble: 103 x 2 ## # Groups: device [3] ## device order_value ## &lt;fct&gt; &lt;dbl&gt; ## 1 tablet 434 ## 2 mobile 651 ## 3 tablet 1049 ## 4 tablet 1304 ## 5 tablet 622 ## 6 tablet 1613 ## 7 mobile 184 ## 8 tablet 286 ## 9 mobile 764 ## 10 tablet 1667 ## # ... with 93 more rows 3.11 Summarise Data The next step is to compute the total order value and total order items for each device. i.e. we need to reduce the order value and order items data to a single summary. We can achieve this using summarise(). As usual, the first argument is the name of a data set and the subsequent arguments are functions that can summarise data. For example, we can use min, max, sum, mean etc. Let us compute the average number of pages browsed by referrer type: split data by referrer type compute the average number of pages using mean # split data by referrer type step_1 &lt;- group_by(ecom, referrer) # compute average number of pages step_2 &lt;- summarise(step_1, mean(n_pages)) ## `summarise()` ungrouping output (override with `.groups` argument) step_2 ## # A tibble: 5 x 2 ## referrer `mean(n_pages)` ## &lt;fct&gt; &lt;dbl&gt; ## 1 bing 6.13 ## 2 direct 6.38 ## 3 social 5.42 ## 4 yahoo 5.99 ## 5 google 5.73 Now let us compute both the mean and the median. # split data by referrer type step_1 &lt;- group_by(ecom, referrer) # compute average number of pages step_2 &lt;- summarise(step_1, mean(n_pages), median(n_pages)) ## `summarise()` ungrouping output (override with `.groups` argument) step_2 ## # A tibble: 5 x 3 ## referrer `mean(n_pages)` `median(n_pages)` ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 bing 6.13 1 ## 2 direct 6.38 1 ## 3 social 5.42 1 ## 4 yahoo 5.99 2 ## 5 google 5.73 1 Another way to achieve the above result is to use the summarise_all() function. How does that work? It generates the specified summary for all the columns in the data set except for the column based on which the data has been grouped or split. So we need to ensure that the data does not have any irrelevant columns. split data by referrer type select order_value and order_items compute the average number of pages by applying the mean function to all the columns # select relevant columns step_1 &lt;- select(ecom, referrer, order_value) # split data by referrer type step_2 &lt;- group_by(step_1, referrer) # compute average number of pages step_3 &lt;- summarise_all(step_2, funs(mean)) step_3 ## # A tibble: 5 x 2 ## referrer order_value ## &lt;fct&gt; &lt;dbl&gt; ## 1 bing 316. ## 2 direct 441. ## 3 social 380. ## 4 yahoo 470. ## 5 google 328. Let us compute mean and median number of pages for each referre type using summarise_all. # select relevant columns step_1 &lt;- select(ecom, referrer, order_value) # split data by referrer type step_2 &lt;- group_by(step_1, referrer) # compute mean and median number of pages step_3 &lt;- summarise_all(step_2, funs(mean, median)) step_3 ## # A tibble: 5 x 3 ## referrer mean median ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 bing 316. 0 ## 2 direct 441. 0 ## 3 social 380. 0 ## 4 yahoo 470. 0 ## 5 google 328. 0 3.11.1 Case Study So far, we have split the data based on the device type and we have selected 2 columns, order_value and order_items. We need the sum of order value and order items. What function can we use to obtain them? The sum() function will generate the sum of the values and hence we will use it inside the summarise() function. Remember, we need to provide a name to the summary being generated. ecom4 &lt;- summarise(ecom3, revenue = sum(order_value), orders = n()) ## `summarise()` ungrouping output (override with `.groups` argument) ecom4 ## # A tibble: 3 x 3 ## device revenue orders ## &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; ## 1 laptop 56531 31 ## 2 tablet 51321 36 ## 3 mobile 51504 36 There you go, we have the total order value and total order items for each device type. If we use summarise_all(), it will generate the summary for the selected columns based on the function specified. To specify the functions, we need to use another argument funs and it can take any number of valid functions. ecom4 &lt;- summarise_all(ecom3, funs(revenue = sum, orders = n())) ecom4 ## # A tibble: 3 x 3 ## device revenue orders ## &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; ## 1 laptop 56531 31 ## 2 tablet 51321 36 ## 3 mobile 51504 36 3.12 Create Columns To create a new column, we will use mutate(). The first argument is the name of the data set and the subsequent arguments are expressions for creating new columns based out of existing columns. Let us add a new column avg_page_time i.e. time on site divided by number of pages visited. # select duration and n_pages from ecom mutate_1 &lt;- select(ecom, n_pages, duration) mutate(mutate_1, avg_page_time = duration / n_pages) ## # A tibble: 1,000 x 3 ## n_pages duration avg_page_time ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 693 693 ## 2 1 459 459 ## 3 1 996 996 ## 4 18 468 26 ## 5 1 955 955 ## 6 5 135 27 ## 7 1 75 75 ## 8 1 908 908 ## 9 19 209 11 ## 10 1 208 208 ## # ... with 990 more rows We can create new columns based on other columns created using mutate. Let us create another column sqrt_avg_page_time i.e. square root of the average time on page using avg_page_time. mutate(mutate_1, avg_page_time = duration / n_pages, sqrt_avg_page_time = sqrt(avg_page_time)) ## # A tibble: 1,000 x 4 ## n_pages duration avg_page_time sqrt_avg_page_time ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 693 693 26.3 ## 2 1 459 459 21.4 ## 3 1 996 996 31.6 ## 4 18 468 26 5.10 ## 5 1 955 955 30.9 ## 6 5 135 27 5.20 ## 7 1 75 75 8.66 ## 8 1 908 908 30.1 ## 9 19 209 11 3.32 ## 10 1 208 208 14.4 ## # ... with 990 more rows 3.12.1 Case Study Back to our case study, from the last step we have the total order value and total order items for each device category and can compute the AOV. We will create a new column to store AOV. ecom5 &lt;- mutate(ecom4, aov = revenue / orders) ecom5 ## # A tibble: 3 x 4 ## device revenue orders aov ## &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 laptop 56531 31 1824. ## 2 tablet 51321 36 1426. ## 3 mobile 51504 36 1431. The last step is to select the relevant columns. We will select the device type and the corresponding aov while getting rid of other columns. Use select() to extract the relevant columns. ecom6 &lt;- select(ecom5, device, aov) ecom6 ## # A tibble: 3 x 2 ## device aov ## &lt;fct&gt; &lt;dbl&gt; ## 1 laptop 1824. ## 2 tablet 1426. ## 3 mobile 1431. 3.13 Arrange Data Arranging data in ascending or descending order is one of the most common tasks in data manipulation. We can use arrange to arrange data by different columns. Let us say we want to arrange data by the number of pages browsed. arrange(ecom, n_pages) ## # A tibble: 1,000 x 8 ## referrer device n_visit n_pages duration purchase order_items order_value ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 google laptop 10 1 693 FALSE 0 0 ## 2 yahoo tablet 9 1 459 FALSE 0 0 ## 3 direct laptop 0 1 996 FALSE 0 0 ## 4 yahoo mobile 9 1 955 FALSE 0 0 ## 5 yahoo mobile 10 1 75 FALSE 0 0 ## 6 direct mobile 10 1 908 FALSE 0 0 ## 7 google mobile 6 1 208 FALSE 0 0 ## 8 direct laptop 9 1 738 FALSE 0 0 ## 9 yahoo mobile 7 1 19 FALSE 7 2423 ## 10 bing laptop 1 1 995 FALSE 0 0 ## # ... with 990 more rows If we want to arrange the data in descending order, we can use desc(). Let us arrange the data in descending order. arrange(ecom , desc(n_pages)) ## # A tibble: 1,000 x 8 ## referrer device n_visit n_pages duration purchase order_items order_value ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 social tablet 9 20 420 TRUE 7 1613 ## 2 bing mobile 4 20 440 TRUE 3 184 ## 3 yahoo tablet 0 20 200 FALSE 0 0 ## 4 direct tablet 6 20 580 TRUE 5 1155 ## 5 social mobile 1 20 520 TRUE 5 1021 ## 6 google mobile 8 20 300 TRUE 7 2091 ## 7 social laptop 4 20 200 FALSE 0 0 ## 8 yahoo mobile 3 20 480 FALSE 0 0 ## 9 social laptop 10 20 280 TRUE 1 2011 ## 10 yahoo mobile 2 20 240 FALSE 0 0 ## # ... with 990 more rows Data can be arranged by multiple variables as well. Let us arrange data first by number of visits and then by number of pages in a descending order. arrange(ecom, n_visit, desc(n_pages)) ## # A tibble: 1,000 x 8 ## referrer device n_visit n_pages duration purchase order_items order_value ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 yahoo tablet 0 20 200 FALSE 0 0 ## 2 google laptop 0 19 418 TRUE 2 996 ## 3 bing laptop 0 18 180 FALSE 0 0 ## 4 yahoo laptop 0 18 522 TRUE 8 1523 ## 5 direct tablet 0 18 252 FALSE 0 0 ## 6 social laptop 0 17 204 FALSE 0 0 ## 7 bing laptop 0 17 272 TRUE 9 1384 ## 8 bing mobile 0 16 272 FALSE 0 0 ## 9 yahoo mobile 0 15 255 FALSE 0 0 ## 10 direct laptop 0 15 255 FALSE 0 0 ## # ... with 990 more rows 3.13.1 Case Study If you observe ecom6, the aov column is arranged in descending order. arrange(ecom6, aov) ## # A tibble: 3 x 2 ## device aov ## &lt;fct&gt; &lt;dbl&gt; ## 1 tablet 1426. ## 2 mobile 1431. ## 3 laptop 1824. 3.14 AOV by Devices Let us combine all the code from the above steps: ecom1 &lt;- filter(ecom, purchase) ecom2 &lt;- select(ecom1, device, order_value) ecom3 &lt;- group_by(ecom2, device) ecom4 &lt;- summarise_all(ecom3, funs(revenue = sum, orders = n())) ecom5 &lt;- mutate(ecom4, aov = revenue / orders) ecom6 &lt;- select(ecom5, device, aov) ecom7 &lt;- arrange(ecom6, aov) ecom7 ## # A tibble: 3 x 2 ## device aov ## &lt;fct&gt; &lt;dbl&gt; ## 1 tablet 1426. ## 2 mobile 1431. ## 3 laptop 1824. If you observe, at each step we create a new variable(data frame) and then use it as an input in the next step i.e. the output from one step becomes the input for the next. Can we achieve the final outcome i.e. ecom7 without creating the intermediate data (ecom1 - ecom6)? Yes, we can. We will use the %&gt;% operator to chain the steps and get rid of the intermediate data. ecom %&gt;% filter(purchase) %&gt;% select(device, order_value) %&gt;% group_by(device) %&gt;% summarise_all(funs(revenue = sum, orders = n())) %&gt;% mutate( aov = revenue / orders ) %&gt;% select(device, aov) %&gt;% arrange(aov) ## # A tibble: 3 x 2 ## device aov ## &lt;fct&gt; &lt;dbl&gt; ## 1 tablet 1426. ## 2 mobile 1431. ## 3 laptop 1824. Below we map the description of each step to dplyr verbs. 3.15 Your Turn what is the average number of pages visited by purchasers and non-purchasers? what is the average time on site for purchasers vs non-purchasers? what is the average number of pages visited by purchasers and non-purchasers using mobile? "],["joining-tables-in-r-dplyr.html", "Chapter 4 Joining Tables using dplyr 4.1 Introduction 4.2 Case Study 4.3 Example Data 4.4 Inner Join 4.5 Left Join 4.6 Case Study: Details of customers and their orders irrespective of whether a customer has 4.7 Right Join 4.8 Semi Join 4.9 Anti Join 4.10 Full Join", " Chapter 4 Joining Tables using dplyr 4.1 Introduction In this chapter, we will learn to combine tables using different *_join functions provided in dplyr. We will use the following R packages: library(dplyr) library(readr) options(tibble.width = Inf) 4.2 Case Study For our case study, we will use two data sets. The first one, order, contains details of orders placed by different customers. The second data set, customer contains details of each customer. The below table displays the details stored in each data set. Let us import both the data sets using read_csv. 4.2.1 Data: Orders order &lt;- read_delim(&#39;https://raw.githubusercontent.com/rsquaredacademy/datasets/master/order.csv&#39;, delim = &#39;;&#39;) order ## # A tibble: 300 x 3 ## id order_date amount ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 368 7/2/2016 365. ## 2 286 11/2/2016 2064. ## 3 28 2/22/2017 432. ## 4 309 3/5/2017 480. ## 5 2 12/28/2016 235. ## 6 31 12/30/2016 2745. ## 7 179 12/21/2016 2358. ## 8 484 11/24/2016 1031. ## 9 115 9/9/2016 1218. ## 10 340 5/6/2017 1184. ## # ... with 290 more rows 4.2.2 Data: Customers customer &lt;- read_delim(&#39;https://raw.githubusercontent.com/rsquaredacademy/datasets/master/customer.csv&#39;, delim = &#39;;&#39;) customer ## # A tibble: 91 x 3 ## id first_name city ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 Elbertine California ## 2 2 Marcella Colorado ## 3 3 Daria Florida ## 4 4 Sherilyn Distric... ## 5 5 Ketty Texas ## 6 6 Jethro California ## 7 7 Jeremiah California ## 8 8 Constancia Texas ## 9 9 Muire Idaho ## 10 10 Abigail Texas ## # ... with 81 more rows We will explore the following in the case study: details of customers who have placed orders and their order details details of customers and their orders irrespective of whether a customer has placed orders or not customer details for each order details of customers who have placed orders details of customers who have not placed orders details of all customers and all orders 4.3 Example Data We will use another data set to illustrate how the different joins work. You can view the example data sets below. 4.4 Inner Join Inner join return all rows from Age where there are matching values in Height, and all columns from Age and Height. If there are multiple matches between Age and Height, all combination of the matches are returned. 4.4.1 Case Study: Details of customers who have placed orders and their order details To get data for all those customers who have placed orders in the past let us join the order data with the customer data using inner_join. inner_join(customer, order, by = &quot;id&quot;) ## # A tibble: 55 x 5 ## id first_name city order_date amount ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 2 Marcella Colorado 12/28/2016 235. ## 2 2 Marcella Colorado 8/31/2016 1150. ## 3 5 Ketty Texas 1/17/2017 346. ## 4 6 Jethro California 1/27/2017 2317. ## 5 7 Jeremiah California 6/21/2016 136. ## 6 7 Jeremiah California 2/13/2017 1407. ## 7 7 Jeremiah California 7/8/2016 1914. ## 8 8 Constancia Texas 11/5/2016 2461. ## 9 8 Constancia Texas 5/19/2017 2714. ## 10 9 Muire Idaho 12/28/2016 187. ## # ... with 45 more rows 4.5 Left Join Left join return all rows from Age, and all columns from Age and Height. Rows in Age with no match in Height will have NA values in the new columns. If there are multiple matches between Age and Height, all combinations of the matches are returned. 4.6 Case Study: Details of customers and their orders irrespective of whether a customer has placed orders or not. To get data for all those customers and their orders irrespective of whether a customer has placed orders or not let us join the order data with the customer data using left_join. left_join(customer, order, by = &quot;id&quot;) ## # A tibble: 104 x 5 ## id first_name city order_date amount ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 Elbertine California &lt;NA&gt; NA ## 2 2 Marcella Colorado 12/28/2016 235. ## 3 2 Marcella Colorado 8/31/2016 1150. ## 4 3 Daria Florida &lt;NA&gt; NA ## 5 4 Sherilyn Distric... &lt;NA&gt; NA ## 6 5 Ketty Texas 1/17/2017 346. ## 7 6 Jethro California 1/27/2017 2317. ## 8 7 Jeremiah California 6/21/2016 136. ## 9 7 Jeremiah California 2/13/2017 1407. ## 10 7 Jeremiah California 7/8/2016 1914. ## # ... with 94 more rows 4.7 Right Join Right join return all rows from Height, and all columns from Age and Height. Rows in Height with no match in Age will have NA values in the new columns. If there are multiple matches between Age and Height, all combinations of the matches are returned. 4.7.1 Case Study: Customer details for each order To get customer data for all orders, let us join the order data with the customer data using right_join. right_join(customer, order, by = &quot;id&quot;) ## # A tibble: 300 x 5 ## id first_name city order_date amount ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 2 Marcella Colorado 12/28/2016 235. ## 2 2 Marcella Colorado 8/31/2016 1150. ## 3 5 Ketty Texas 1/17/2017 346. ## 4 6 Jethro California 1/27/2017 2317. ## 5 7 Jeremiah California 6/21/2016 136. ## 6 7 Jeremiah California 2/13/2017 1407. ## 7 7 Jeremiah California 7/8/2016 1914. ## 8 8 Constancia Texas 11/5/2016 2461. ## 9 8 Constancia Texas 5/19/2017 2714. ## 10 9 Muire Idaho 12/28/2016 187. ## # ... with 290 more rows 4.8 Semi Join Semi join return all rows from Age where there are matching values in Height, keeping just columns from Age. A semi join differs from an inner join because an inner join will return one row of Age for each matching row of Height, where a semi join will never duplicate rows of Age. 4.8.1 Case Study: Details of customers who have placed orders To get customer data for all orders where customer data exists, let us join the order data with the customer data using semi_join. You can observe that data is returned only for those cases where customer data is present. semi_join(customer, order, by = &quot;id&quot;) ## # A tibble: 42 x 3 ## id first_name city ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 2 Marcella Colorado ## 2 5 Ketty Texas ## 3 6 Jethro California ## 4 7 Jeremiah California ## 5 8 Constancia Texas ## 6 9 Muire Idaho ## 7 15 Valentijn California ## 8 16 Monique Missouri ## 9 20 Colette Texas ## 10 28 Avrit Texas ## # ... with 32 more rows 4.9 Anti Join Anti join return all rows from Age where there are not matching values in Height, keeping just columns from Age. 4.9.1 Case Study: Details of customers who have not placed orders To get details of customers who have not placed orders, let us join the order data with the customer data using anti_join. anti_join(customer, order, by = &quot;id&quot;) ## # A tibble: 49 x 3 ## id first_name city ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 Elbertine California ## 2 3 Daria Florida ## 3 4 Sherilyn Distric... ## 4 10 Abigail Texas ## 5 11 Wynne Georgia ## 6 12 Pietra Minnesota ## 7 13 Bram Iowa ## 8 14 Rees New York ## 9 17 Orazio Louisiana ## 10 18 Mason Texas ## # ... with 39 more rows 4.10 Full Join Full join return all rows and all columns from both Age and Height. Where there are not matching values, returns NA for the one missing. 4.10.1 Case Study: Details of all customers and all orders To get details of all customers and all orders, let us join the order data with the customer data using full_join. full_join(customer, order, by = &quot;id&quot;) ## # A tibble: 349 x 5 ## id first_name city order_date amount ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 Elbertine California &lt;NA&gt; NA ## 2 2 Marcella Colorado 12/28/2016 235. ## 3 2 Marcella Colorado 8/31/2016 1150. ## 4 3 Daria Florida &lt;NA&gt; NA ## 5 4 Sherilyn Distric... &lt;NA&gt; NA ## 6 5 Ketty Texas 1/17/2017 346. ## 7 6 Jethro California 1/27/2017 2317. ## 8 7 Jeremiah California 6/21/2016 136. ## 9 7 Jeremiah California 2/13/2017 1407. ## 10 7 Jeremiah California 7/8/2016 1914. ## # ... with 339 more rows "],["dplyr-helper-functions.html", "Chapter 5 dplyr Helpers 5.1 Introduction 5.2 Case Study 5.3 Data Sanitization 5.4 Rename Columns 5.5 Data Tabulation 5.6 Sampling Data 5.7 Data Extraction 5.8 Between 5.9 Case When", " Chapter 5 dplyr Helpers 5.1 Introduction In this chapter, we will explore a set of helper functions in order to: extract unique rows rename columns sample data extract columns slice rows arrange rows compare tables extract/mutate data using predicate functions count observations for different levels of a variable We will use the following R packages: library(dplyr) library(readr) 5.2 Case Study Let us look at a case study (e-commerce data) and see how we can use dplyr helper functions to answer questions we have about and to modify/transform the underlying data set. 5.2.1 Data ecom &lt;- read_csv(&#39;https://raw.githubusercontent.com/rsquaredacademy/datasets/master/web.csv&#39;, col_types = cols_only(device = col_factor(levels = c(&quot;laptop&quot;, &quot;tablet&quot;, &quot;mobile&quot;)), referrer = col_factor(levels = c(&quot;bing&quot;, &quot;direct&quot;, &quot;social&quot;, &quot;yahoo&quot;, &quot;google&quot;)), purchase = col_logical(), bouncers = col_logical(), duration = col_double(), n_visit = col_double(), n_pages = col_double() ) ) ecom ## # A tibble: 1,000 x 7 ## referrer device bouncers n_visit n_pages duration purchase ## &lt;fct&gt; &lt;fct&gt; &lt;lgl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 google laptop TRUE 10 1 693 FALSE ## 2 yahoo tablet TRUE 9 1 459 FALSE ## 3 direct laptop TRUE 0 1 996 FALSE ## 4 bing tablet FALSE 3 18 468 TRUE ## 5 yahoo mobile TRUE 9 1 955 FALSE ## 6 yahoo laptop FALSE 5 5 135 FALSE ## 7 yahoo mobile TRUE 10 1 75 FALSE ## 8 direct mobile TRUE 10 1 908 FALSE ## 9 bing mobile FALSE 3 19 209 FALSE ## 10 google mobile TRUE 6 1 208 FALSE ## # ... with 990 more rows 5.2.2 Data Dictionary referrer: referrer website/search engine device: device used to visit the website bouncers: whether a visit bounced (exited from landing page) duration: time spent on the website (in seconds) purchase: whether visitor purchased n_visit: number of visits n_pages: number of pages visited/browsed 5.3 Data Sanitization Let us ensure that the data is sanitized by checking the sources of traffic and devices used to visit the site. We will use distinct to examine the values in the referrer column distinct(ecom, referrer) ## # A tibble: 5 x 1 ## referrer ## &lt;fct&gt; ## 1 google ## 2 yahoo ## 3 direct ## 4 bing ## 5 social and the device column as well. distinct(ecom, device) ## # A tibble: 3 x 1 ## device ## &lt;fct&gt; ## 1 laptop ## 2 tablet ## 3 mobile 5.4 Rename Columns Columns can be renamed using rename(). rename(ecom, time_on_site = duration) ## # A tibble: 1,000 x 7 ## referrer device bouncers n_visit n_pages time_on_site purchase ## &lt;fct&gt; &lt;fct&gt; &lt;lgl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 google laptop TRUE 10 1 693 FALSE ## 2 yahoo tablet TRUE 9 1 459 FALSE ## 3 direct laptop TRUE 0 1 996 FALSE ## 4 bing tablet FALSE 3 18 468 TRUE ## 5 yahoo mobile TRUE 9 1 955 FALSE ## 6 yahoo laptop FALSE 5 5 135 FALSE ## 7 yahoo mobile TRUE 10 1 75 FALSE ## 8 direct mobile TRUE 10 1 908 FALSE ## 9 bing mobile FALSE 3 19 209 FALSE ## 10 google mobile TRUE 6 1 208 FALSE ## # ... with 990 more rows 5.5 Data Tabulation Let us now look at the proportion or share of visits driven by different sources of traffic. ecom %&gt;% group_by(referrer) %&gt;% tally() ## # A tibble: 5 x 2 ## referrer n ## &lt;fct&gt; &lt;int&gt; ## 1 bing 194 ## 2 direct 191 ## 3 social 200 ## 4 yahoo 207 ## 5 google 208 We would also like to know the number of bouncers driven by the different sources of traffic. ecom %&gt;% group_by(referrer, bouncers) %&gt;% tally() ## # A tibble: 10 x 3 ## # Groups: referrer [5] ## referrer bouncers n ## &lt;fct&gt; &lt;lgl&gt; &lt;int&gt; ## 1 bing FALSE 104 ## 2 bing TRUE 90 ## 3 direct FALSE 98 ## 4 direct TRUE 93 ## 5 social FALSE 93 ## 6 social TRUE 107 ## 7 yahoo FALSE 110 ## 8 yahoo TRUE 97 ## 9 google FALSE 101 ## 10 google TRUE 107 Let us look at how many conversions happen across different devices. ecom %&gt;% group_by(device, purchase) %&gt;% tally() %&gt;% filter(purchase) ## # A tibble: 3 x 3 ## # Groups: device [3] ## device purchase n ## &lt;fct&gt; &lt;lgl&gt; &lt;int&gt; ## 1 laptop TRUE 31 ## 2 tablet TRUE 36 ## 3 mobile TRUE 36 Another way to extract the above information is by using count ecom %&gt;% count(referrer, purchase) %&gt;% filter(purchase) ## # A tibble: 5 x 3 ## referrer purchase n ## &lt;fct&gt; &lt;lgl&gt; &lt;int&gt; ## 1 bing TRUE 17 ## 2 direct TRUE 25 ## 3 social TRUE 20 ## 4 yahoo TRUE 22 ## 5 google TRUE 19 5.6 Sampling Data dplyr offers sampling functions which allow us to specify either the number or percentage of observations. sample_n() allows sampling a specific number of observations. sample_n(ecom, 700) ## # A tibble: 700 x 7 ## referrer device bouncers n_visit n_pages duration purchase ## &lt;fct&gt; &lt;fct&gt; &lt;lgl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 bing laptop FALSE 8 6 180 FALSE ## 2 social tablet TRUE 6 1 133 FALSE ## 3 direct tablet FALSE 6 13 338 TRUE ## 4 direct tablet FALSE 1 2 48 FALSE ## 5 social mobile FALSE 0 8 160 FALSE ## 6 yahoo laptop FALSE 7 4 88 FALSE ## 7 direct mobile FALSE 2 9 243 FALSE ## 8 google mobile FALSE 4 16 416 FALSE ## 9 yahoo laptop FALSE 1 2 30 FALSE ## 10 social mobile TRUE 10 1 866 FALSE ## # ... with 690 more rows We can combine the sampling functions with other dplyr functions as shown below where we sample observation after grouping them according to the source of traffic. ecom %&gt;% group_by(referrer) %&gt;% sample_n(100) ## # A tibble: 500 x 7 ## # Groups: referrer [5] ## referrer device bouncers n_visit n_pages duration purchase ## &lt;fct&gt; &lt;fct&gt; &lt;lgl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 bing tablet FALSE 8 19 570 TRUE ## 2 bing mobile FALSE 5 15 405 TRUE ## 3 bing mobile TRUE 1 1 905 FALSE ## 4 bing laptop FALSE 0 12 144 FALSE ## 5 bing tablet FALSE 2 11 220 FALSE ## 6 bing laptop TRUE 4 1 748 FALSE ## 7 bing laptop TRUE 3 1 682 FALSE ## 8 bing mobile FALSE 8 20 380 FALSE ## 9 bing mobile TRUE 9 1 574 FALSE ## 10 bing mobile TRUE 3 1 490 FALSE ## # ... with 490 more rows sample_frac() allows a specific percentage of observations. sample_frac(ecom, size = 0.7) ## # A tibble: 700 x 7 ## referrer device bouncers n_visit n_pages duration purchase ## &lt;fct&gt; &lt;fct&gt; &lt;lgl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 direct tablet TRUE 2 1 184 FALSE ## 2 google laptop TRUE 5 1 951 FALSE ## 3 direct tablet TRUE 4 1 141 FALSE ## 4 direct mobile TRUE 10 1 908 FALSE ## 5 bing tablet FALSE 1 19 361 TRUE ## 6 yahoo tablet FALSE 5 20 500 FALSE ## 7 google tablet TRUE 10 1 512 FALSE ## 8 social laptop FALSE 5 2 26 FALSE ## 9 direct tablet FALSE 6 13 338 TRUE ## 10 bing mobile TRUE 3 1 490 FALSE ## # ... with 690 more rows 5.7 Data Extraction In the first chapter, we had observed that dplyr verbs always returned a tibble. What if you want to extract a specific column or a bunch of rows but not as a tibble? Use pull to extract columns either by name or position. It will return a vector. In the below example, we extract the device column as a vector. I am using head in addition to limit the output printed. 5.7.1 Sample Data ecom_mini &lt;- sample_n(ecom, size = 10) pull(ecom_mini, device) ## [1] laptop tablet tablet mobile laptop laptop tablet tablet mobile laptop ## Levels: laptop tablet mobile Let us extract the first column from ecom using column position instead of name. pull(ecom_mini, 1) ## [1] google bing social direct yahoo bing social social yahoo yahoo ## Levels: bing direct social yahoo google You can use - before the column position to indicate the position in reverse. The below example extracts data from the last column. pull(ecom_mini, -1) ## [1] FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE Let us now look at extracting rows using slice(). In the below example, we extract data starting from the 5th row and upto the 15th row. slice(ecom, 5:15) ## # A tibble: 11 x 7 ## referrer device bouncers n_visit n_pages duration purchase ## &lt;fct&gt; &lt;fct&gt; &lt;lgl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 yahoo mobile TRUE 9 1 955 FALSE ## 2 yahoo laptop FALSE 5 5 135 FALSE ## 3 yahoo mobile TRUE 10 1 75 FALSE ## 4 direct mobile TRUE 10 1 908 FALSE ## 5 bing mobile FALSE 3 19 209 FALSE ## 6 google mobile TRUE 6 1 208 FALSE ## 7 direct laptop TRUE 9 1 738 FALSE ## 8 direct tablet FALSE 6 12 132 FALSE ## 9 direct mobile FALSE 9 14 406 TRUE ## 10 yahoo tablet FALSE 5 8 80 FALSE ## 11 yahoo mobile FALSE 7 1 19 FALSE Use n() inside slice() to extract the last row. slice(ecom, n()) ## # A tibble: 1 x 7 ## referrer device bouncers n_visit n_pages duration purchase ## &lt;fct&gt; &lt;fct&gt; &lt;lgl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 google mobile TRUE 9 1 269 FALSE 5.8 Between between() allows us to test if the values in a column lie between two specific values. In the below example, we check how many visits browsed pages between 5 and 15. ecom_sample &lt;- sample_n(ecom, 30) ecom_sample %&gt;% pull(n_pages) %&gt;% between(5, 15) ## [1] TRUE FALSE TRUE TRUE TRUE TRUE FALSE FALSE FALSE FALSE FALSE TRUE ## [13] FALSE FALSE FALSE TRUE FALSE FALSE FALSE TRUE FALSE FALSE FALSE TRUE ## [25] FALSE FALSE FALSE TRUE FALSE FALSE 5.9 Case When case_when() is an alternative to if else. It allows us to lay down the conditions clearly and makes the code more readable. In the below example, we create a new column repeat_visit from n_visit (the number of previous visits). ecom %&gt;% mutate( repeat_visit = case_when( n_visit &gt; 0 ~ TRUE, TRUE ~ FALSE ) ) %&gt;% select(n_visit, repeat_visit) ## # A tibble: 1,000 x 2 ## n_visit repeat_visit ## &lt;dbl&gt; &lt;lgl&gt; ## 1 10 TRUE ## 2 9 TRUE ## 3 0 FALSE ## 4 3 TRUE ## 5 9 TRUE ## 6 5 TRUE ## 7 10 TRUE ## 8 10 TRUE ## 9 3 TRUE ## 10 6 TRUE ## # ... with 990 more rows "],["r-pipe-magrittr.html", "Chapter 6 Pipe Operator 6.1 Introduction 6.2 Pipes 6.3 Data 6.4 head() 6.5 Square Root 6.6 Visualization 6.7 Correlation 6.8 Regression 6.9 String Manipulation 6.10 Data Extraction 6.11 Arithmetic Operations 6.12 Logical Operators", " Chapter 6 Pipe Operator 6.1 Introduction R code contain a lot of parentheses in case of a sequence of multiple operations. When you are dealing with complex code, it results in nested function calls which are hard to read and maintain. The magrittr package by Stefan Milton Bache provides pipes enabling us to write R code that is readable. Pipes allow us to clearly express a sequence of multiple operations by: structuring operations from left to right avoiding nested function calls intermediate steps overwriting of original data minimizing creation of local variables 6.2 Pipes If you are using tidyverse, magrittr will be automatically loaded. We will look at 3 different types of pipes: %&gt;% : pipe a value forward into an expression or function call %&lt;&gt;%: result assigned to left hand side object instead of returning it %$% : expose names within left hand side objects to right hand side expressions We will use the following R packages: library(magrittr) library(readr) library(dplyr) library(stringr) library(purrr) 6.3 Data ecom &lt;- read_csv(&#39;https://raw.githubusercontent.com/rsquaredacademy/datasets/master/web.csv&#39;, col_types = cols_only( referrer = col_factor(levels = c(&quot;bing&quot;, &quot;direct&quot;, &quot;social&quot;, &quot;yahoo&quot;, &quot;google&quot;)), n_pages = col_double(), duration = col_double(), purchase = col_logical() ) ) ecom ## # A tibble: 1,000 x 4 ## referrer n_pages duration purchase ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 google 1 693 FALSE ## 2 yahoo 1 459 FALSE ## 3 direct 1 996 FALSE ## 4 bing 18 468 TRUE ## 5 yahoo 1 955 FALSE ## 6 yahoo 5 135 FALSE ## 7 yahoo 1 75 FALSE ## 8 direct 1 908 FALSE ## 9 bing 19 209 FALSE ## 10 google 1 208 FALSE ## # ... with 990 more rows We will create a smaller data set from the above data to be used in some examples: ecom_mini &lt;- sample_n(ecom, size = 10) ecom_mini ## # A tibble: 10 x 4 ## referrer n_pages duration purchase ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 social 2 44 FALSE ## 2 direct 1 239 FALSE ## 3 yahoo 8 232 FALSE ## 4 yahoo 18 270 TRUE ## 5 direct 1 244 FALSE ## 6 google 1 249 FALSE ## 7 bing 7 119 FALSE ## 8 google 1 806 FALSE ## 9 google 5 75 FALSE ## 10 bing 1 169 FALSE 6.3.1 Data Dictionary referrer: referrer website/search engine n_pages: number of pages visited duration: time spent on the website (in seconds) purchase: whether visitor purchased 6.4 head() Let us start with a simple example. You must be aware of head(). If not, do not worry. It returns the first few observations/rows of data. We can specify the number of observations it should return as well. Let us use it to view the first 10 rows of our data set. head(ecom, 10) ## # A tibble: 10 x 4 ## referrer n_pages duration purchase ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 google 1 693 FALSE ## 2 yahoo 1 459 FALSE ## 3 direct 1 996 FALSE ## 4 bing 18 468 TRUE ## 5 yahoo 1 955 FALSE ## 6 yahoo 5 135 FALSE ## 7 yahoo 1 75 FALSE ## 8 direct 1 908 FALSE ## 9 bing 19 209 FALSE ## 10 google 1 208 FALSE 6.4.1 Using Pipe Now let us do the same but with %&gt;%. ecom %&gt;% head(10) ## # A tibble: 10 x 4 ## referrer n_pages duration purchase ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 google 1 693 FALSE ## 2 yahoo 1 459 FALSE ## 3 direct 1 996 FALSE ## 4 bing 18 468 TRUE ## 5 yahoo 1 955 FALSE ## 6 yahoo 5 135 FALSE ## 7 yahoo 1 75 FALSE ## 8 direct 1 908 FALSE ## 9 bing 19 209 FALSE ## 10 google 1 208 FALSE 6.5 Square Root Time to try a slightly more challenging example. We want the square root of n_pages column from the data set. y &lt;- sqrt(ecom_mini$n_pages) Let us break down the above computation into small steps: select/expose the n_pages column from ecom data compute the square root assign the first few observations to y Let us reproduce y using pipes. # select n_pages variable and assign it to y y &lt;- ecom_mini %$% n_pages # compute square root of y and assign it to y y %&lt;&gt;% sqrt Another way to compute the square root of y is shown below. y &lt;- ecom_mini %$% n_pages %&gt;% sqrt() 6.6 Visualization Let us look at a data visualization example. We will create a bar plot to visualize the frequency of different referrer types that drove purchasers to the website. Let us look at the steps involved in creating the bar plot: extract rows where purchase is TRUE select/expose referrer column tabulate referrer data using table() use the tabulated data to create bar plot using barplot() barplot(table(subset(ecom, purchase)$referrer)) 6.6.1 Using pipe ecom %&gt;% subset(purchase) %&gt;% extract(&#39;referrer&#39;) %&gt;% table() %&gt;% barplot() 6.7 Correlation Correlation is a statistical measure that indicates the extent to which two or more variables fluctuate together. In R, correlation is computed using cor(). Let us look at the correlation between the number of pages browsed and time spent on the site for visitors who purchased some product. Below are the steps for computing correlation: extract rows where purchase is TRUE select/expose n_pages and duration columns use cor() to compute the correlation # without pipe ecom1 &lt;- subset(ecom, purchase) cor(ecom1$n_pages, ecom1$duration) ## [1] 0.4290905 # with pipe ecom %&gt;% subset(purchase) %$% cor(n_pages, duration) ## [1] 0.4290905 # with pipe ecom %&gt;% filter(purchase) %$% cor(n_pages, duration) ## [1] 0.4290905 6.8 Regression Let us look at a regression example. We regress time spent on the site on number of pages visited. Below are the steps involved in running the regression: use duration and n_pages columns from ecom data pass the above data to lm() pass the output from lm() to summary() summary(lm(duration ~ n_pages, data = ecom)) ## ## Call: ## lm(formula = duration ~ n_pages, data = ecom) ## ## Residuals: ## Min 1Q Median 3Q Max ## -386.45 -213.03 -38.93 179.31 602.55 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 404.803 11.323 35.750 &lt; 2e-16 *** ## n_pages -8.355 1.296 -6.449 1.76e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 263.3 on 998 degrees of freedom ## Multiple R-squared: 0.04, Adjusted R-squared: 0.03904 ## F-statistic: 41.58 on 1 and 998 DF, p-value: 1.756e-10 6.8.1 Using pipe ecom %$% lm(duration ~ n_pages) %&gt;% summary() ## ## Call: ## lm(formula = duration ~ n_pages) ## ## Residuals: ## Min 1Q Median 3Q Max ## -386.45 -213.03 -38.93 179.31 602.55 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 404.803 11.323 35.750 &lt; 2e-16 *** ## n_pages -8.355 1.296 -6.449 1.76e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 263.3 on 998 degrees of freedom ## Multiple R-squared: 0.04, Adjusted R-squared: 0.03904 ## F-statistic: 41.58 on 1 and 998 DF, p-value: 1.756e-10 6.9 String Manipulation We want to extract the first name (jovial) from the below email id and convert it to upper case. Below are the steps to achieve this: split the email id using the pattern @ using str_split() extract the first element from the resulting list using extract2() extract the first element from the character vector using extract() extract the first six characters using str_sub() convert to upper case using str_to_upper() email &lt;- &#39;jovialcann@anymail.com&#39; # without pipe str_to_upper(str_sub(str_split(email, &#39;@&#39;)[[1]][1], start = 1, end = 6)) ## [1] &quot;JOVIAL&quot; # with pipe email %&gt;% str_split(pattern = &#39;@&#39;) %&gt;% extract2(1) %&gt;% extract(1) %&gt;% str_sub(start = 1, end = 6) %&gt;% str_to_upper() ## [1] &quot;JOVIAL&quot; Another method that uses map_chr() from the purrr package. email %&gt;% str_split(pattern = &#39;@&#39;) %&gt;% map_chr(1) %&gt;% str_sub(start = 1, end = 6) %&gt;% str_to_upper() ## [1] &quot;JOVIAL&quot; 6.10 Data Extraction Let us turn our attention towards data extraction. magrittr provides alternatives to $, [ and [[. extract() extract2() use_series() 6.10.1 Extract Column To extract a specific column using the column name, we mention the name of the column in single/double quotes within [ or [[. In case of $, we do not use quotes. # base ecom_mini[&#39;n_pages&#39;] ## # A tibble: 10 x 1 ## n_pages ## &lt;dbl&gt; ## 1 2 ## 2 1 ## 3 8 ## 4 18 ## 5 1 ## 6 1 ## 7 7 ## 8 1 ## 9 5 ## 10 1 # magrittr extract(ecom_mini, &#39;n_pages&#39;) ## # A tibble: 10 x 1 ## n_pages ## &lt;dbl&gt; ## 1 2 ## 2 1 ## 3 8 ## 4 18 ## 5 1 ## 6 1 ## 7 7 ## 8 1 ## 9 5 ## 10 1 We can extract columns using their index position. Keep in mind that index position starts from 1 in R. In the below example, we show how to extract n_pages column but instead of using the column name, we use the column position. # base ecom_mini[2] ## # A tibble: 10 x 1 ## n_pages ## &lt;dbl&gt; ## 1 2 ## 2 1 ## 3 8 ## 4 18 ## 5 1 ## 6 1 ## 7 7 ## 8 1 ## 9 5 ## 10 1 # magrittr extract(ecom_mini, 2) ## # A tibble: 10 x 1 ## n_pages ## &lt;dbl&gt; ## 1 2 ## 2 1 ## 3 8 ## 4 18 ## 5 1 ## 6 1 ## 7 7 ## 8 1 ## 9 5 ## 10 1 One important differentiator between [ and [[ is that [[ will return a atomic vector and not a data.frame. $ will also return a atomic vector. In magrittr, we can use use_series() in place of $. # base ecom_mini$n_pages ## [1] 2 1 8 18 1 1 7 1 5 1 # magrittr use_series(ecom_mini, &#39;n_pages&#39;) ## [1] 2 1 8 18 1 1 7 1 5 1 6.10.2 Extract List Element Let us convert ecom_mini into a list using as.list() as shown below: ecom_list &lt;- as.list(ecom_mini) To extract elements of a list, we can use extract2(). It is an alternative for [[. # base ecom_list[[&#39;n_pages&#39;]] ## [1] 2 1 8 18 1 1 7 1 5 1 # magrittr extract2(ecom_list, &#39;n_pages&#39;) ## [1] 2 1 8 18 1 1 7 1 5 1 # base ecom_list[[1]] ## [1] social direct yahoo yahoo direct google bing google google bing ## Levels: bing direct social yahoo google # magrittr extract2(ecom_list, 1) ## [1] social direct yahoo yahoo direct google bing google google bing ## Levels: bing direct social yahoo google We can extract the elements of a list using use_series() as well. # base ecom_list$n_pages ## [1] 2 1 8 18 1 1 7 1 5 1 # magrittr use_series(ecom_list, n_pages) ## [1] 2 1 8 18 1 1 7 1 5 1 6.11 Arithmetic Operations magrittr offer alternatives for arithemtic operations as well. We will look at a few examples below. add() subtract() multiply_by() multiply_by_matrix() divide_by() divide_by_int() mod() raise_to_power() 6.11.1 Addition 1:10 + 1 ## [1] 2 3 4 5 6 7 8 9 10 11 add(1:10, 1) ## [1] 2 3 4 5 6 7 8 9 10 11 `+`(1:10, 1) ## [1] 2 3 4 5 6 7 8 9 10 11 6.11.2 Multiplication 1:10 * 3 ## [1] 3 6 9 12 15 18 21 24 27 30 multiply_by(1:10, 3) ## [1] 3 6 9 12 15 18 21 24 27 30 `*`(1:10, 3) ## [1] 3 6 9 12 15 18 21 24 27 30 6.11.3 Division 1:10 / 2 ## [1] 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 divide_by(1:10, 2) ## [1] 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 `/`(1:10, 2) ## [1] 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 6.11.4 Power 1:10 ^ 2 ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## [19] 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 ## [37] 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 ## [55] 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 ## [73] 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 ## [91] 91 92 93 94 95 96 97 98 99 100 raise_to_power(1:10, 2) ## [1] 1 4 9 16 25 36 49 64 81 100 `^`(1:10, 2) ## [1] 1 4 9 16 25 36 49 64 81 100 6.12 Logical Operators There are alternatives for logical operators as well. We will look at a few examples below. and() or() equals() not() is_greater_than() is_weakly_greater_than() is_less_than() is_weakly_less_than() 6.12.1 Greater Than 1:10 &gt; 5 ## [1] FALSE FALSE FALSE FALSE FALSE TRUE TRUE TRUE TRUE TRUE is_greater_than(1:10, 5) ## [1] FALSE FALSE FALSE FALSE FALSE TRUE TRUE TRUE TRUE TRUE `&gt;`(1:10, 5) ## [1] FALSE FALSE FALSE FALSE FALSE TRUE TRUE TRUE TRUE TRUE 6.12.2 Weakly Greater Than 1:10 &gt;= 5 ## [1] FALSE FALSE FALSE FALSE TRUE TRUE TRUE TRUE TRUE TRUE is_weakly_greater_than(1:10, 5) ## [1] FALSE FALSE FALSE FALSE TRUE TRUE TRUE TRUE TRUE TRUE `&gt;=`(1:10, 5) ## [1] FALSE FALSE FALSE FALSE TRUE TRUE TRUE TRUE TRUE TRUE "],["tibbles-in-r.html", "Chapter 7 tibbles 7.1 Introduction 7.2 Creating tibbles 7.3 tibble features 7.4 Membership Testing 7.5 Tribble 7.6 Column Names 7.7 Add Rows 7.8 Add Columns 7.9 Rownames 7.10 Glimpse 7.11 Check Column 7.12 Summary", " Chapter 7 tibbles 7.1 Introduction A tibble, or tbl_df, is a modern reimagining of the data.frame, keeping what time has proven to be effective, and throwing out what is not. Tibbles are data.frames that are lazy and surly: they do less (i.e. they dont change variable names or types, and dont do partial matching) and complain more (e.g. when a variable does not exist). This forces you to confront problems earlier, typically leading to cleaner, more expressive code. Tibbles also have an enhanced print method() which makes them easier to use with large datasets containing complex objects. Source: https://tibble.tidyverse.org/ In this chapter, we will explore tibbles. To be more precise, we will learn: how tibbles are different from data frames? how to create tibbles? how to manipulate tibbles? We will use the following R packages: library(tibble) library(dplyr) 7.2 Creating tibbles tibble can be created using any of the following: tibble() as_tibble() tribble() Let us start with tibble(). tibble(x = letters, y = 1:26, z = sample(100, 26)) ## # A tibble: 26 x 3 ## x y z ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 a 1 100 ## 2 b 2 24 ## 3 c 3 35 ## 4 d 4 84 ## 5 e 5 71 ## 6 f 6 22 ## 7 g 7 54 ## 8 h 8 63 ## 9 i 9 66 ## 10 j 10 53 ## # ... with 16 more rows We mentioned the column names followed by the data. If you do not specify the column names, tibble() will supply them. Ensure that the length of each column is same. 7.3 tibble features 7.3.1 never changes inputs types tibble() will never alter the inputs type. For example, if you supply a character vector it will not be converted to factor unlike data.frame where you need to set stringsAsFactors to FALSE. tibble(x = letters, y = 1:26, z = sample(100, 26)) ## # A tibble: 26 x 3 ## x y z ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 a 1 47 ## 2 b 2 24 ## 3 c 3 2 ## 4 d 4 87 ## 5 e 5 61 ## 6 f 6 37 ## 7 g 7 93 ## 8 h 8 67 ## 9 i 9 16 ## 10 j 10 42 ## # ... with 16 more rows 7.3.2 never adjusts variable names tibble() will never modify the column names. In the below example, you can observe that while data.frame adds a ., tibble() retains the column names as is. names(data.frame(`order value` = 10)) ## [1] &quot;order.value&quot; names(tibble(`order value` = 10)) ## [1] &quot;order value&quot; 7.3.3 never prints all rows tibble() will never print all the rows and clutter your console. It will only print the first 10 rows and only as many columns that fit the width of the console. x &lt;- 1:100 y &lt;- letters[1] z &lt;- sample(c(TRUE, FALSE), 100, replace = TRUE) tibble(x, y, z) ## # A tibble: 100 x 3 ## x y z ## &lt;int&gt; &lt;chr&gt; &lt;lgl&gt; ## 1 1 a FALSE ## 2 2 a TRUE ## 3 3 a FALSE ## 4 4 a TRUE ## 5 5 a TRUE ## 6 6 a FALSE ## 7 7 a TRUE ## 8 8 a FALSE ## 9 9 a TRUE ## 10 10 a FALSE ## # ... with 90 more rows 7.3.4 never recycles vector of length greater than 1 Recycling vectors of length greater than 1 often leads to errors and as such tibble() will only recycle vectors of length 1. x &lt;- 1:100 y &lt;- letters z &lt;- sample(c(TRUE, FALSE), 100, replace = TRUE) tibble(x, y, z) Error in overscope_eval_next(overscope, expr) : object &#39;y&#39; not found 7.4 Membership Testing We can test if an object is a tibble using is_tibble(). is_tibble(mtcars) ## [1] FALSE is_tibble(as_tibble(mtcars)) ## [1] TRUE 7.5 Tribble Another way to create tibbles is using tribble(): it is short for transposed tibbles it is customized for data entry in code column names start with ~ and values are separated by commas tribble( ~x, ~y, ~z, #--|--|---- 1, TRUE, &#39;a&#39;, 2, FALSE, &#39;b&#39; ) ## # A tibble: 2 x 3 ## x y z ## &lt;dbl&gt; &lt;lgl&gt; &lt;chr&gt; ## 1 1 TRUE a ## 2 2 FALSE b 7.6 Column Names Names of the columns in tibbles need not be valid R variable names. They can contain unusual characters like a space or a smiley but must be enclosed in ticks. tibble( ` ` = &#39;space&#39;, `2` = &#39;integer&#39;, `:)` = &#39;smiley&#39; ) ## # A tibble: 1 x 3 ## ` ` `2` `:)` ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 space integer smiley 7.7 Add Rows Let us add data related to Safari browser to the web traffic data using add_row(). browsers &lt;- enframe(c(chrome = 40, firefox = 20, edge = 30)) browsers ## # A tibble: 3 x 2 ## name value ## &lt;chr&gt; &lt;dbl&gt; ## 1 chrome 40 ## 2 firefox 20 ## 3 edge 30 add_row(browsers, name = &#39;safari&#39;, value = 10) ## # A tibble: 4 x 2 ## name value ## &lt;chr&gt; &lt;dbl&gt; ## 1 chrome 40 ## 2 firefox 20 ## 3 edge 30 ## 4 safari 10 If we want to add the data at a particular row, we can specify the row number using the .before argument. Let us add the data related to Safari browser in the second row instead of the last row. add_row(browsers, name = &#39;safari&#39;, value = 10, .before = 2) ## # A tibble: 4 x 2 ## name value ## &lt;chr&gt; &lt;dbl&gt; ## 1 chrome 40 ## 2 safari 10 ## 3 firefox 20 ## 4 edge 30 7.8 Add Columns add_column() adds a new column to tibbles. browsers &lt;- enframe(c(chrome = 40, firefox = 20, edge = 30, safari = 10)) add_column(browsers, visits = c(4000, 2000, 3000, 1000)) ## # A tibble: 4 x 3 ## name value visits ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 chrome 40 4000 ## 2 firefox 20 2000 ## 3 edge 30 3000 ## 4 safari 10 1000 7.9 Rownames The tibble package provides a set of functions to deal with rownames. Remember, tibble does not have rownames unlike data.frame. To check whether a data set has rownames, use has_rownames(). has_rownames(mtcars) ## [1] TRUE 7.9.1 Remove Rownames remove_rownames(mtcars) ## mpg cyl disp hp drat wt qsec vs am gear carb ## 1 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 4 ## 2 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 4 ## 3 22.8 4 108.0 93 3.85 2.320 18.61 1 1 4 1 ## 4 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1 ## 5 18.7 8 360.0 175 3.15 3.440 17.02 0 0 3 2 ## 6 18.1 6 225.0 105 2.76 3.460 20.22 1 0 3 1 ## 7 14.3 8 360.0 245 3.21 3.570 15.84 0 0 3 4 ## 8 24.4 4 146.7 62 3.69 3.190 20.00 1 0 4 2 ## 9 22.8 4 140.8 95 3.92 3.150 22.90 1 0 4 2 ## 10 19.2 6 167.6 123 3.92 3.440 18.30 1 0 4 4 ## 11 17.8 6 167.6 123 3.92 3.440 18.90 1 0 4 4 ## 12 16.4 8 275.8 180 3.07 4.070 17.40 0 0 3 3 ## 13 17.3 8 275.8 180 3.07 3.730 17.60 0 0 3 3 ## 14 15.2 8 275.8 180 3.07 3.780 18.00 0 0 3 3 ## 15 10.4 8 472.0 205 2.93 5.250 17.98 0 0 3 4 ## 16 10.4 8 460.0 215 3.00 5.424 17.82 0 0 3 4 ## 17 14.7 8 440.0 230 3.23 5.345 17.42 0 0 3 4 ## 18 32.4 4 78.7 66 4.08 2.200 19.47 1 1 4 1 ## 19 30.4 4 75.7 52 4.93 1.615 18.52 1 1 4 2 ## 20 33.9 4 71.1 65 4.22 1.835 19.90 1 1 4 1 ## 21 21.5 4 120.1 97 3.70 2.465 20.01 1 0 3 1 ## 22 15.5 8 318.0 150 2.76 3.520 16.87 0 0 3 2 ## 23 15.2 8 304.0 150 3.15 3.435 17.30 0 0 3 2 ## 24 13.3 8 350.0 245 3.73 3.840 15.41 0 0 3 4 ## 25 19.2 8 400.0 175 3.08 3.845 17.05 0 0 3 2 ## 26 27.3 4 79.0 66 4.08 1.935 18.90 1 1 4 1 ## 27 26.0 4 120.3 91 4.43 2.140 16.70 0 1 5 2 ## 28 30.4 4 95.1 113 3.77 1.513 16.90 1 1 5 2 ## 29 15.8 8 351.0 264 4.22 3.170 14.50 0 1 5 4 ## 30 19.7 6 145.0 175 3.62 2.770 15.50 0 1 5 6 ## 31 15.0 8 301.0 335 3.54 3.570 14.60 0 1 5 8 ## 32 21.4 4 121.0 109 4.11 2.780 18.60 1 1 4 2 7.9.2 Rownames to Column head(rownames_to_column(mtcars)) ## rowname mpg cyl disp hp drat wt qsec vs am gear carb ## 1 Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## 2 Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## 3 Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 ## 4 Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 ## 5 Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 ## 6 Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 7.9.3 Column to Rownames To convert the first column in the data set to rownames, use column_to_rownames(): mtcars_tbl &lt;- rownames_to_column(mtcars) column_to_rownames(mtcars_tbl) ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108.0 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360.0 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225.0 105 2.76 3.460 20.22 1 0 3 1 ## Duster 360 14.3 8 360.0 245 3.21 3.570 15.84 0 0 3 4 ## Merc 240D 24.4 4 146.7 62 3.69 3.190 20.00 1 0 4 2 ## Merc 230 22.8 4 140.8 95 3.92 3.150 22.90 1 0 4 2 ## Merc 280 19.2 6 167.6 123 3.92 3.440 18.30 1 0 4 4 ## Merc 280C 17.8 6 167.6 123 3.92 3.440 18.90 1 0 4 4 ## Merc 450SE 16.4 8 275.8 180 3.07 4.070 17.40 0 0 3 3 ## Merc 450SL 17.3 8 275.8 180 3.07 3.730 17.60 0 0 3 3 ## Merc 450SLC 15.2 8 275.8 180 3.07 3.780 18.00 0 0 3 3 ## Cadillac Fleetwood 10.4 8 472.0 205 2.93 5.250 17.98 0 0 3 4 ## Lincoln Continental 10.4 8 460.0 215 3.00 5.424 17.82 0 0 3 4 ## Chrysler Imperial 14.7 8 440.0 230 3.23 5.345 17.42 0 0 3 4 ## Fiat 128 32.4 4 78.7 66 4.08 2.200 19.47 1 1 4 1 ## Honda Civic 30.4 4 75.7 52 4.93 1.615 18.52 1 1 4 2 ## Toyota Corolla 33.9 4 71.1 65 4.22 1.835 19.90 1 1 4 1 ## Toyota Corona 21.5 4 120.1 97 3.70 2.465 20.01 1 0 3 1 ## Dodge Challenger 15.5 8 318.0 150 2.76 3.520 16.87 0 0 3 2 ## AMC Javelin 15.2 8 304.0 150 3.15 3.435 17.30 0 0 3 2 ## Camaro Z28 13.3 8 350.0 245 3.73 3.840 15.41 0 0 3 4 ## Pontiac Firebird 19.2 8 400.0 175 3.08 3.845 17.05 0 0 3 2 ## Fiat X1-9 27.3 4 79.0 66 4.08 1.935 18.90 1 1 4 1 ## Porsche 914-2 26.0 4 120.3 91 4.43 2.140 16.70 0 1 5 2 ## Lotus Europa 30.4 4 95.1 113 3.77 1.513 16.90 1 1 5 2 ## Ford Pantera L 15.8 8 351.0 264 4.22 3.170 14.50 0 1 5 4 ## Ferrari Dino 19.7 6 145.0 175 3.62 2.770 15.50 0 1 5 6 ## Maserati Bora 15.0 8 301.0 335 3.54 3.570 14.60 0 1 5 8 ## Volvo 142E 21.4 4 121.0 109 4.11 2.780 18.60 1 1 4 2 7.10 Glimpse Use glimpse() to get an overview of the data. glimpse(mtcars) ## Rows: 32 ## Columns: 11 ## $ mpg &lt;dbl&gt; 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17.8,~ ## $ cyl &lt;dbl&gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4, 8,~ ## $ disp &lt;dbl&gt; 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 140.8, 16~ ## $ hp &lt;dbl&gt; 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 180, 180~ ## $ drat &lt;dbl&gt; 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.92, 3.92,~ ## $ wt &lt;dbl&gt; 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3.150, 3.~ ## $ qsec &lt;dbl&gt; 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 22.90, 18~ ## $ vs &lt;dbl&gt; 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,~ ## $ am &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,~ ## $ gear &lt;dbl&gt; 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, 3,~ ## $ carb &lt;dbl&gt; 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, 1, 1, 2,~ 7.11 Check Column has_name() can be used to check if a tibble has a specific column. has_name(mtcars, &#39;cyl&#39;) ## [1] TRUE has_name(mtcars, &#39;gears&#39;) ## [1] FALSE 7.12 Summary 7.12.1 Creating tibbles use tibble() to create tibbles use as_tibble() to coerce other objects to tibble use enframe() to coerce vector to tibble use tribble() to create tibble using data entry 7.12.2 Modifying tibbles use add_row() to add a new row use add_column() to add a new column use remove_rownames() to remove rownames from data use rownames_to_colum() to coerce rowname to first column use column_to_rownames() to coerce first column to rownames 7.12.3 Testing tibbles use is_tibble() to test if an object is a tibble use has_rownames() to check whether a data set has rownames use has_name() to check if tibble has a specific column use glimpse() to get an overview of data "],["strings-in-r.html", "Chapter 8 Hacking Strings 8.1 Introduction 8.2 Case Study 8.3 Overview 8.4 Extract domain name from email ids 8.5 Extract Domain Extension 8.6 Extract image type from URL 8.7 Extract Image Dimesion from URL 8.8 Extract HTTP Protocol from URL 8.9 Extract file type", " Chapter 8 Hacking Strings 8.1 Introduction In this chapter, we will learn to work with string data in R using stringr. As we did in the other chapters, we will use a case study to explore the various features of the stringr package. We will use the following R packages: library(stringr) library(tibble) library(magrittr) library(purrr) library(dplyr) library(readr) 8.2 Case Study extract domain name from random email ids extract image type from url extract image dimension from url extract extension from domain name extract http protocol from url extract file type from url 8.2.1 Data mockstring &lt;- read_csv(&#39;https://raw.githubusercontent.com/rsquaredacademy/datasets/master/mock_strings.csv&#39;) mockstring ## # A tibble: 1,000 x 12 ## id ## &lt;dbl&gt; ## 1 1 ## 2 2 ## 3 3 ## 4 4 ## 5 5 ## 6 6 ## 7 7 ## 8 8 ## 9 9 ## 10 10 ## image_url ## &lt;chr&gt; ## 1 https://robohash.org/providentassumendaexplicabo.jpg?size=50x50&amp;set=set1 ## 2 https://robohash.org/etillumvoluptate.jpg?size=50x50&amp;set=set1 ## 3 https://robohash.org/nonoptiovoluptatibus.jpg?size=50x50&amp;set=set1 ## 4 https://robohash.org/voluptatumauthic.jpg?size=50x50&amp;set=set1 ## 5 https://robohash.org/placeaterrorqui.jpg?size=50x50&amp;set=set1 ## 6 https://robohash.org/temporeutea.jpg?size=50x50&amp;set=set1 ## 7 https://robohash.org/maximesaepequi.bmp?size=50x50&amp;set=set1 ## 8 https://robohash.org/nemoautesse.png?size=50x50&amp;set=set1 ## 9 https://robohash.org/odiorerumaut.png?size=50x50&amp;set=set1 ## 10 https://robohash.org/omnismolestiaearchitecto.png?size=50x50&amp;set=set1 ## domain imageurl ## &lt;chr&gt; &lt;chr&gt; ## 1 addtoany.com http://dummyimage.com/130x183.jpg/dddddd/000000 ## 2 gmpg.org http://dummyimage.com/106x217.bmp/dddddd/000000 ## 3 samsung.com http://dummyimage.com/146x127.bmp/cc0000/ffffff ## 4 spotify.com http://dummyimage.com/181x194.png/5fa2dd/ffffff ## 5 wunderground.com http://dummyimage.com/220x123.jpg/ff4444/ffffff ## 6 alexa.com http://dummyimage.com/118x176.bmp/dddddd/000000 ## 7 google.it http://dummyimage.com/185x202.jpg/ff4444/ffffff ## 8 ed.gov http://dummyimage.com/223x163.jpg/ff4444/ffffff ## 9 jigsy.com http://dummyimage.com/145x113.jpg/5fa2dd/ffffff ## 10 jugem.jp http://dummyimage.com/238x214.png/cc0000/ffffff ## email filename phone ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 mnewburn0@fastcompany.com PedeMalesuada.xls 66-(777)902-6181 ## 2 mdankersley1@digg.com LobortisVel.mp3 351-(422)736-6807 ## 3 hgirhard2@altervista.org CongueDiamId.pdf 33-(371)684-5114 ## 4 pmcmenamy3@sciencedirect.com EleifendQuam.avi 86-(410)823-6712 ## 5 drisbrough4@bandcamp.com PurusPhasellus.mp3 223-(518)814-6361 ## 6 cphlippi5@surveymonkey.com ElementumInHac.avi 420-(760)354-8671 ## 7 kdodswell6@un.org Mattis.doc 1-(712)615-2879 ## 8 vhourihane7@ovh.net PurusEu.tiff 62-(437)705-1118 ## 9 rdike8@timesonline.co.uk JustoEtiamPretium.xls 1-(683)965-1323 ## 10 tdudbridge9@clickbank.net Ante.tiff 30-(553)559-7448 ## address ## &lt;chr&gt; ## 1 8 Anhalt Crossing ## 2 697 East Avenue ## 3 89 Dottie Circle ## 4 98135 Blue Bill Park Drive ## 5 7814 Pennsylvania Street ## 6 4897 Little Fleur Drive ## 7 53541 Morrow Center ## 8 4819 Hermina Parkway ## 9 68096 Monument Park ## 10 9595 Spaight Avenue ## url ## &lt;chr&gt; ## 1 https://engadget.com/nascetur/ridiculus/mus/vivamus/vestibulum.jsp?eu=est&amp;ti~ ## 2 http://delicious.com/phasellus/in/felis/donec.json?interdum=risus&amp;mauris=dap~ ## 3 https://w3.org/sed/augue/aliquam/erat/volutpat.json?dictumst=mi&amp;morbi=sit&amp;ve~ ## 4 http://indiatimes.com/pede/lobortis/ligula/sit/amet.jpg?quam=nullam&amp;sollicit~ ## 5 https://tumblr.com/id/mauris/vulputate/elementum.png?tincidunt=maecenas&amp;eget~ ## 6 https://unblog.fr/est/quam/pharetra.jpg?amet=phasellus&amp;erat=sit&amp;nulla=amet&amp;t~ ## 7 http://vinaora.com/posuere.jpg?convallis=in&amp;nulla=faucibus&amp;neque=orci&amp;libero~ ## 8 https://globo.com/accumsan.png?elementum=eu&amp;pellentesque=mi&amp;quisque=nulla&amp;po~ ## 9 https://xing.com/elementum/eu/interdum/eu/tincidunt.html?sit=proin&amp;amet=eu&amp;s~ ## 10 https://bigcartel.com/tortor/quis/turpis/sed/ante/vivamus.html?in=lorem&amp;elei~ ## full_name currency passwords ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Mufi Ruit ¥34.37 VybPYpEXUjJh6nQk ## 2 Leese Furmagier $67.37 mxET3n6dz42X8YUv ## 3 Blakelee Wilshire 33,85 Z9f4WeNVQ28FwKML ## 4 Terencio McIllrick 42,89 Ndbm8nwCps6jUze3 ## 5 Debee McErlaine 13,19 U3Lj9xJw8NHzB5Sg ## 6 Fran Painten ¥87.35 KEhVAC3QNvjWDFJ7 ## 7 Frasco Bowich $34.89 jydGPCW7fa2bZpU4 ## 8 Car Ponten ¥41.66 pytVHesNZjAL8WKc ## 9 Tades Checcucci 70,80 Rsw4EQGk9tKTnzDp ## 10 Wilton Kemmey 62,76 KvrNGQ7yL3pfsaZA ## # ... with 990 more rows 8.2.2 Data Dictionary domain: dummy website domain imageurl: url of an image email: dummy email id filename: dummy file name with different extensions phone: dummy phone number address: dummy address with door and street names url: randomyly generated urls full_name: dummy first and last names currency: different currencies passwords: dummy passwords 8.3 Overview Before we start with the case study, let us take a quick tour of stringr and introduce ourselves to some of the functions we will be using later in the case study. One of the columns in the case study data is email. It contains random email ids. We want to ensure that the email ids adher to a particular format .i.e they contain @ they contain only one @ Let us first detect if the email ids contain @. Since the data set has 1000 rows, we will use a smaller sample in the examples. mockdata &lt;- slice(mockstring, 1:10) mockdata ## # A tibble: 10 x 12 ## id ## &lt;dbl&gt; ## 1 1 ## 2 2 ## 3 3 ## 4 4 ## 5 5 ## 6 6 ## 7 7 ## 8 8 ## 9 9 ## 10 10 ## image_url ## &lt;chr&gt; ## 1 https://robohash.org/providentassumendaexplicabo.jpg?size=50x50&amp;set=set1 ## 2 https://robohash.org/etillumvoluptate.jpg?size=50x50&amp;set=set1 ## 3 https://robohash.org/nonoptiovoluptatibus.jpg?size=50x50&amp;set=set1 ## 4 https://robohash.org/voluptatumauthic.jpg?size=50x50&amp;set=set1 ## 5 https://robohash.org/placeaterrorqui.jpg?size=50x50&amp;set=set1 ## 6 https://robohash.org/temporeutea.jpg?size=50x50&amp;set=set1 ## 7 https://robohash.org/maximesaepequi.bmp?size=50x50&amp;set=set1 ## 8 https://robohash.org/nemoautesse.png?size=50x50&amp;set=set1 ## 9 https://robohash.org/odiorerumaut.png?size=50x50&amp;set=set1 ## 10 https://robohash.org/omnismolestiaearchitecto.png?size=50x50&amp;set=set1 ## domain imageurl ## &lt;chr&gt; &lt;chr&gt; ## 1 addtoany.com http://dummyimage.com/130x183.jpg/dddddd/000000 ## 2 gmpg.org http://dummyimage.com/106x217.bmp/dddddd/000000 ## 3 samsung.com http://dummyimage.com/146x127.bmp/cc0000/ffffff ## 4 spotify.com http://dummyimage.com/181x194.png/5fa2dd/ffffff ## 5 wunderground.com http://dummyimage.com/220x123.jpg/ff4444/ffffff ## 6 alexa.com http://dummyimage.com/118x176.bmp/dddddd/000000 ## 7 google.it http://dummyimage.com/185x202.jpg/ff4444/ffffff ## 8 ed.gov http://dummyimage.com/223x163.jpg/ff4444/ffffff ## 9 jigsy.com http://dummyimage.com/145x113.jpg/5fa2dd/ffffff ## 10 jugem.jp http://dummyimage.com/238x214.png/cc0000/ffffff ## email filename phone ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 mnewburn0@fastcompany.com PedeMalesuada.xls 66-(777)902-6181 ## 2 mdankersley1@digg.com LobortisVel.mp3 351-(422)736-6807 ## 3 hgirhard2@altervista.org CongueDiamId.pdf 33-(371)684-5114 ## 4 pmcmenamy3@sciencedirect.com EleifendQuam.avi 86-(410)823-6712 ## 5 drisbrough4@bandcamp.com PurusPhasellus.mp3 223-(518)814-6361 ## 6 cphlippi5@surveymonkey.com ElementumInHac.avi 420-(760)354-8671 ## 7 kdodswell6@un.org Mattis.doc 1-(712)615-2879 ## 8 vhourihane7@ovh.net PurusEu.tiff 62-(437)705-1118 ## 9 rdike8@timesonline.co.uk JustoEtiamPretium.xls 1-(683)965-1323 ## 10 tdudbridge9@clickbank.net Ante.tiff 30-(553)559-7448 ## address ## &lt;chr&gt; ## 1 8 Anhalt Crossing ## 2 697 East Avenue ## 3 89 Dottie Circle ## 4 98135 Blue Bill Park Drive ## 5 7814 Pennsylvania Street ## 6 4897 Little Fleur Drive ## 7 53541 Morrow Center ## 8 4819 Hermina Parkway ## 9 68096 Monument Park ## 10 9595 Spaight Avenue ## url ## &lt;chr&gt; ## 1 https://engadget.com/nascetur/ridiculus/mus/vivamus/vestibulum.jsp?eu=est&amp;ti~ ## 2 http://delicious.com/phasellus/in/felis/donec.json?interdum=risus&amp;mauris=dap~ ## 3 https://w3.org/sed/augue/aliquam/erat/volutpat.json?dictumst=mi&amp;morbi=sit&amp;ve~ ## 4 http://indiatimes.com/pede/lobortis/ligula/sit/amet.jpg?quam=nullam&amp;sollicit~ ## 5 https://tumblr.com/id/mauris/vulputate/elementum.png?tincidunt=maecenas&amp;eget~ ## 6 https://unblog.fr/est/quam/pharetra.jpg?amet=phasellus&amp;erat=sit&amp;nulla=amet&amp;t~ ## 7 http://vinaora.com/posuere.jpg?convallis=in&amp;nulla=faucibus&amp;neque=orci&amp;libero~ ## 8 https://globo.com/accumsan.png?elementum=eu&amp;pellentesque=mi&amp;quisque=nulla&amp;po~ ## 9 https://xing.com/elementum/eu/interdum/eu/tincidunt.html?sit=proin&amp;amet=eu&amp;s~ ## 10 https://bigcartel.com/tortor/quis/turpis/sed/ante/vivamus.html?in=lorem&amp;elei~ ## full_name currency passwords ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Mufi Ruit ¥34.37 VybPYpEXUjJh6nQk ## 2 Leese Furmagier $67.37 mxET3n6dz42X8YUv ## 3 Blakelee Wilshire 33,85 Z9f4WeNVQ28FwKML ## 4 Terencio McIllrick 42,89 Ndbm8nwCps6jUze3 ## 5 Debee McErlaine 13,19 U3Lj9xJw8NHzB5Sg ## 6 Fran Painten ¥87.35 KEhVAC3QNvjWDFJ7 ## 7 Frasco Bowich $34.89 jydGPCW7fa2bZpU4 ## 8 Car Ponten ¥41.66 pytVHesNZjAL8WKc ## 9 Tades Checcucci 70,80 Rsw4EQGk9tKTnzDp ## 10 Wilton Kemmey 62,76 KvrNGQ7yL3pfsaZA Use str_detect() to detect @ and str_count() to count the number of times @ appears in the email ids. # detect @ str_detect(mockdata$email, pattern = &quot;@&quot;) ## [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE # count @ str_count(mockdata$email, pattern = &quot;@&quot;) ## [1] 1 1 1 1 1 1 1 1 1 1 We can use str_c() to concatenate strings. Let us add the string email id: before each email id in the data set. str_c(&quot;email id:&quot;, mockdata$email) ## [1] &quot;email id:mnewburn0@fastcompany.com&quot; ## [2] &quot;email id:mdankersley1@digg.com&quot; ## [3] &quot;email id:hgirhard2@altervista.org&quot; ## [4] &quot;email id:pmcmenamy3@sciencedirect.com&quot; ## [5] &quot;email id:drisbrough4@bandcamp.com&quot; ## [6] &quot;email id:cphlippi5@surveymonkey.com&quot; ## [7] &quot;email id:kdodswell6@un.org&quot; ## [8] &quot;email id:vhourihane7@ovh.net&quot; ## [9] &quot;email id:rdike8@timesonline.co.uk&quot; ## [10] &quot;email id:tdudbridge9@clickbank.net&quot; If we want to split a string into two parts using a particular pattern, we use str_split(). Let us split the domain name and extension from the domain column in the data. The domain name and extension are separated by . and we will use it to split the domain column. Since . is a special character, we will use two slashes to escape the special character. str_split(mockdata$domain, pattern = &quot;\\\\.&quot;) ## [[1]] ## [1] &quot;addtoany&quot; &quot;com&quot; ## ## [[2]] ## [1] &quot;gmpg&quot; &quot;org&quot; ## ## [[3]] ## [1] &quot;samsung&quot; &quot;com&quot; ## ## [[4]] ## [1] &quot;spotify&quot; &quot;com&quot; ## ## [[5]] ## [1] &quot;wunderground&quot; &quot;com&quot; ## ## [[6]] ## [1] &quot;alexa&quot; &quot;com&quot; ## ## [[7]] ## [1] &quot;google&quot; &quot;it&quot; ## ## [[8]] ## [1] &quot;ed&quot; &quot;gov&quot; ## ## [[9]] ## [1] &quot;jigsy&quot; &quot;com&quot; ## ## [[10]] ## [1] &quot;jugem&quot; &quot;jp&quot; We can truncate a string using str_trunc(). The default truncation happens at the beggining of the string but we can truncate the central part or the end of the string as well. str_trunc(mockdata$email, width = 10) ## [1] &quot;mnewbur...&quot; &quot;mdanker...&quot; &quot;hgirhar...&quot; &quot;pmcmena...&quot; &quot;drisbro...&quot; ## [6] &quot;cphlipp...&quot; &quot;kdodswe...&quot; &quot;vhourih...&quot; &quot;rdike8@...&quot; &quot;tdudbri...&quot; str_trunc(mockdata$email, width = 10, side = &quot;left&quot;) ## [1] &quot;...any.com&quot; &quot;...igg.com&quot; &quot;...sta.org&quot; &quot;...ect.com&quot; &quot;...amp.com&quot; ## [6] &quot;...key.com&quot; &quot;...@un.org&quot; &quot;...ovh.net&quot; &quot;...e.co.uk&quot; &quot;...ank.net&quot; str_trunc(mockdata$email, width = 10, side = &quot;center&quot;) ## [1] &quot;mnew...com&quot; &quot;mdan...com&quot; &quot;hgir...org&quot; &quot;pmcm...com&quot; &quot;dris...com&quot; ## [6] &quot;cphl...com&quot; &quot;kdod...org&quot; &quot;vhou...net&quot; &quot;rdik....uk&quot; &quot;tdud...net&quot; Strings can be sorted using str_sort(). Let us quickly sort the emails in both ascending and descending orders. str_sort(mockdata$email) ## [1] &quot;cphlippi5@surveymonkey.com&quot; &quot;drisbrough4@bandcamp.com&quot; ## [3] &quot;hgirhard2@altervista.org&quot; &quot;kdodswell6@un.org&quot; ## [5] &quot;mdankersley1@digg.com&quot; &quot;mnewburn0@fastcompany.com&quot; ## [7] &quot;pmcmenamy3@sciencedirect.com&quot; &quot;rdike8@timesonline.co.uk&quot; ## [9] &quot;tdudbridge9@clickbank.net&quot; &quot;vhourihane7@ovh.net&quot; str_sort(mockdata$email, decreasing = TRUE) ## [1] &quot;vhourihane7@ovh.net&quot; &quot;tdudbridge9@clickbank.net&quot; ## [3] &quot;rdike8@timesonline.co.uk&quot; &quot;pmcmenamy3@sciencedirect.com&quot; ## [5] &quot;mnewburn0@fastcompany.com&quot; &quot;mdankersley1@digg.com&quot; ## [7] &quot;kdodswell6@un.org&quot; &quot;hgirhard2@altervista.org&quot; ## [9] &quot;drisbrough4@bandcamp.com&quot; &quot;cphlippi5@surveymonkey.com&quot; The case of a string can be changed to upper, lower or title case as shown below. str_to_upper(mockdata$full_name) ## [1] &quot;MUFI RUIT&quot; &quot;LEESE FURMAGIER&quot; &quot;BLAKELEE WILSHIRE&quot; ## [4] &quot;TERENCIO MCILLRICK&quot; &quot;DEBEE MCERLAINE&quot; &quot;FRAN PAINTEN&quot; ## [7] &quot;FRASCO BOWICH&quot; &quot;CAR PONTEN&quot; &quot;TADES CHECCUCCI&quot; ## [10] &quot;WILTON KEMMEY&quot; str_to_lower(mockdata$full_name) ## [1] &quot;mufi ruit&quot; &quot;leese furmagier&quot; &quot;blakelee wilshire&quot; ## [4] &quot;terencio mcillrick&quot; &quot;debee mcerlaine&quot; &quot;fran painten&quot; ## [7] &quot;frasco bowich&quot; &quot;car ponten&quot; &quot;tades checcucci&quot; ## [10] &quot;wilton kemmey&quot; Parts of a string can be replaced using str_replace(). In the address column of the data set, let us replace: Street with ST Road with RD str_replace(mockdata$address, &quot;Street&quot;, &quot;ST&quot;) ## [1] &quot;8 Anhalt Crossing&quot; &quot;697 East Avenue&quot; ## [3] &quot;89 Dottie Circle&quot; &quot;98135 Blue Bill Park Drive&quot; ## [5] &quot;7814 Pennsylvania ST&quot; &quot;4897 Little Fleur Drive&quot; ## [7] &quot;53541 Morrow Center&quot; &quot;4819 Hermina Parkway&quot; ## [9] &quot;68096 Monument Park&quot; &quot;9595 Spaight Avenue&quot; str_replace(mockdata$address, &quot;Road&quot;, &quot;RD&quot;) ## [1] &quot;8 Anhalt Crossing&quot; &quot;697 East Avenue&quot; ## [3] &quot;89 Dottie Circle&quot; &quot;98135 Blue Bill Park Drive&quot; ## [5] &quot;7814 Pennsylvania Street&quot; &quot;4897 Little Fleur Drive&quot; ## [7] &quot;53541 Morrow Center&quot; &quot;4819 Hermina Parkway&quot; ## [9] &quot;68096 Monument Park&quot; &quot;9595 Spaight Avenue&quot; We can extract parts of the string that match a particular pattern using str_extract(). str_extract(mockdata$email, pattern = &quot;org&quot;) ## [1] NA NA &quot;org&quot; NA NA NA &quot;org&quot; NA NA NA Before we extract, we need to know whether the string contains text that match our pattern. Use str_match() to see if the pattern is present in the string. str_match(mockdata$email, pattern = &quot;org&quot;) ## [,1] ## [1,] NA ## [2,] NA ## [3,] &quot;org&quot; ## [4,] NA ## [5,] NA ## [6,] NA ## [7,] &quot;org&quot; ## [8,] NA ## [9,] NA ## [10,] NA If we are dealing with a character vector and know that the pattern we are looking at is present in the vector, we might want to know the index of the strings in which it is present. Use str_which() to identify the index of the strings that match our pattern. str_which(mockdata$email, pattern = &quot;org&quot;) ## [1] 3 7 Another objective might be to locate the position of the pattern we are looking for in the string. For example, if we want to know the position of @ in the email ids, we can use str_locate(). str_locate(mockdata$email, pattern = &quot;@&quot;) ## start end ## [1,] 10 10 ## [2,] 13 13 ## [3,] 10 10 ## [4,] 11 11 ## [5,] 12 12 ## [6,] 10 10 ## [7,] 11 11 ## [8,] 12 12 ## [9,] 7 7 ## [10,] 12 12 The length of the string can be computed using str_length(). Let us ensure that the length of the strings in the password column is 16. str_length(mockdata$passwords) ## [1] 16 16 16 16 16 16 16 16 16 16 We can extract parts of a string by specifying the starting and ending position using str_sub(). Let us extract the currency type from the currency column. str_sub(mockdata$currency, start = 1, end = 1) ## [1] &quot;¥&quot; &quot;$&quot; &quot;\\200&quot; &quot;\\200&quot; &quot;\\200&quot; &quot;¥&quot; &quot;$&quot; &quot;¥&quot; &quot;\\200&quot; &quot;\\200&quot; One final function that we will look at before the case study is word(). It extracts word(s) from sentences. We do not have any sentences in the data set, but let us use it to extract the first and last name from the full_name column. word(mockdata$full_name, 1) ## [1] &quot;Mufi&quot; &quot;Leese&quot; &quot;Blakelee&quot; &quot;Terencio&quot; &quot;Debee&quot; &quot;Fran&quot; ## [7] &quot;Frasco&quot; &quot;Car&quot; &quot;Tades&quot; &quot;Wilton&quot; word(mockdata$full_name, 2) ## [1] &quot;Ruit&quot; &quot;Furmagier&quot; &quot;Wilshire&quot; &quot;McIllrick&quot; &quot;McErlaine&quot; &quot;Painten&quot; ## [7] &quot;Bowich&quot; &quot;Ponten&quot; &quot;Checcucci&quot; &quot;Kemmey&quot; Alright, now let us apply what we have learned so far to our case study. 8.4 Extract domain name from email ids 8.4.1 Steps split email using pattern @ extract the second element from the resulting list split the above using pattern \\\\. extract the first element from the resulting list Let us take a look at the emails before we extract the domain names. emails &lt;- mockstring %&gt;% pull(email) %&gt;% head() emails ## [1] &quot;mnewburn0@fastcompany.com&quot; &quot;mdankersley1@digg.com&quot; ## [3] &quot;hgirhard2@altervista.org&quot; &quot;pmcmenamy3@sciencedirect.com&quot; ## [5] &quot;drisbrough4@bandcamp.com&quot; &quot;cphlippi5@surveymonkey.com&quot; 8.4.1.1 Step 1: Split email using pattern @. We will split the email using str_split. It will split a string using the pattern supplied. In our case the pattern is @. str_split(emails, pattern = &#39;@&#39;) ## [[1]] ## [1] &quot;mnewburn0&quot; &quot;fastcompany.com&quot; ## ## [[2]] ## [1] &quot;mdankersley1&quot; &quot;digg.com&quot; ## ## [[3]] ## [1] &quot;hgirhard2&quot; &quot;altervista.org&quot; ## ## [[4]] ## [1] &quot;pmcmenamy3&quot; &quot;sciencedirect.com&quot; ## ## [[5]] ## [1] &quot;drisbrough4&quot; &quot;bandcamp.com&quot; ## ## [[6]] ## [1] &quot;cphlippi5&quot; &quot;surveymonkey.com&quot; 8.4.1.2 Step 2: Extract the second element from the resulting list. Step 1 returned a list. Each element of the list has two values. The first one is the username and the second is the domain name. Since we are extracting the domain name, we want the second value from each element of the list. We will use map_chr() from purrr to extract the domain names. It will return the second value from each element in the list. Since the domain name is a string, map_chr() will return a character vector. emails %&gt;% str_split(pattern = &#39;@&#39;) %&gt;% map_chr(2) ## [1] &quot;fastcompany.com&quot; &quot;digg.com&quot; &quot;altervista.org&quot; ## [4] &quot;sciencedirect.com&quot; &quot;bandcamp.com&quot; &quot;surveymonkey.com&quot; 8.4.1.3 Step 3: Split the above using pattern \\\\.. We want the domain name and not the extension. Step 2 returned a character vector and we need to split the domain name and the domain extension. They are separated by .. Since . is a special character, we will use \\\\ before . to escape it. Let us split the domain name and domain extension using str_split and \\\\. as the pattern. emails %&gt;% str_split(pattern = &#39;@&#39;) %&gt;% map_chr(2) %&gt;% str_split(pattern = &#39;\\\\.&#39;) ## [[1]] ## [1] &quot;fastcompany&quot; &quot;com&quot; ## ## [[2]] ## [1] &quot;digg&quot; &quot;com&quot; ## ## [[3]] ## [1] &quot;altervista&quot; &quot;org&quot; ## ## [[4]] ## [1] &quot;sciencedirect&quot; &quot;com&quot; ## ## [[5]] ## [1] &quot;bandcamp&quot; &quot;com&quot; ## ## [[6]] ## [1] &quot;surveymonkey&quot; &quot;com&quot; 8.4.1.4 Step 4: Extract the first element from the resulting list. Now that we have separated the domain name from its extension, let us extract the first value from each element in the list returned in step 3. We will again use map_chr to achieve this. emails %&gt;% str_split(pattern = &#39;@&#39;) %&gt;% map_chr(2) %&gt;% str_split(pattern = &#39;\\\\.&#39;) %&gt;% map_chr(extract(1)) ## [1] &quot;fastcompany&quot; &quot;digg&quot; &quot;altervista&quot; &quot;sciencedirect&quot; ## [5] &quot;bandcamp&quot; &quot;surveymonkey&quot; 8.5 Extract Domain Extension The below code extracts the domain extension instead of the domain name. emails %&gt;% str_split(pattern = &#39;@&#39;) %&gt;% map_chr(2) %&gt;% str_split(pattern = &#39;\\\\.&#39;, simplify = TRUE) %&gt;% extract(, 2) ## [1] &quot;com&quot; &quot;com&quot; &quot;org&quot; &quot;com&quot; &quot;com&quot; &quot;com&quot; 8.6 Extract image type from URL 8.6.1 Steps split imageurl using pattern \\\\. extract the third value from each element of the resulting list subset the string using the index position Let us take a look at the URL of the image. img &lt;- mockstring %&gt;% pull(imageurl) %&gt;% head() img ## [1] &quot;http://dummyimage.com/130x183.jpg/dddddd/000000&quot; ## [2] &quot;http://dummyimage.com/106x217.bmp/dddddd/000000&quot; ## [3] &quot;http://dummyimage.com/146x127.bmp/cc0000/ffffff&quot; ## [4] &quot;http://dummyimage.com/181x194.png/5fa2dd/ffffff&quot; ## [5] &quot;http://dummyimage.com/220x123.jpg/ff4444/ffffff&quot; ## [6] &quot;http://dummyimage.com/118x176.bmp/dddddd/000000&quot; 8.6.1.1 Step 1: Split imageurl using pattern \\\\. Let us split imageurl using str_split and the pattern \\\\.. str_split(img, pattern = &#39;\\\\.&#39;) ## [[1]] ## [1] &quot;http://dummyimage&quot; &quot;com/130x183&quot; &quot;jpg/dddddd/000000&quot; ## ## [[2]] ## [1] &quot;http://dummyimage&quot; &quot;com/106x217&quot; &quot;bmp/dddddd/000000&quot; ## ## [[3]] ## [1] &quot;http://dummyimage&quot; &quot;com/146x127&quot; &quot;bmp/cc0000/ffffff&quot; ## ## [[4]] ## [1] &quot;http://dummyimage&quot; &quot;com/181x194&quot; &quot;png/5fa2dd/ffffff&quot; ## ## [[5]] ## [1] &quot;http://dummyimage&quot; &quot;com/220x123&quot; &quot;jpg/ff4444/ffffff&quot; ## ## [[6]] ## [1] &quot;http://dummyimage&quot; &quot;com/118x176&quot; &quot;bmp/dddddd/000000&quot; 8.6.1.2 Step 2: Extract the third value from each element of the resulting list Step 1 returned a list the elements of which have 3 values each. If you observe the list, the image type is in the 3rd value. We will now extract the third value from each element of the list using map_chr. img %&gt;% str_split(pattern = &#39;\\\\.&#39;) %&gt;% map_chr(extract(3)) ## [1] &quot;jpg/dddddd/000000&quot; &quot;bmp/dddddd/000000&quot; &quot;bmp/cc0000/ffffff&quot; ## [4] &quot;png/5fa2dd/ffffff&quot; &quot;jpg/ff4444/ffffff&quot; &quot;bmp/dddddd/000000&quot; 8.6.1.3 Step 3: Subset the string using the index position We can now extract the image type in two ways: subset the first 3 characters of the string split the string using pattern / and extract the first value from the elements of the resulting list Below is the first method. We know that the image type is 3 characters. So we use str_sub to subset the first 3 characters. The index positions are mentioned using start and stop. img %&gt;% str_split(pattern = &#39;\\\\.&#39;) %&gt;% map_chr(extract(3)) %&gt;% str_sub(start = 1, end = 3) ## [1] &quot;jpg&quot; &quot;bmp&quot; &quot;bmp&quot; &quot;png&quot; &quot;jpg&quot; &quot;bmp&quot; In case you are not sure about the length of the image type. In such cases, we will split the string using pattern / and then use map_chr to extract the first value of each element of the resulting list. img %&gt;% str_split(pattern = &#39;\\\\.&#39;) %&gt;% map_chr(extract(3)) %&gt;% str_split(pattern = &#39;/&#39;) %&gt;% map_chr(extract(1)) ## [1] &quot;jpg&quot; &quot;bmp&quot; &quot;bmp&quot; &quot;png&quot; &quot;jpg&quot; &quot;bmp&quot; 8.7 Extract Image Dimesion from URL 8.7.1 Steps locate numbers between 0 and 9 extract part of url starting with image dimension split the string using the pattern \\\\. extract the first element 8.7.1.1 Step 1: Locate numbers between 0 and 9. Let us inspect the image url. The dimension of the image appears after the domain extension and there are no numbers in the url before. We will locate the position or index of the first number in the url using str_locate() and using the pattern [0-9] which instructs to look for any number between and including 0 and 9. str_locate(img, pattern = &quot;[0-9]&quot;) ## start end ## [1,] 23 23 ## [2,] 23 23 ## [3,] 23 23 ## [4,] 23 23 ## [5,] 23 23 ## [6,] 23 23 8.7.1.2 Step 2: Extract url We know where the dimension is located in the url. Let us extract the part of the url that contains the image dimension using str_sub(). str_sub(img, start = 23) ## [1] &quot;130x183.jpg/dddddd/000000&quot; &quot;106x217.bmp/dddddd/000000&quot; ## [3] &quot;146x127.bmp/cc0000/ffffff&quot; &quot;181x194.png/5fa2dd/ffffff&quot; ## [5] &quot;220x123.jpg/ff4444/ffffff&quot; &quot;118x176.bmp/dddddd/000000&quot; 8.7.1.3 Step 3: Split the string using the pattern \\\\.. From the previous step, we have the part of the url that contains the image dimension. To extract the dimension, we will split it from the rest of the url using str_split() and using the pattern \\\\. as it separates the dimension and the image extension. img %&gt;% str_sub(start = 23) %&gt;% str_split(pattern = &#39;\\\\.&#39;) ## [[1]] ## [1] &quot;130x183&quot; &quot;jpg/dddddd/000000&quot; ## ## [[2]] ## [1] &quot;106x217&quot; &quot;bmp/dddddd/000000&quot; ## ## [[3]] ## [1] &quot;146x127&quot; &quot;bmp/cc0000/ffffff&quot; ## ## [[4]] ## [1] &quot;181x194&quot; &quot;png/5fa2dd/ffffff&quot; ## ## [[5]] ## [1] &quot;220x123&quot; &quot;jpg/ff4444/ffffff&quot; ## ## [[6]] ## [1] &quot;118x176&quot; &quot;bmp/dddddd/000000&quot; 8.7.1.4 Step 4: Extract the first element. The above step resulted in a list which contains the image dimension and the rest of the url. Each element of the list is a character vector. We want to extract the first value in the character vector. Let us use map_chr() to extract the first value from each element of the list. img %&gt;% str_sub(start = 23) %&gt;% str_split(pattern = &#39;\\\\.&#39;) %&gt;% map_chr(extract(1)) ## [1] &quot;130x183&quot; &quot;106x217&quot; &quot;146x127&quot; &quot;181x194&quot; &quot;220x123&quot; &quot;118x176&quot; 8.8 Extract HTTP Protocol from URL url1 &lt;- mockstring %&gt;% pull(url) %&gt;% first() url1 ## [1] &quot;https://engadget.com/nascetur/ridiculus/mus/vivamus/vestibulum.jsp?eu=est&amp;tincidunt=risus&amp;in=auctor&amp;leo=sed&amp;maecenas=tristique&amp;pulvinar=in&amp;lobortis=tempus&amp;est=sit&amp;phasellus=amet&amp;sit=sem&amp;amet=fusce&amp;erat=consequat&amp;nulla=nulla&amp;tempus=nisl&amp;vivamus=nunc&amp;in=nisl&amp;felis=duis&amp;eu=bibendum&amp;sapien=felis&amp;cursus=sed&amp;vestibulum=interdum&amp;proin=venenatis&amp;eu=turpis&amp;mi=enim&amp;nulla=blandit&amp;ac=mi&amp;enim=in&amp;in=porttitor&amp;tempor=pede&amp;turpis=justo&amp;nec=eu&amp;euismod=massa&amp;scelerisque=donec&amp;quam=dapibus&amp;turpis=duis&amp;adipiscing=at&amp;lorem=velit&amp;vitae=eu&amp;mattis=est&amp;nibh=congue&amp;ligula=elementum&amp;nec=in&amp;sem=hac&amp;duis=habitasse&amp;aliquam=platea&amp;convallis=dictumst&amp;nunc=morbi&amp;proin=vestibulum&amp;at=velit&amp;turpis=id&amp;a=pretium&amp;pede=iaculis&amp;posuere=diam&amp;nonummy=erat&amp;integer=fermentum&amp;non=justo&amp;velit=nec&amp;donec=condimentum&amp;diam=neque&amp;neque=sapien&amp;vestibulum=placerat&amp;eget=ante&amp;vulputate=nulla&amp;ut=justo&amp;ultrices=aliquam&amp;vel=quis&amp;augue=turpis&amp;vestibulum=eget&amp;ante=elit&amp;ipsum=sodales&amp;primis=scelerisque&amp;in=mauris&amp;faucibus=sit&amp;orci=amet&amp;luctus=eros&amp;et=suspendisse&amp;ultrices=accumsan&amp;posuere=tortor&amp;cubilia=quis&amp;curae=turpis&amp;donec=sed&amp;pharetra=ante&amp;magna=vivamus&amp;vestibulum=tortor&amp;aliquet=duis&amp;ultrices=mattis&amp;erat=egestas&amp;tortor=metus&amp;sollicitudin=aenean&amp;mi=fermentum&amp;sit=donec&quot; 8.8.1 Steps split the url using the pattern :// extract the first element 8.8.1.1 Step 1: Split the url using the pattern ://. The HTTP protocol is the first part of the url and is separated from the rest of the url by :. Let us split the url using str_split() and using the pattern :. Since : is a special character, we will escape it using \\\\. str_split(url1, pattern = &#39;://&#39;) ## [[1]] ## [1] &quot;https&quot; ## [2] &quot;engadget.com/nascetur/ridiculus/mus/vivamus/vestibulum.jsp?eu=est&amp;tincidunt=risus&amp;in=auctor&amp;leo=sed&amp;maecenas=tristique&amp;pulvinar=in&amp;lobortis=tempus&amp;est=sit&amp;phasellus=amet&amp;sit=sem&amp;amet=fusce&amp;erat=consequat&amp;nulla=nulla&amp;tempus=nisl&amp;vivamus=nunc&amp;in=nisl&amp;felis=duis&amp;eu=bibendum&amp;sapien=felis&amp;cursus=sed&amp;vestibulum=interdum&amp;proin=venenatis&amp;eu=turpis&amp;mi=enim&amp;nulla=blandit&amp;ac=mi&amp;enim=in&amp;in=porttitor&amp;tempor=pede&amp;turpis=justo&amp;nec=eu&amp;euismod=massa&amp;scelerisque=donec&amp;quam=dapibus&amp;turpis=duis&amp;adipiscing=at&amp;lorem=velit&amp;vitae=eu&amp;mattis=est&amp;nibh=congue&amp;ligula=elementum&amp;nec=in&amp;sem=hac&amp;duis=habitasse&amp;aliquam=platea&amp;convallis=dictumst&amp;nunc=morbi&amp;proin=vestibulum&amp;at=velit&amp;turpis=id&amp;a=pretium&amp;pede=iaculis&amp;posuere=diam&amp;nonummy=erat&amp;integer=fermentum&amp;non=justo&amp;velit=nec&amp;donec=condimentum&amp;diam=neque&amp;neque=sapien&amp;vestibulum=placerat&amp;eget=ante&amp;vulputate=nulla&amp;ut=justo&amp;ultrices=aliquam&amp;vel=quis&amp;augue=turpis&amp;vestibulum=eget&amp;ante=elit&amp;ipsum=sodales&amp;primis=scelerisque&amp;in=mauris&amp;faucibus=sit&amp;orci=amet&amp;luctus=eros&amp;et=suspendisse&amp;ultrices=accumsan&amp;posuere=tortor&amp;cubilia=quis&amp;curae=turpis&amp;donec=sed&amp;pharetra=ante&amp;magna=vivamus&amp;vestibulum=tortor&amp;aliquet=duis&amp;ultrices=mattis&amp;erat=egestas&amp;tortor=metus&amp;sollicitudin=aenean&amp;mi=fermentum&amp;sit=donec&quot; 8.8.1.2 Step 2: Extract the first element. The HTTP protocol is the first value in each element of the list. As we did in the previous example, we will extact it using map_chr() and extract(). url1 %&gt;% str_split(pattern = &#39;://&#39;) %&gt;% map_chr(extract(1)) ## [1] &quot;https&quot; 8.9 Extract file type urls &lt;- mockstring %&gt;% use_series(url) %&gt;% extract(1:3) 8.9.1 Steps check if there are only 2 dots in the URL check if there is only 1 question mark in the URL detect the staritng position of file type tetect the ending position of file type use the locations to specify the index position for extracting file type 8.9.1.1 Step 1: Check if there are only 2 dots in the URL Let us locate all the dots in the url using str_locate_all() and see if any of them contain more than 2 dots. urls %&gt;% str_locate_all(pattern = &#39;\\\\.&#39;) %&gt;% map_int(nrow) %&gt;% is_greater_than(2) %&gt;% sum() ## [1] 0 8.9.1.2 Step 2: Check if there is only 1 question mark in the URL The next step is to check if there is only one ? (question mark) in the url. urls %&gt;% str_locate_all(pattern = &quot;[?]&quot;) %&gt;% map_int(nrow) %&gt;% is_greater_than(1) %&gt;% sum() ## [1] 0 8.9.1.3 Step 3: Detect the staritng position of file type Since the file type is located between the second dot and the first quesiton mark in the url, let us extract the location of the second dot and add 1 as the file type starts after the dot. d &lt;- urls %&gt;% str_locate_all(pattern = &#39;\\\\.&#39;) %&gt;% map_int(extract(2)) %&gt;% add(1) d ## [1] 64 47 48 8.9.1.4 Step 4: Detect the ending position of file type In step 2, we confirmed that the url has only one question mark. Let us locate the question mark in the url and subtract 1 (as the file type ends before the question mark) so that we get the ending chapterion of the file type. . q &lt;- urls %&gt;% str_locate_all(pattern = &quot;[?]&quot;) %&gt;% map_int(extract(1)) %&gt;% subtract(1) q ## [1] 66 50 51 8.9.1.5 Step 5: Specify the index position for extracting file type From steps 3 and 4, we have the location of the second dot and the first question mark in the url. Let us use them with str_sub() to extract the file type. str_sub(urls, start = d, end = q) ## [1] &quot;jsp&quot; &quot;json&quot; &quot;json&quot; "],["date-and-time-in-r.html", "Chapter 9 Date &amp; Time 9.1 Introduction 9.2 Case Study 9.3 Date &amp; Time Classes 9.4 Date Arithmetic 9.5 Time Zones 9.6 Date &amp; Time Formats 9.7 Parse Date &amp; Time 9.8 Date &amp; Time Components 9.9 Create, Update &amp; Verify 9.10 Intervals, Duration &amp; Period 9.11 Others", " Chapter 9 Date &amp; Time 9.1 Introduction Let us begin by looking at the current date and time. 9.1.1 Date Sys.Date() and today() will return the current date. Sys.Date() ## [1] &quot;2021-06-02&quot; lubridate::today() ## [1] &quot;2021-06-02&quot; 9.1.2 Time Sys.time() and now() return the date, time and the timezone. In now(), we can specify the timezone using the tzone argument. Sys.time() ## [1] &quot;2021-06-02 16:58:59 IST&quot; lubridate::now() ## [1] &quot;2021-06-02 16:58:59 IST&quot; lubridate::now(tzone = &quot;UTC&quot;) ## [1] &quot;2021-06-02 11:28:59 UTC&quot; 9.1.3 AM or PM? am() and pm() allow us to check whether date/time occur in the am or pm? They return a logical value i.e. TRUE or FALSE lubridate::am(now()) ## [1] FALSE lubridate::pm(now()) ## [1] TRUE 9.1.4 Leap Year We can also check if the current year is a leap year using leap_year(). lubridate::leap_year(Sys.Date()) ## [1] FALSE 9.1.5 Summary Function Description Sys.Date() Current Date lubridate::today() Current Date Sys.time() Current Time lubridate::now() Current Time lubridate::am() Whether time occurs in am? lubridate::pm() Whether time occurs in pm? lubridate::leap_year() Check if the year is a leap year? 9.1.6 Your Turn get current date get current time check whether the time occurs in am or pm? check whether the following years were leap years 2018 2016 9.2 Case Study Throughout the tutorial, we will work on a case study related to transactions of a imaginary company. The data set includes information about invoice and payment dates. 9.2.1 Data transact &lt;- readr::read_csv(&#39;https://raw.githubusercontent.com/rsquaredacademy/datasets/master/transact.csv&#39;) ## # A tibble: 2,466 x 3 ## Invoice Due Payment ## &lt;date&gt; &lt;date&gt; &lt;date&gt; ## 1 2013-01-02 2013-02-01 2013-01-15 ## 2 2013-01-26 2013-02-25 2013-03-03 ## 3 2013-07-03 2013-08-02 2013-07-08 ## 4 2013-02-10 2013-03-12 2013-03-17 ## 5 2012-10-25 2012-11-24 2012-11-28 ## 6 2012-01-27 2012-02-26 2012-02-22 ## 7 2013-08-13 2013-09-12 2013-09-09 ## 8 2012-12-16 2013-01-15 2013-01-12 ## 9 2012-05-14 2012-06-13 2012-07-01 ## 10 2013-07-01 2013-07-31 2013-07-26 ## # ... with 2,456 more rows 9.2.2 Data Dictionary The data set has 3 columns. All the dates are in the format (yyyy-mm-dd). Column Description Invoice Invoice Date Due Due Date Payment Payment Date In the case study, we will try to answer a few questions we have about the transact data. extract date, month and year from Due compute the number of days to settle invoice compute days over due check if due year is a leap year check when due day in february is 29, whether it is a leap year how many invoices were settled within due date how many invoices are due in each quarter 9.3 Date &amp; Time Classes 9.3.1 Introduction In this section, we will look at two things. First, how to create date/time data in R, and second, how to convert other data types to date/time. Let us begin by creating the latest R release date manually. release_date &lt;- 2019-12-12 release_date ## [1] 1995 Okay! Why do we see 1995 when we call the date? What is happening here? Let us quickly check the data type of release_date. class(release_date) ## [1] &quot;numeric&quot; The data type is numeric i.e. R has subtracted 12 twice from 2019 to return 1995. Clearly, the above method is not the right way to store date/time. Let us see if we can get some hints from the builtin R functions we used in the previous section. If you observe the output, all of them returned date/time wrapped in quotes. Hmmm let us wrap our date in quotes and see what happens. release_date &lt;- &quot;2019-12-12&quot; release_date ## [1] &quot;2019-12-12&quot; Alright, now R does not do any arithmetic and returns the date as we specified. Great! Is this the right format to store date/time? No. Why? What is the problem if date/time is saved as character/string? The problem is the nature or type of operations done on date or time is different when compared to string/character, number or logical values. how do we add/subtract dates? how do we extract components such as year, month, day etc. To answer the above questions, we will first check the data type of Sys.Date() and now(). class(Sys.Date()) ## [1] &quot;Date&quot; class(lubridate::now()) ## [1] &quot;POSIXct&quot; &quot;POSIXt&quot; class(release_date) ## [1] &quot;character&quot; As you can see from the above output, there are 3 different classes for storing date/time in R Date POSIXct POSIXlt Let us explore each of the above classes one by one. 9.3.2 Date 9.3.2.1 Introduction The Date class represents calendar dates. Let us go back to Sys.Date(). If you check the class of Sys.Date(), it is Date. Internally, this date is a number i.e. an integer. The unclass() function will show dates are stored internally. unclass(Sys.Date()) ## [1] 18780 What does this integer represent? Why has R stored the date as an integer? Before we answer this question, we need to know something else. In R, dates are represented as the number of days since 1970-01-01. All the dates in R are internally stored in this way. Before we explore this concept further, let us learn to create Date objects in R. We will continue to use the latest R release date, 2019-12-12. Until now, we have stored the above date as character/string but now we will use as.Date() to save it as a Date object. as.Date() is the easiest and simplest way to create dates in R. release_date &lt;- as.Date(&quot;2019-12-12&quot;) release_date ## [1] &quot;2019-12-12&quot; The as_date() function from the lubridate package is similar to as.Date(). release_date &lt;- lubridate::as_date(&quot;2019-12-12&quot;) release_date ## [1] &quot;2019-12-12&quot; If you look at the difference between release_date and 1970-01-01, it will be the same as unclass(release_date). release_date - as.Date(&quot;1970-01-01&quot;) ## Time difference of 18242 days unclass(release_date) ## [1] 18242 Let us come back to 1970-01-01 i.e. the origin for dates in R. lubridate::origin ## [1] &quot;1970-01-01 UTC&quot; From the previous examples, we know that dates are internally stored as number of days since 1970-01-01. How about dates older than the origin? How are they stored? Let us look at that briefly. unclass(as.Date(&quot;1963-08-28&quot;)) ## [1] -2318 Dates older than the origin are stored as negative integers. For those who are not aware, Martin Luther King, Jr. delivered his famous I Have a Dream speech on 1963-08-28. Let us move on and learn how to convert numbers into dates. 9.3.2.2 Convert Numeric The as.Date() function can be used to convert any of the following to a Date object character/string number factor (categorical/qualitative) We have explored how to convert strings to date. How about converting numbers to date? Sure, we can create date from numbers by specifying the origin and number of days since it. as.Date(18242, origin = &quot;1970-01-01&quot;) ## [1] &quot;2019-12-12&quot; The origin can be changed to another date (while changing the number as well.) as.Date(7285, origin = &quot;2000-01-01&quot;) ## [1] &quot;2019-12-12&quot; 9.3.3 ISO 8601 If you have carefully observed, the format in which we have been specifying the dates as well as of those returned by functions such as Sys.Date() or Sys.time() is the same i.e. YYYY-MM-DD. It includes the year including the century the month the date The month and date separated by -. This default format used in R is the ISO 8601 standard for date/time. ISO 8601 is the internationally accepted way to represent dates and times and uses the 24 hour clock system. Let us create the release date using another function ISOdate(). ISOdate(year = 2019, month = 12, day = 12, hour = 8, min = 5, sec = 3, tz = &quot;UTC&quot;) ## [1] &quot;2019-12-12 08:05:03 UTC&quot; We will look at all the different weird ways in which date/time are specified in the real world in the Date &amp; Time Formats section. For the time being, let us continue exploring date/time classes in R. The next class we are going to look at is POSIXct/POSIXlt. 9.3.4 POSIX You might be wondering what is this POSIX thing? POSIX stands for Portable Operating System Interface. It is a family of standards specified f or maintaining compatibility between different operating systems. Before we learn to create POSIX objects, let us look at now() from lubridate. class(lubridate::now()) ## [1] &quot;POSIXct&quot; &quot;POSIXt&quot; now() returns current date/time as a POSIXct object. Let us look at its internal representation using unclass() unclass(lubridate::now()) ## [1] 1622633347 ## attr(,&quot;tzone&quot;) ## [1] &quot;&quot; The output you see is the number of seconds since January 1, 1970. 9.3.4.1 POSIXct POSIXct represents the number of seconds since the beginning of 1970 (UTC) and ct stands for calendar time. To store date/time as POSIXct objects, use as.POSIXct(). Let us now store the latest R release date as POSIXct as shown below release_date &lt;- as.POSIXct(&quot;2019-12-12 08:05:03&quot;) class(release_date) ## [1] &quot;POSIXct&quot; &quot;POSIXt&quot; unclass(release_date) ## [1] 1576118103 ## attr(,&quot;tzone&quot;) ## [1] &quot;&quot; 9.3.4.2 POSIXlt POSIXlt represents the following information in a list seconds minutes hour day of the month month year day of week day of year daylight saving time flag time zone offset in seconds from GMT The lt in POSIXlt stands for local time. Use as.POSIXlt() to store date/time as POSIXlt objects. Let us store the release date as a POSIXlt object as shown below release_date &lt;- as.POSIXlt(&quot;2019-12-12 08:05:03&quot;) release_date ## [1] &quot;2019-12-12 08:05:03 IST&quot; As we said earlier, POSIXlt stores date/time components in a list and these can be extracted. Let us look at the date/time components returned by POSIXlt using unclass(). release_date &lt;- as.POSIXlt(&quot;2019-12-12 08:05:03&quot;) unclass(release_date) ## $sec ## [1] 3 ## ## $min ## [1] 5 ## ## $hour ## [1] 8 ## ## $mday ## [1] 12 ## ## $mon ## [1] 11 ## ## $year ## [1] 119 ## ## $wday ## [1] 4 ## ## $yday ## [1] 345 ## ## $isdst ## [1] 0 ## ## $zone ## [1] &quot;IST&quot; ## ## $gmtoff ## [1] NA Use unlist() if you want the components returned as a vector. release_date &lt;- as.POSIXlt(&quot;2019-12-12 08:05:03&quot;) unlist(release_date) ## sec min hour mday mon year wday yday isdst zone gmtoff ## &quot;3&quot; &quot;5&quot; &quot;8&quot; &quot;12&quot; &quot;11&quot; &quot;119&quot; &quot;4&quot; &quot;345&quot; &quot;0&quot; &quot;IST&quot; NA To extract specific components, use $. release_date &lt;- as.POSIXlt(&quot;2019-12-12 08:05:03&quot;) release_date$hour ## [1] 8 release_date$mon ## [1] 11 release_date$zone ## [1] &quot;IST&quot; Now, let us look at the components returned by POSIXlt. Some of them are intuitive Component Description sec Second min Minute hour Hour of the day mon Month of the year (0-11 zone Timezone wday Day of week mday Day of month year Years since 1900 yday Day of year isdst Daylight saving flag gmtoff Offset is seconds from GMT Great! We will end this section with a few tips/suggestions on when to use Date or POSIXct/POSIXlt. use Date when there is no time component use POSIX when dealing with time and timezones use POSIXlt when you want to access/extract the different components 9.3.5 Your Turn R 1.0.0 was released on 2000-02-29 08:55:23 UTC. Save it as Date using character Date using origin and number POSIXct POSIXlt and extract month day day of year month zone ISODate 9.4 Date Arithmetic 9.4.1 Introduction Time to do some arithmetic with the dates. Let us calculate the length of a course you have enrolled for (Become a Rock Star Data Scientist in 10 Days) by subtracting the course start date from the course end date. course_start &lt;- as_date(&#39;2017-04-12&#39;) course_end &lt;- as_date(&#39;2017-04-21&#39;) course_duration &lt;- course_end - course_start course_duration ## Time difference of 9 days 9.4.2 Shift Date Time to shift the course dates. We can shift a date by days, weeks or months. Let us shift the course start date by: 2 days 3 weeks 1 year course_start + days(2) ## [1] &quot;2017-04-14&quot; course_start + weeks(3) ## [1] &quot;2017-05-03&quot; course_start + years(1) ## [1] &quot;2018-04-12&quot; 9.4.3 Case Study 9.4.3.1 Compute days to settle invoice Let us estimate the number of days to settle the invoice by subtracting the date of invoice from the date of payment. transact %&gt;% mutate( days_to_pay = Payment - Invoice ) ## # A tibble: 2,466 x 4 ## Invoice Due Payment days_to_pay ## &lt;date&gt; &lt;date&gt; &lt;date&gt; &lt;drtn&gt; ## 1 2013-01-02 2013-02-01 2013-01-15 13 days ## 2 2013-01-26 2013-02-25 2013-03-03 36 days ## 3 2013-07-03 2013-08-02 2013-07-08 5 days ## 4 2013-02-10 2013-03-12 2013-03-17 35 days ## 5 2012-10-25 2012-11-24 2012-11-28 34 days ## 6 2012-01-27 2012-02-26 2012-02-22 26 days ## 7 2013-08-13 2013-09-12 2013-09-09 27 days ## 8 2012-12-16 2013-01-15 2013-01-12 27 days ## 9 2012-05-14 2012-06-13 2012-07-01 48 days ## 10 2013-07-01 2013-07-31 2013-07-26 25 days ## # ... with 2,456 more rows 9.4.3.2 Compute days over due How many of the invoices were settled post the due date? We can find this by: subtracting the due date from the payment date counting the number of rows where delay &lt; 0 transact %&gt;% mutate( delay = Due - Payment ) %&gt;% filter(delay &lt; 0) %&gt;% mutate( delay = delay * -1 ) %&gt;% count(delay) ## # A tibble: 36 x 2 ## delay n ## &lt;drtn&gt; &lt;int&gt; ## 1 1 days 61 ## 2 2 days 65 ## 3 3 days 51 ## 4 4 days 62 ## 5 5 days 69 ## 6 6 days 56 ## 7 7 days 55 ## 8 8 days 49 ## 9 9 days 38 ## 10 10 days 33 ## # ... with 26 more rows 9.4.4 Your Turn compute the length of a vacation which begins on 2020-04-19 and ends on 2020-04-25 recompute the length of the vacation after shifting the vacation start and end date by 10 days and 2 weeks compute the days to settle invoice and days overdue from the receivables.csv data set compute the length of employment (only for those employees who have been terminated) from the hr-data.csv data set (use date of hire and termination) 9.5 Time Zones 9.5.1 Introduction In the previous section, POSIXlt stored date/time components as a list. Among the different components it returned were gmtoff zone gmtoff is offset in seconds from GMT i.e. difference in hours and minutes from UTC. Wait.. What do UTC and GMT stand for? Coordinated Universal Time (UTC) Greenwich Meridian Time (GMT) Since we are talking about UTC, GMT etc., let us spend a little time on understanding the basics of time zones and daylight savings. 9.5.2 Time Zones Timezones exist because different parts of the Earth receive sun light at different times. If there was a single timezone, noon or morning would mean different things in different parts of the world. The timezones are based on Earths rotation. The Earth moves ~15 degrees every 60 minutes i.e. 360 degrees in 24 hours. The planet is divided into 24 timezones each 15 degrees of longitude width. Now, you have heard of Greenwich Meridian Time (GMT) right? We just saw GMT off set in POSIXlt and you would have come across it in other time formats as well. For example, India timezone is given as GMT +5:30. Let us explore GMT in a little more detail. Greenwich is a suburb of London and the time at Greenwich is Greenwich Mean Time. As you move West from Greenwich, every 15 degree section is one hour earlier than GMT and every 15 degree section to the East is an hour later. Alright! What is UTC then? Coordinated Universal Time (UTC) , on the other hand, is the time standard commonly used across the world. Even though they share the same current time, GMT is a timezone while UTC is a time standard. So how do we check the timezone in R? When you run Sys.timezone(), you should be able to see the timezone you are in. Sys.timezone() ## [1] &quot;Asia/Calcutta&quot; If you do not see the timezone, use Sys.getenv() to get the value of the TZ environment variable. Sys.getenv(&quot;TZ&quot;) ## [1] &quot;&quot; If nothing is returned, it means we have to set the timezone. Use Sys.setenv() to set the timezone as shown below. The author resides in India and hence the timezone is set to Asia/Calcutta. You need to set the timezone in which you reside or work. Sys.setenv(TZ = &quot;Asia/Calcutta&quot;) Another way to get the timezone is through tz() from the lubridate package. lubridate::tz(Sys.time()) ## [1] &quot;&quot; If you want to view the time in a different timezone, use with_tz(). Let us look at the current time in UTC instead of Indian Standard Time. lubridate::with_tz(Sys.time(), &quot;UTC&quot;) ## [1] &quot;2021-06-02 11:29:10 UTC&quot; 9.5.3 Daylight Savings Daylight savings also known as daylight saving time daylight savings time daylight time summer time is the practice of advancing clocks during summer months so that darkness falls later each day according to the clock. In other words advance clock by one hour in spring (spring forward) retard clocks by one hour in autumn (fall back) In R, the dst() function is an indicator for daylight savings. It returns TRUE if daylight saving is in force, FALSE if not and NA if unknown. dst(Sys.Date()) ## [1] FALSE 9.5.4 Your Turn check the timezone you live in check if daylight savings in on check the current time in UTC or a different time zone 9.6 Date &amp; Time Formats 9.6.1 Introduction After the timezones and daylight savings detour, let us get back on path and explore another important aspect, date &amp; time formats. Although it is a good practice to adher to ISO 8601 format, not all date/time data will comply with it. In real world, date/time data may come in all types of weird formats. Below is a sample Format December 12, 2019 12th Dec, 2019 Dec 12th, 19 12-Dec-19 2019 December 12.12.19 When the data is not in the default ISO 8601 format, we need to explicitly specify the format in R. We do this using conversion specifications. A conversion specification is introduced by %, usually followed by a single letter or O or E and then a single letter. 9.6.2 Conversion Specifications Specification Description Example %d Day of the month (decimal number) 12 %m Month (decimal number) 12 %b Month (abbreviated) Dec %B Month (full name) December %y Year (2 digit) 19 %Y Year (4 digit) 2019 %H Hour 8 %M Minute 5 %S Second 3 Time to work through a few examples. Let us say you are dealing with dates in the format 19/12/12. In this format, the year comes first followed by month and the date; each separated by a backslash (/). The year consists of only 2 digits i.e. it does not include the century. Let us now map each component of the date to the format table shown at the beginning. Date Specification 19 %y 12 %m 12 %d Using the format argument, we will specify the date format as a character vector i.e. enclosed in quotes. as.Date(&quot;19/12/12&quot;, format = &quot;%y/%m/%d&quot;) ## [1] &quot;2019-12-12&quot; Another way in which the release data can be written is 2019-Dec-12. We still have the year followed by the month and the date but there are a few changes here: the components are separated by a - instead of / year has 4 digits i.e. includes the century the month is specified using abbreviation instead of digits Let us map the components to the format table: Date Specification 2019 %Y Dec %b 12 %d Let us specify the format for the date using the above mapping. as.Date(&quot;2019-Dec-12&quot;, format = &quot;%Y-%b-%d&quot;) ## [1] &quot;2019-12-12&quot; In both the above examples, we have not dealt with time components. Let us include the time of the latest R release in the next one i.e.  19/12/12 08:05:03. Date Specification 19 %y 12 %m 12 %d 08 %H 05 %M 03 %S Since we are dealing with time, we will use as.POSIXct() instead of as.Date(). as.POSIXct(&quot;19/12/12 08:05:03&quot;, tz = &quot;UTC&quot;, format = &quot;%y/%m/%d %H:%M:%S&quot;) ## [1] &quot;2019-12-12 08:05:03 UTC&quot; In the below table, we look at some of the most widely used conversion specifications. You can learn more about these specifications by running ?strptime or help(strptime). Specification Description %a Abbreviated weekday %A Full weekday %C Century (00-99) %D Same as %m/%d/%y %e Day of month [1 - 31] %F Same as %Y-%m-%d %h Same as %b %I Hours as decimal [01 - 12] %j Day of year [001 - 366] %R Same as %H:%M %t Tab %T Same as %H:%M:%S %u Weekday 1 - 7 %U Week of year [00 - 53] %V Week of year [01 - 53] %w Weekday 0 - 6 %W Week of year [00 - 53] We have included a lot of practice questions for you to explore the different date/time formats. The solutions are available in the Learning Management system as well as in our GitHub repo. Try them and let us know if you have any doubts. 9.6.3 Guess Format guess_formats() from lubridate is a very useful function. It will guess the date/time format if you specify the order in which year, month, date, hour, minute and second appear. release_date_formats &lt;- c(&quot;December 12th 2019&quot;, &quot;Dec 12th 19&quot;, &quot;dec 12 2019&quot;) guess_formats(release_date_formats, orders = &quot;mdy&quot;, print_matches = TRUE) ## Omdy mdy ## [1,] &quot;December 12th 2019&quot; &quot;%Om %dth %Y&quot; &quot;%B %dth %Y&quot; ## [2,] &quot;Dec 12th 19&quot; &quot;%Om %dth %y&quot; &quot;%b %dth %y&quot; ## [3,] &quot;dec 12 2019&quot; &quot;%Om %d %Y&quot; &quot;%b %d %Y&quot; ## Omdy Omdy Omdy mdy mdy ## &quot;%Om %dth %Y&quot; &quot;%Om %dth %y&quot; &quot;%Om %d %Y&quot; &quot;%B %dth %Y&quot; &quot;%b %dth %y&quot; ## mdy ## &quot;%b %d %Y&quot; 9.6.4 Your Turn Below, we have specified July 5th, 2019 in different ways. Create the date using as.Date() while specifying the correct format for each of them. July-05-19 JUL-05-19 05.07.19 5-July 2019 July 5th, 2019 July 05, 2019 2019-July- 05 05/07/2019 07/05/2019 7/5/2019 07/5/19 2019-07-05 9.7 Parse Date &amp; Time While creating date-time objects, we specified different formats using the conversion specification but most often you will not create date/time and instead deal with data thay comes your way from a system or colleague/collaborator. In such cases, we need to be able to parse date/time from the data provided to us. In this section, we will focus on parsing date/time from character data. Both base R and the lubridate package offer functions to parse date and time and we will explore a few of them in this section. We will initially use functions from base R and later on explore those from lubridate which will give us an opportunity to compare and contrast. It will also allow us to choose the functions based on the data we are dealing with. strptime() will convert character data to POSIXlt. You will use this when converting from character data to date/time. On the other hand, if you want to convert date/time to character data, use any of the following: strftime() format() as.character() The above functions will convert POSIXct/POSIXlt to character. Let us start with a simple example. The data we have been supplied has date/time as character data and in the format YYYYMMDD i.e. nothing separates the year, month and date from each other. We will use strptime() to convert this to an object of class POSIXlt. rel_date &lt;- strptime(&quot;20191212&quot;, format = &quot;%Y%m%d&quot;) class(rel_date) ## [1] &quot;POSIXlt&quot; &quot;POSIXt&quot; If you have a basic knowledge of conversion specifications, you can use strptime() to convert character data to POSIXlt. Let us quickly explore the functions to convert date/time to character data before moving on to the functions from lubridate. rel_date_strf &lt;- strftime(rel_date) class(rel_date_strf) ## [1] &quot;character&quot; rel_date_format &lt;- format(rel_date) class(rel_date_format) ## [1] &quot;character&quot; rel_date_char &lt;- as.character(rel_date) class(rel_date_char) ## [1] &quot;character&quot; As you can see, all the 3 functions converted date/time to character. Time to move on and explore the lubridate package. We will start with an example in which the release date is formatted in 3 different ways but they have one thing in common i.e. the order in which the components appear. In all the 3 formats, the year is followed by the month and then the date. To parse the release date, we will use parse_date_time() from lubridate which parses the input into POSIXct objects. release_date &lt;- c(&quot;19-12-12&quot;, &quot;20191212&quot;, &quot;19-12 12&quot;) parse_date_time(release_date, &quot;ymd&quot;) ## [1] &quot;2019-12-12 UTC&quot; &quot;2019-12-12 UTC&quot; &quot;2019-12-12 UTC&quot; parse_date_time(release_date, &quot;y m d&quot;) ## [1] &quot;2019-12-12 UTC&quot; &quot;2019-12-12 UTC&quot; &quot;2019-12-12 UTC&quot; parse_date_time(release_date, &quot;%y%m%d&quot;) ## [1] &quot;2019-12-12 UTC&quot; &quot;2019-12-12 UTC&quot; &quot;2019-12-12 UTC&quot; Try to use strptime() in the above example and see what happens. Now, let us look at another data set. release_date &lt;- c(&quot;19-07-05&quot;, &quot;2019-07-05&quot;, &quot;05-07-2019&quot;, &quot;07-05-2019&quot;) What happens in the below case? The same date appears in multiple formats. How do we parse them? parse_date_time() allows us to specify mutiple date-time formats. Let us first map the dates to their formats. Date Specification 19-07-05 ymd 2019-07-05 ymd 05-07-2019 dmy 07-05-2019 mdy The above specifications can be supplied as a character vector. parse_date_time(release_date, c(&quot;ymd&quot;, &quot;ymd&quot;, &quot;dmy&quot;, &quot;mdy&quot;)) ## [1] &quot;2019-07-05 UTC&quot; &quot;2019-07-05 UTC&quot; &quot;2019-07-05 UTC&quot; &quot;2019-05-07 UTC&quot; Great! We have used both strptime() and parse_date_time() now. Can you tell what differentiates parse_date_time() when compared to strptime()? We summarize it in the points below: no need to include % prefix or separator specify several date/time formats There are other helper functions that can be used to parse dates with year, month, day components parse dates with year, month, day, hour, minute, seconds components parse period with hour, minute, second components and are explored in the below examples. # year/month/date ymd(&quot;2019-12-12&quot;) ## [1] &quot;2019-12-12&quot; # year/month/date ymd(&quot;19/12/12&quot;) ## [1] &quot;2019-12-12&quot; # date/month/year dmy(121219) ## [1] &quot;2019-12-12&quot; # year/month/date/hour/minute/second ymd_hms(191212080503) ## [1] &quot;2019-12-12 08:05:03 UTC&quot; # hour/minute/second hms(&quot;8, 5, 3&quot;) ## [1] &quot;8H 5M 3S&quot; # hour/minute/second hms(&quot;08:05:03&quot;) ## [1] &quot;8H 5M 3S&quot; # minute/second ms(&quot;5,3&quot;) ## [1] &quot;5M 3S&quot; # hour/minute hm(&quot;8, 5&quot;) ## [1] &quot;8H 5M 0S&quot; Note, in a couple of cases where the components are not separated by /, - or space, we have not enclosed the values in quotes. 9.7.1 Your Turn Below, we have specified July 5th, 2019 in different ways. Parse the dates using strptime() or parse_date_time() or any other helper function. July-05-19 JUL-05-19 05.07.19 5-July 2019 July 5th, 2019 July 05, 2019 2019-July- 05 05/07/2019 07/05/2019 7/5/2019 07/5/19 2019-07-05 9.8 Date &amp; Time Components In the second section, we discussed the downside of saving date/time as character/string in R. One of the points we discussed was that we cant extract components such as year, month, day etc. In this section, we will learn to extract date/time components such as year month date week day quarter semester hour minute second timezone The below table outlines the functions we will explore in the first part of this section. Function Description year() Get year month() Get month (number) month(label = TRUE) Get month (abbreviated name) month(abbr = FALSE) Get month (full name) months() Get month week() Get week 9.8.1 Year release_date &lt;- ymd_hms(&quot;2019-12-12 08:05:03&quot;) year(release_date) ## [1] 2019 9.8.2 Month month() will return the month as a number i.e. 07 for July. month(release_date) ## [1] 12 If you want the name of the month instead, use the label argument and set it to TRUE. Now it returns Jul instead of 07. month(release_date, label = TRUE) ## [1] Dec ## 12 Levels: Jan &lt; Feb &lt; Mar &lt; Apr &lt; May &lt; Jun &lt; Jul &lt; Aug &lt; Sep &lt; ... &lt; Dec But this is the abbreviated name and not the full name. How do we get the full name of the month? Set the abbr argument to FALSE. month(release_date, label = TRUE, abbr = FALSE) ## [1] December ## 12 Levels: January &lt; February &lt; March &lt; April &lt; May &lt; June &lt; ... &lt; December Ah! now we can see the full name of the month. months() from base R will return the full name of the month by default. If you want the abbreviated name, use the abbreviate argument and set it to TRUE. months(release_date) ## [1] &quot;December&quot; 9.8.3 Week week() returns the number of complete 7 day periods between the date and 1st January plus one. week(release_date) ## [1] 50 9.8.4 Day Use day() to extract the date component. There are other variations such as Function Description day Get day mday() Day of the month wday() Day of the week qday() Day of quarter yday() Day of year weekdays() Day of week days_in_month() Days in the month day(release_date) ## [1] 12 mday(release_date) ## [1] 12 qday(release_date) ## [1] 73 yday(release_date) ## [1] 346 wday can return a number abbreviation of the weekday full name of the weekday wday(release_date) ## [1] 5 wday(release_date, label = TRUE) ## [1] Thu ## Levels: Sun &lt; Mon &lt; Tue &lt; Wed &lt; Thu &lt; Fri &lt; Sat wday(release_date, label = TRUE, abbr = FALSE) ## [1] Thursday ## 7 Levels: Sunday &lt; Monday &lt; Tuesday &lt; Wednesday &lt; Thursday &lt; ... &lt; Saturday weekdays() from base R also returns the day of the week (the name and not the number). If you want the abbreviated name, use the abbreviate argument. weekdays(release_date) ## [1] &quot;Thursday&quot; weekdays(release_date, abbreviate = TRUE) ## [1] &quot;Thu&quot; 9.8.5 Days in Month If you want to know the number of days in the month, use days_in_month(). In our example, the month is December and it has 31 days. days_in_month(release_date) ## Dec ## 31 9.8.6 Hour, Minute &amp; Seconds Function Description hour() Get hour minute() Get minute second() Get second seconds() Number of seconds since 1970-01-01 So far we have been looking at date components. Now, let us look at time components. hour(release_date) ## [1] 8 minute(release_date) ## [1] 5 second(release_date) ## [1] 3 seconds() returns the number of seconds since 1970-01-01. seconds(release_date) ## [1] &quot;1576137903S&quot; 9.8.7 Quarter &amp; Semester quarter() will return the quarter from the date. December is in the 4th quarter and hence it returns 4. quarter(release_date) ## [1] 4 If you want the year along with the quarter, set the with_year argument to TRUE. quarter(release_date, with_year = TRUE) ## [1] 2019.4 In India, the fiscal starts in April and December falls in the 3rd quarter. How can we accommodate this change? The fiscal_start argument allows us to set the month in which the fiscal begins. We will set it to 4 for April. Now it returns 3 instead of 4. quarter(release_date, fiscal_start = 4) ## [1] 3 quarters() from base R also returns the quarter. quarters(release_date) ## [1] &quot;Q4&quot; Function Description quarter() Get quarter quarter(with_year = TRUE) Quarter with year quarter(fiscal_start = 4) Fiscal starts in April quarters() Get quarter semester() Get semester 9.8.8 Case Study 9.8.8.1 Extract Date, Month &amp; Year from Due Date Let us now extract the date, month and year from the Due column. transact %&gt;% mutate( due_day = day(Due), due_month = month(Due), due_year = year(Due) ) ## # A tibble: 2,466 x 6 ## Invoice Due Payment due_day due_month due_year ## &lt;date&gt; &lt;date&gt; &lt;date&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2013-01-02 2013-02-01 2013-01-15 1 2 2013 ## 2 2013-01-26 2013-02-25 2013-03-03 25 2 2013 ## 3 2013-07-03 2013-08-02 2013-07-08 2 8 2013 ## 4 2013-02-10 2013-03-12 2013-03-17 12 3 2013 ## 5 2012-10-25 2012-11-24 2012-11-28 24 11 2012 ## 6 2012-01-27 2012-02-26 2012-02-22 26 2 2012 ## 7 2013-08-13 2013-09-12 2013-09-09 12 9 2013 ## 8 2012-12-16 2013-01-15 2013-01-12 15 1 2013 ## 9 2012-05-14 2012-06-13 2012-07-01 13 6 2012 ## 10 2013-07-01 2013-07-31 2013-07-26 31 7 2013 ## # ... with 2,456 more rows 9.8.8.2 Data Sanitization Let us do some data sanitization. If the due day happens to be February 29, let us ensure that the due year is a leap year. Below are the steps to check if the due year is a leap year: we will extract the following from the due date: day month year we will then create a new column is_leap which will have be set to TRUE if the year is a leap year else it will be set to FALSE filter all the payments due on 29th Feb select the following columns: Due is_leap transact %&gt;% mutate( due_day = day(Due), due_month = month(Due), due_year = year(Due), is_leap = leap_year(due_year) ) %&gt;% filter(due_month == 2 &amp; due_day == 29) %&gt;% select(Due, is_leap) ## # A tibble: 4 x 2 ## Due is_leap ## &lt;date&gt; &lt;lgl&gt; ## 1 2012-02-29 TRUE ## 2 2012-02-29 TRUE ## 3 2012-02-29 TRUE ## 4 2012-02-29 TRUE 9.8.8.3 Invoices Distribution by Quarter Let us count the invoices due for each quarter. transact %&gt;% mutate( quarter_due = quarter(Due) ) %&gt;% count(quarter_due) ## # A tibble: 4 x 2 ## quarter_due n ## &lt;int&gt; &lt;int&gt; ## 1 1 521 ## 2 2 661 ## 3 3 618 ## 4 4 666 9.8.9 Your Turn Get the R release dates using r_versions() from the rversions package and tabulate the following year month with label weekday with label hour and quarter 9.9 Create, Update &amp; Verify In the second section, we learnt to create date-time objects using as.Date(), as.POSIXct() etc. In this section, we will explore a few other functions that will allow us to do the same make_date() make_datetime() 9.9.1 Create To create date without time components, use make_date() and specify the following: year month date We need to specify all the components in numbers i.e. we cannot use Jul or July for the month. It has to be 7. make_date(year = 2019, month = 12, day = 12) ## [1] &quot;2019-12-12&quot; When you need to include time components, use make_datetime(). make_datetime(year = 2019, month = 12, day = 12, hour = 08, min = 05, sec = 03, tz = &quot;UTC&quot;) ## [1] &quot;2019-12-12 08:05:03 UTC&quot; 9.9.2 Update Let us look at another scenario. You have a date-time object and want to change one of its components i.e. any of the following year month date Instead of creating another date-time object, you can change any of the components using update(). In the below example, we will start with the date of release of R version 3.6.1 and using update(), we will change it to 2019-12-12. prev_release &lt;- ymd(&quot;2019-07-05&quot;) prev_release %&gt;% update(year = 2019, month = 12, mday = 12) ## [1] &quot;2019-12-12&quot; 9.9.3 Date Sequence So far we have created a single date-time instance. How about creating a sequence of dates? We can do that using seq.Date(). We need to specify the from date as the bare minimum input. If the end date is not specified, it will create the sequence uptil the current date. The interval of the sequence can be specified in any of the following units: day week month quarter year We can add the following to the interval units integer + / - (increment or decrement) Using the integer, we can specify multiples of the units mentioned and using the sign, we can specify whether to increment or decrement. The below table displays the main arguments used in seq.Date(): Function Description from Starting date of the sequence by End date of the sequence to Date increment of the sequence length.out Length of the sequence along.with Use length of this value as length of sequence In the first example, we will create a sequence of dates from 2010-01-01 to 2019-12-31. The unit of increment should be a year i.e. the difference between the dates in the sequence should be 1 year, specified using the by argument. seq.Date(from = as.Date(&quot;2010-01-01&quot;), to = as.Date(&quot;2019-12-31&quot;), by = &quot;year&quot;) ## [1] &quot;2010-01-01&quot; &quot;2011-01-01&quot; &quot;2012-01-01&quot; &quot;2013-01-01&quot; &quot;2014-01-01&quot; ## [6] &quot;2015-01-01&quot; &quot;2016-01-01&quot; &quot;2017-01-01&quot; &quot;2018-01-01&quot; &quot;2019-01-01&quot; In the next example, we change the unit of increment to a quarter i.e. the difference between the dates in the sequence should be a quarter or 3 months. seq.Date(from = as.Date(&quot;2009-12-12&quot;), to = as.Date(&quot;2019-12-12&quot;), by = &quot;quarter&quot;) ## [1] &quot;2009-12-12&quot; &quot;2010-03-12&quot; &quot;2010-06-12&quot; &quot;2010-09-12&quot; &quot;2010-12-12&quot; ## [6] &quot;2011-03-12&quot; &quot;2011-06-12&quot; &quot;2011-09-12&quot; &quot;2011-12-12&quot; &quot;2012-03-12&quot; ## [11] &quot;2012-06-12&quot; &quot;2012-09-12&quot; &quot;2012-12-12&quot; &quot;2013-03-12&quot; &quot;2013-06-12&quot; ## [16] &quot;2013-09-12&quot; &quot;2013-12-12&quot; &quot;2014-03-12&quot; &quot;2014-06-12&quot; &quot;2014-09-12&quot; ## [21] &quot;2014-12-12&quot; &quot;2015-03-12&quot; &quot;2015-06-12&quot; &quot;2015-09-12&quot; &quot;2015-12-12&quot; ## [26] &quot;2016-03-12&quot; &quot;2016-06-12&quot; &quot;2016-09-12&quot; &quot;2016-12-12&quot; &quot;2017-03-12&quot; ## [31] &quot;2017-06-12&quot; &quot;2017-09-12&quot; &quot;2017-12-12&quot; &quot;2018-03-12&quot; &quot;2018-06-12&quot; ## [36] &quot;2018-09-12&quot; &quot;2018-12-12&quot; &quot;2019-03-12&quot; &quot;2019-06-12&quot; &quot;2019-09-12&quot; ## [41] &quot;2019-12-12&quot; We will now create a sequence of dates but instead of specifying the unit of increment, we specify the number of dates in the sequence i.e. the length of the sequence. We do this using the length.out argument which specifies the desired length of the sequence. We want the sequence to have 10 dates including the start and end date, and hence we supply the value 10 for the length.out argument. seq.Date(from = as.Date(&quot;2010-01-01&quot;), to = as.Date(&quot;2019-12-31&quot;), length.out = 10) ## [1] &quot;2010-01-01&quot; &quot;2011-02-10&quot; &quot;2012-03-22&quot; &quot;2013-05-02&quot; &quot;2014-06-11&quot; ## [6] &quot;2015-07-22&quot; &quot;2016-08-31&quot; &quot;2017-10-10&quot; &quot;2018-11-20&quot; &quot;2019-12-31&quot; In all of the previous examples, we have specified both the start and the end date. Let us look at a few examples where we create a sequence of dates where we only specify the start date. In the below example, we want to create a sequence of dates starting from 2010-01-01. The unit of increment should be 1 year i.e. the difference between the dates in the sequence should be 1 year and the length of the sequence should be 10 i.e. the number of dates including the start date should be 10. seq.Date(from = as.Date(&quot;2010-01-01&quot;), by = &quot;year&quot;, length.out = 10) ## [1] &quot;2010-01-01&quot; &quot;2011-01-01&quot; &quot;2012-01-01&quot; &quot;2013-01-01&quot; &quot;2014-01-01&quot; ## [6] &quot;2015-01-01&quot; &quot;2016-01-01&quot; &quot;2017-01-01&quot; &quot;2018-01-01&quot; &quot;2019-01-01&quot; The unit of increment can include multiples and +/- sign i.e. it can be an unit of increment or decrement. In the next example, we can increment the dates in the sequence by 2 i.e. the difference between the dates should be 2 instead of 1. This is achieved by specifying the unit of increment (multiple) first followed by a space and then the unit. In our example, it is 2 year. As you can see, the sequence now goes all the way till 2028 and the gap between the dates is 2 years. seq.Date(from = as.Date(&quot;2010-01-01&quot;), by = &quot;2 year&quot;, length.out = 10) ## [1] &quot;2010-01-01&quot; &quot;2012-01-01&quot; &quot;2014-01-01&quot; &quot;2016-01-01&quot; &quot;2018-01-01&quot; ## [6] &quot;2020-01-01&quot; &quot;2022-01-01&quot; &quot;2024-01-01&quot; &quot;2026-01-01&quot; &quot;2028-01-01&quot; Let us say instead of increment we want to decrement the dates i.e. the sequence of dates will go backwards as shown in the next example. We achieve this by using the - sign along with the unit of decrement. The sequence of dates in next example starts from 2010 and goes back upto 1992 and the difference between the dates in 2 years. seq.Date(from = as.Date(&quot;2010-01-01&quot;), by = &quot;-2 year&quot;, length.out = 10) ## [1] &quot;2010-01-01&quot; &quot;2008-01-01&quot; &quot;2006-01-01&quot; &quot;2004-01-01&quot; &quot;2002-01-01&quot; ## [6] &quot;2000-01-01&quot; &quot;1998-01-01&quot; &quot;1996-01-01&quot; &quot;1994-01-01&quot; &quot;1992-01-01&quot; In the last example, we will explore the along.with argument. Here we have supplied a vector which is a sequence of numbers from 1 to 10. The length of this vector is 10 and the same length is used as the length of the sequence i.e.  the length of value supplied to along.with is also the length of the sequence. seq.Date(from = as.Date(&quot;2010-01-01&quot;), by = &quot;-2 year&quot;, along.with = 1:10) ## [1] &quot;2010-01-01&quot; &quot;2008-01-01&quot; &quot;2006-01-01&quot; &quot;2004-01-01&quot; &quot;2002-01-01&quot; ## [6] &quot;2000-01-01&quot; &quot;1998-01-01&quot; &quot;1996-01-01&quot; &quot;1994-01-01&quot; &quot;1992-01-01&quot; 9.9.4 Verify Type How do you check if the data is a date-time object? You can do that using any of the following from the lubridate package. is.Date() is.POSIXct() is.POSIXlt() is.Date(release_date) ## [1] FALSE is.POSIXct(release_date) ## [1] TRUE is.POSIXlt(release_date) ## [1] FALSE 9.9.5 Your Turn R 2.0.0 was released on 2004-10-04 14:24:38. Create this date using both make_date() and make_datetime() R 3.0.0 was released on 2013-04-03 07:12:36. Update the date created in the previous step to the above using update() 9.10 Intervals, Duration &amp; Period In this chapter, we will learn about intervals duration and period 9.10.1 Interval An interval is a timespan defined by two date-times. Let us represent the length of the course using interval. course_start &lt;- as_date(&#39;2017-04-12&#39;) course_end &lt;- as_date(&#39;2017-04-21&#39;) interval(course_start, course_end) ## [1] 2017-04-12 UTC--2017-04-21 UTC If you observe carefully, the interval is represented by the course start and end dates. We will learn how to use intervals in the case study. 9.10.1.1 Overlapping Intervals Let us say you are planning a vacation and want to check if the vacation dates overlap with the course dates. You can do this by: creating vacation and course intervals use int_overlaps() to check if two intervals overlap. It returns TRUE if the intervals overlap else FALSE. Let us use the vacation start and end dates to create vacation_interval and then check if it overlaps with course_interval. vacation_start &lt;- as_date(&#39;2017-04-19&#39;) vacation_end &lt;- as_date(&#39;2017-04-25&#39;) course_interval &lt;- interval(course_start, course_end) vacation_interval &lt;- interval(vacation_start, vacation_end) int_overlaps(course_interval, vacation_interval) ## [1] TRUE 9.10.1.2 How many invoices were settled within due date? Let us use intervals to count the number of invoices that were settled within the due date. To do this, we will: create an interval for the invoice and due date create a new column due_next by incrementing the due date by 1 day another interval for due_next and the payment date if the intervals overlap, the payment was made within the due date transact %&gt;% mutate( inv_due_interval = interval(Invoice, Due), due_next = Due + days(1), due_pay_interval = interval(due_next, Payment), overlaps = int_overlaps(inv_due_interval, due_pay_interval) ) %&gt;% select(Invoice, Due, Payment, overlaps) ## # A tibble: 2,466 x 4 ## Invoice Due Payment overlaps ## &lt;date&gt; &lt;date&gt; &lt;date&gt; &lt;lgl&gt; ## 1 2013-01-02 2013-02-01 2013-01-15 TRUE ## 2 2013-01-26 2013-02-25 2013-03-03 FALSE ## 3 2013-07-03 2013-08-02 2013-07-08 TRUE ## 4 2013-02-10 2013-03-12 2013-03-17 FALSE ## 5 2012-10-25 2012-11-24 2012-11-28 FALSE ## 6 2012-01-27 2012-02-26 2012-02-22 TRUE ## 7 2013-08-13 2013-09-12 2013-09-09 TRUE ## 8 2012-12-16 2013-01-15 2013-01-12 TRUE ## 9 2012-05-14 2012-06-13 2012-07-01 FALSE ## 10 2013-07-01 2013-07-31 2013-07-26 TRUE ## # ... with 2,456 more rows Below we show another method to count the number of invoices paid within the due date. Instead of using days to change the due date, we use int_shift to shift it by 1 day. transact %&gt;% mutate( inv_due_interval = interval(Invoice, Due), due_pay_interval = interval(Due, Payment), due_pay_next = int_shift(due_pay_interval, by = days(1)), overlaps = int_overlaps(inv_due_interval, due_pay_next) ) %&gt;% select(Invoice, Due, Payment, overlaps) ## # A tibble: 2,466 x 4 ## Invoice Due Payment overlaps ## &lt;date&gt; &lt;date&gt; &lt;date&gt; &lt;lgl&gt; ## 1 2013-01-02 2013-02-01 2013-01-15 TRUE ## 2 2013-01-26 2013-02-25 2013-03-03 FALSE ## 3 2013-07-03 2013-08-02 2013-07-08 TRUE ## 4 2013-02-10 2013-03-12 2013-03-17 FALSE ## 5 2012-10-25 2012-11-24 2012-11-28 FALSE ## 6 2012-01-27 2012-02-26 2012-02-22 TRUE ## 7 2013-08-13 2013-09-12 2013-09-09 TRUE ## 8 2012-12-16 2013-01-15 2013-01-12 TRUE ## 9 2012-05-14 2012-06-13 2012-07-01 FALSE ## 10 2013-07-01 2013-07-31 2013-07-26 TRUE ## # ... with 2,456 more rows You might be thinking why we incremented the due date by a day before creating the interval between the due day and the payment day. If we do not increment, both the intervals will share a common date i.e. the due date and they will always overlap as shown below: transact %&gt;% mutate( inv_due_interval = interval(Invoice, Due), due_pay_interval = interval(Due, Payment), overlaps = int_overlaps(inv_due_interval, due_pay_interval) ) %&gt;% select(Invoice, Due, Payment, overlaps) ## # A tibble: 2,466 x 4 ## Invoice Due Payment overlaps ## &lt;date&gt; &lt;date&gt; &lt;date&gt; &lt;lgl&gt; ## 1 2013-01-02 2013-02-01 2013-01-15 TRUE ## 2 2013-01-26 2013-02-25 2013-03-03 TRUE ## 3 2013-07-03 2013-08-02 2013-07-08 TRUE ## 4 2013-02-10 2013-03-12 2013-03-17 TRUE ## 5 2012-10-25 2012-11-24 2012-11-28 TRUE ## 6 2012-01-27 2012-02-26 2012-02-22 TRUE ## 7 2013-08-13 2013-09-12 2013-09-09 TRUE ## 8 2012-12-16 2013-01-15 2013-01-12 TRUE ## 9 2012-05-14 2012-06-13 2012-07-01 TRUE ## 10 2013-07-01 2013-07-31 2013-07-26 TRUE ## # ... with 2,456 more rows 9.10.1.3 Shift Interval Intervals can be shifted too. In the below example, we shift the course interval by: 1 day 3 weeks 1 year course_interval &lt;- interval(course_start, course_end) # shift course_interval by 1 day int_shift(course_interval, by = days(1)) ## [1] 2017-04-13 UTC--2017-04-22 UTC # shift course_interval by 3 weeks int_shift(course_interval, by = weeks(3)) ## [1] 2017-05-03 UTC--2017-05-12 UTC # shift course_interval by 1 year int_shift(course_interval, by = years(1)) ## [1] 2018-04-12 UTC--2018-04-21 UTC 9.10.2 Within Let us assume that we have to attend a conference in April 2017. Does it occur during the course duration? We can answer this using %within% which will return TRUE if a date falls within an interval. conference &lt;- as_date(&#39;2017-04-15&#39;) conference %within% course_interval ## [1] TRUE 9.10.2.1 How many invoices were settled within due date? Let us use %within% to count the number of invoices that were settled within the due date. We will do this by: creating an interval for the invoice and due date check if the payment date falls within the above interval transact %&gt;% mutate( inv_due_interval = interval(Invoice, Due), overlaps = Payment %within% inv_due_interval ) %&gt;% select(Due, Payment, overlaps) ## # A tibble: 2,466 x 3 ## Due Payment overlaps ## &lt;date&gt; &lt;date&gt; &lt;lgl&gt; ## 1 2013-02-01 2013-01-15 TRUE ## 2 2013-02-25 2013-03-03 FALSE ## 3 2013-08-02 2013-07-08 TRUE ## 4 2013-03-12 2013-03-17 FALSE ## 5 2012-11-24 2012-11-28 FALSE ## 6 2012-02-26 2012-02-22 TRUE ## 7 2013-09-12 2013-09-09 TRUE ## 8 2013-01-15 2013-01-12 TRUE ## 9 2012-06-13 2012-07-01 FALSE ## 10 2013-07-31 2013-07-26 TRUE ## # ... with 2,456 more rows 9.10.3 Duration Duration is timespan measured in seconds. To create a duration object, use duration(). The timespan can be anything from seconds to years but it will be represented as seconds. Let us begin by creating a duration object where the timespan is in seconds. duration(50, &quot;seconds&quot;) ## [1] &quot;50s&quot; Another way to specify the above timespan is shown below: duration(second = 50) ## [1] &quot;50s&quot; As you can see, the output is same in both the cases. Let us increase the timespan to 60 seconds and see what happens. duration(second = 60) ## [1] &quot;60s (~1 minutes)&quot; Although the timespan is primarily measured in seconds, it also shows ~1 minutes in the brackets. As the length of the timespan increases i.e. the number becomes large, it is represented using larger units such as hours and days. In the below examples, as the number of seconds increases, you can observe larger units being used to represent the timespan. # minutes duration(minute = 50) ## [1] &quot;3000s (~50 minutes)&quot; duration(minute = 60) ## [1] &quot;3600s (~1 hours)&quot; # hours duration(hour = 23) ## [1] &quot;82800s (~23 hours)&quot; duration(hour = 24) ## [1] &quot;86400s (~1 days)&quot; The following helper functions can be used to create duration objects as well. # default dseconds() ## [1] &quot;1s&quot; dminutes() ## [1] &quot;60s (~1 minutes)&quot; # seconds duration(second = 59) ## [1] &quot;59s&quot; dseconds(59) ## [1] &quot;59s&quot; # minutes duration(minute = 50) ## [1] &quot;3000s (~50 minutes)&quot; dminutes(50) ## [1] &quot;3000s (~50 minutes)&quot; # hours duration(hour = 36) ## [1] &quot;129600s (~1.5 days)&quot; dhours(36) ## [1] &quot;129600s (~1.5 days)&quot; # weeks duration(week = 56) ## [1] &quot;33868800s (~1.07 years)&quot; dweeks(56) ## [1] &quot;33868800s (~1.07 years)&quot; Let us use the above helper functions to get the course length in different units. # course length in seconds course_interval / dseconds() ## [1] 777600 # course length in minutes course_interval / dminutes() ## [1] 12960 # course length in hours course_interval / dhours() ## [1] 216 # course length in weeks course_interval / dweeks() ## [1] 1.285714 # course length in years course_interval / dyears() ## [1] 0.02464066 9.10.4 Period A period is a timespan defined in units such as years, months, and days. In the below examples, we use period() to represent timespan using different units. # second period(5, &quot;second&quot;) ## [1] &quot;5S&quot; period(second = 5) ## [1] &quot;5S&quot; # minute &amp; second period(c(3, 5), c(&quot;minute&quot;, &quot;second&quot;)) ## [1] &quot;3M 5S&quot; period(minute = 3, second = 5) ## [1] &quot;3M 5S&quot; # hour, minte &amp; second period(c(1, 3, 5), c(&quot;hour&quot;, &quot;minute&quot;, &quot;second&quot;)) ## [1] &quot;1H 3M 5S&quot; period(hour = 1, minute = 3, second = 5) ## [1] &quot;1H 3M 5S&quot; # day, hour, minute &amp; second period(c(3, 1, 3, 5), c(&quot;day&quot;, &quot;hour&quot;, &quot;minute&quot;, &quot;second&quot;)) ## [1] &quot;3d 1H 3M 5S&quot; period(day = 3, hour = 1, minute = 3, second = 5) ## [1] &quot;3d 1H 3M 5S&quot; Let us get the course length in different units using as.period(). # course length in second as.period(course_interval, unit = &quot;seconds&quot;) ## [1] &quot;777600S&quot; # course length in hours and minutes as.period(course_interval, unit = &quot;minutes&quot;) ## [1] &quot;12960M 0S&quot; # course length in hours, minutes and seconds as.period(course_interval, unit = &quot;hours&quot;) ## [1] &quot;216H 0M 0S&quot; time_length() computes the exact length of a timespan i.e. duration, interval or period. Let us use time_length() to compute the length of the course in different units. # course length in seconds time_length(course_interval, unit = &quot;seconds&quot;) ## [1] 777600 # course length in minutes time_length(course_interval, unit = &quot;minutes&quot;) ## [1] 12960 # course length in hours time_length(course_interval, unit = &quot;hours&quot;) ## [1] 216 9.11 Others In this section, we will learn to round date/time to the nearest unit and roll back dates. 9.11.1 Rounding Dates We will explore functions for rounding dates to the nearest value using round_dates() down using floor_date() up using ceiling_date() The unit for rounding can be any of the following: second minute hour day week month bimonth quarter season halfyear and year We will look at a few examples using round_date() and you will then practice using the other two functions. # minute round_date(release_date, unit = &quot;minute&quot;) ## [1] &quot;2019-12-12 08:05:00 UTC&quot; round_date(release_date, unit = &quot;mins&quot;) ## [1] &quot;2019-12-12 08:05:00 UTC&quot; round_date(release_date, unit = &quot;5 mins&quot;) ## [1] &quot;2019-12-12 08:05:00 UTC&quot; # hour round_date(release_date, unit = &quot;hour&quot;) ## [1] &quot;2019-12-12 08:00:00 UTC&quot; # day round_date(release_date, unit = &quot;day&quot;) ## [1] &quot;2019-12-12 UTC&quot; 9.11.2 Rollback Use rollback() if you want to change the date to the last day of the previous month or the first day of the month. rollback(release_date) ## [1] &quot;2019-11-30 08:05:03 UTC&quot; To change the date to the first day of the month, use the roll_to_first argument and set it to TRUE. rollback(release_date, roll_to_first = TRUE) ## [1] &quot;2019-12-01 08:05:03 UTC&quot; 9.11.3 Your Turn round up R release dates to hours round down R release dates to minutes rollback R release dates to the beginning of the month "],["categorical-data-in-r.html", "Chapter 10 Categorical Data 10.1 Introduction 10.2 Case Study 10.3 Factors 10.4 Data Manipulation 10.5 Data Visualization 10.6 Useful R Packages", " Chapter 10 Categorical Data 10.1 Introduction Handling categorical/qualitative data is integral to data analysis. Almost every data science project involves working with categorical data and students should know how to store, summarize, visualize &amp; manipulate such data. Working with categorical data is different from working with numbers or text. In this chapter, we will understand categorical data and explore the rich set of functions (built-in &amp; through packages) provided by R for working with such data. The word categorical is used interchangeably with qualitative. 10.1.1 Data Types Before we begin our deep dive on categorical data, let us get a quick overview of different data types. In the chart above, we can see that data can be primarily classified into qualitative or quantitative. Qualitative data consists of labels or names. Quantitative data, on the other hand, consists of numbers and indicate how much or how many. This brings us to the next level of classification: discrete continuous In the chart, we can observe that qualitative data is always discrete where as quantitative data may be discrete or continuous. Qualitative data is further classified into nominal ordinal First, we will understand discrete and continuous data, and then proceed to explore nominal and ordinal data. Discrete Data Discrete data arises in situations where counting is involved. It can take on only a finite number of values and cannot be divided into smaller parts. For example, let us consider the number of students in a class. We can have 5 0r 10 students but not 5.5 students (we cant have half a student). Continuous Data Continuous data arises in situations where measuring is involved. It can take any numeric value in a specified range and can be divided into smaller parts and still have meaning. Examples include money, temperature, length, volume etc. 10.1.2 Categorical Data Since our interest is in categorical data, we will spend more time understanding the different types of categorical data through various examples. Let us begin by formally defining categorical data: it is always discrete it may be divided into groups consists of names or labels takes on limited &amp; fixed number of possible values arises in situation when counting is involved analysis generally involves the use of data tables 10.1.3 Dichotomous A categorical variable that can take on exactly two values is termed as binary or dichotomous variable. 10.1.4 Polychotomous Categorical variables with more than two possible values are called polychotomous variables. 10.1.5 Ordinal In ordinal data, the categories can be ordered or ranked. Examples include socio-economic status education level income level satisfaction rating While we can rank the categories, we cannot assign a value to them. For example, in satisfaction ranking, we cannot say that like is twice as positive as dislike i.e. we are unable to say how much they differ from each other. While the order or rank of data is meaningful, the difference between two pieces of data cannot be measured/determined or are meaningless. Ordinal data provide information about relative comparisons, but not the magnitude of the differences. 10.1.6 Nominal Nominal data do not have an intrinsic order and cannot be ordered or measured. Examples include blood group gender religion color Categorical data are sometimes coded with numbers, with those numbers replacing names. Although such numbers might appear to be quantitative, they are actually categorical data. When they do take numerical values, those numbers do not have any mathematical meaning. Examples include months expressed in numbers. 10.1.7 Summary Data can be qualitative or quantitative. Qualitative data is always discrete. Dichotomous data consists of only 2 groups/levels. Polychotomous data consists of more than 2 groups/levels. Nominal data do not have an intrinsic order. In ordinal data categories can be ordered or ranked the difference between the categories cannot be determined 10.1.8 Your Turn State whether the following are quantitative or qualitative Age Gender Annual Sales Weight Education Level Vehicle Type Height Ice Cream Flavor Job/Work Experience Blood Group Color Mode of Payment State whether the following are dichotomous or polychotomous Heads / Tails Blood Group Transportation Mode Rich / Poor Democrat / Republican Education Level Gender Pass / Fail Yes / No Positive / Negative State whether the following are nominal or ordinal STD Codes Prepaid / Postpaid Hotel Ratings Smart Phone Brands Student Grades Cellular Operator ISPs (Internet Service Providers) Occupation Bank Account Number Class of Travel Military Rank 10.2 Case Study As is the practice, throughout this chapter, we will work on a case study related to an e-commerce firm. As most of you would already be aware, a lot of data is captured when you go on the internet by the websites you browse as well as by third party cookies. Data collected is then used to display ads as well as to feed to recommendation algorithms. The data used in the case study represents the basic information that is captured when users visit any website. It closely resembles real world data for an e-commerce store. We will try to generate insights about the visitors to be used by an imaginary marketing team for better targeting and promotion. The case study data set can be imported using the RStudio IDE or R code. 10.2.1 Data The data set is available in both CSV &amp; RDS formats. CSV If you want to specify the data types while reading the data, use the readr package. We have explored how to import data into R in a previous chapter. We will read a subset of columns from the data set (it has 20 columns) which will cover both nominal and ordinal data types. To import the data, we will use the read_csv() function. The first input is the name of the data set, analytics.csv. Ensure that the name is enclosed in single/double quotes. read_csv(&quot;analytics_raw.csv&quot;, col_types = cols_only(device = col_factor(levels = c(&quot;Desktop&quot;, &quot;Tablet&quot;, &quot;Mobile&quot;)), gender = col_factor(levels = c(&quot;female&quot;, &quot;male&quot;, &quot;NA&quot;)), user_rating = col_factor(levels = c(&quot;1&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4&quot;, &quot;5&quot;), ordered = TRUE))) ## Warning: Missing column names filled in: &#39;X1&#39; [1] ## # A tibble: 244,398 x 3 ## device gender user_rating ## &lt;fct&gt; &lt;fct&gt; &lt;ord&gt; ## 1 Desktop female 4 ## 2 Mobile &lt;NA&gt; 5 ## 3 Desktop &lt;NA&gt; 4 ## 4 Desktop &lt;NA&gt; 5 ## 5 Desktop &lt;NA&gt; 4 ## 6 Mobile &lt;NA&gt; 4 ## 7 Desktop &lt;NA&gt; 4 ## 8 Desktop &lt;NA&gt; 4 ## 9 Desktop female 5 ## 10 Desktop &lt;NA&gt; 4 ## # ... with 244,388 more rows Since we are specifying the column data types while importing the data, we will use the col_types argument to list out the data types. As we are reading in a subset of the columns and not all of them, we will use the cols_only() function indicating that only the columns specified must be read in and not all of them. Categorical data and the levels/groups are specified using the col_factor() function. Use the levels argument to specify the levels/groups and the ordered argument to indicate if the data is ordinal. By default, it is set to FALSE, change this to TRUE if the column is ordinal. RDS The .rds file can be read using readRDS(). data &lt;- readRDS(&#39;analytics.rds&#39;) head(data) ## # A tibble: 6 x 19 ## device os browser user_type channel gender frequency ## &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 Desktop Windows Chrome New Visitor Organic Search female 1 ## 2 Mobile iOS Safari Returning Visitor Organic Search &lt;NA&gt; 3 ## 3 Desktop Chrome OS Chrome New Visitor Direct &lt;NA&gt; 1 ## 4 Desktop Macintosh Chrome Returning Visitor Organic Search &lt;NA&gt; 2 ## 5 Desktop Macintosh Chrome Returning Visitor Referral &lt;NA&gt; 5 ## 6 Mobile Android Chrome New Visitor Organic Search &lt;NA&gt; 1 ## recency page_depth hour_of_day age duration landing_page exit_page ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; ## 1 0 1 02 64 0 Home Home ## 2 1 1 20 NA 0 Accessories Accessories ## 3 0 5 05 NA 238 Sign In Shop by Brand ## 4 0 1 17 NA 0 Apparel Apparel ## 5 8 1 04 NA 0 Shop by Brand Shop by Brand ## 6 0 5 00 NA 110 Apparel Lifestyle ## country quantity revenue purchase_flag user_rating ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;dbl&gt; ## 1 France 0 0 FALSE 4 ## 2 Norway 0 0 FALSE 5 ## 3 India 0 0 FALSE 4 ## 4 Ukraine 0 0 FALSE 5 ## 5 Chile 0 0 FALSE 4 ## 6 United Kingdom 0 0 FALSE 4 10.2.2 Data Dictionary Column Description device Device used to browse the website os Operating system of the device browser Browser used to visit the website user_type New or returning visitor channel Source of traffic gender Gender of the visitor frequency Count of visits to the website recency Number of days since last visit page_depth Number of website pages browsed hour_of_day Hour of day age Age of the visitor duration Time spent on the website (in seconds) landing_page Page on which visitor landed exit_page Page on which visitor exited country Country of origin city City of the visitor quantity Number of units purchased revenue Total revenue purchase_flag Whether the visitor checked out? user_rating Website UI rating given by visitor 10.3 Factors In this very important section, we will learn how R stores categorical data checks if given data is categorical converts other data types to factor handles missing values in categorical data specifies the orders of the categories/levels stores ordinal data 10.3.1 Introduction In R, categorical data is stored as factor. Before we explore the factor family of functions, let us generate the sample data we will use in this module. We will generate the device column from the case study data set using the sample() function. We provide the following inputs to generate the data: values from which the data must be generated the size of the sample indicate if the values must be repeated (TRUE/FALSE) device &lt;- sample(c(&quot;Desktop&quot;, &quot;Mobile&quot;, &quot;Tablet&quot;), size = 25, replace = TRUE) device ## [1] &quot;Tablet&quot; &quot;Mobile&quot; &quot;Tablet&quot; &quot;Mobile&quot; &quot;Mobile&quot; &quot;Desktop&quot; &quot;Tablet&quot; ## [8] &quot;Tablet&quot; &quot;Tablet&quot; &quot;Mobile&quot; &quot;Desktop&quot; &quot;Tablet&quot; &quot;Mobile&quot; &quot;Desktop&quot; ## [15] &quot;Desktop&quot; &quot;Desktop&quot; &quot;Tablet&quot; &quot;Tablet&quot; &quot;Desktop&quot; &quot;Desktop&quot; &quot;Mobile&quot; ## [22] &quot;Desktop&quot; &quot;Desktop&quot; &quot;Tablet&quot; &quot;Tablet&quot; 10.3.2 Membership Testing Great! We have successfully generated the sample data and along the way learnt a new R function for sampling. First, let us check if the sample is a factor using the membership function is.factor(). is.factor(device) ## [1] FALSE Membership testing functions always have the prefix is_ and return only logical values. If the object is a member of the specified class, they return TRUE else FALSE. Since our sample data is not stored as a factor, R has returned FALSE. 10.3.3 Coercion Let us try to coerce it into factor using the coercion function as.factor(). as.factor(device) ## [1] Tablet Mobile Tablet Mobile Mobile Desktop Tablet Tablet Tablet ## [10] Mobile Desktop Tablet Mobile Desktop Desktop Desktop Tablet Tablet ## [19] Desktop Desktop Mobile Desktop Desktop Tablet Tablet ## Levels: Desktop Mobile Tablet Do you spot any difference in the output? In the last line, it displays the levels or categories of the variable. Dont worry if you didnt spot it. We are just getting started and you will pick it up by the end of this section. Another function that can be used to coerce data into factor is as_factor() from the forcats package. as_factor(device) ## [1] Tablet Mobile Tablet Mobile Mobile Desktop Tablet Tablet Tablet ## [10] Mobile Desktop Tablet Mobile Desktop Desktop Desktop Tablet Tablet ## [19] Desktop Desktop Mobile Desktop Desktop Tablet Tablet ## Levels: Tablet Mobile Desktop Did you notice any difference between these two functions? Focus on the last line of the output where the levels are displayed. Now observe the order of the levels. as.factor() displays levels in the alphabetical order whereas as_factor() displays them in order of appearance in the data. Mobile, followed by Tablet, and then Desktop. If you look at the data, they appear in the same order. 10.3.4 Factor Function If you want finer control while creating factors, use the factor() function. as.factor() should suffice in most cases but use factor() when you want to: specify levels modify labels include NA as a level/category create ordered factors specify order of levels The first input is a vector, usually a numeric or character vector with a small number of unique values. In our example, it is a character vector of length 25 (i.e. 25 values) but 3 unique values. factor(device) ## [1] Tablet Mobile Tablet Mobile Mobile Desktop Tablet Tablet Tablet ## [10] Mobile Desktop Tablet Mobile Desktop Desktop Desktop Tablet Tablet ## [19] Desktop Desktop Mobile Desktop Desktop Tablet Tablet ## Levels: Desktop Mobile Tablet If you want to specify the levels or categories, use the levels argument. factor(device, levels = c(&quot;Desktop&quot;, &quot;Mobile&quot;, &quot;Tablet&quot;)) ## [1] Tablet Mobile Tablet Mobile Mobile Desktop Tablet Tablet Tablet ## [10] Mobile Desktop Tablet Mobile Desktop Desktop Desktop Tablet Tablet ## [19] Desktop Desktop Mobile Desktop Desktop Tablet Tablet ## Levels: Desktop Mobile Tablet Levels not specified will be replaced by NA. Let us specify only Desktop and Mobile as the levels in the device column and see what happens. factor(device, levels = c(&quot;Desktop&quot;, &quot;Mobile&quot;)) ## [1] &lt;NA&gt; Mobile &lt;NA&gt; Mobile Mobile Desktop &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## [10] Mobile Desktop &lt;NA&gt; Mobile Desktop Desktop Desktop &lt;NA&gt; &lt;NA&gt; ## [19] Desktop Desktop Mobile Desktop Desktop &lt;NA&gt; &lt;NA&gt; ## Levels: Desktop Mobile As you can see, Tablet has been replaced by NA. 10.3.5 Modify Labels You can change the labels of the levels using the labels argument. The labels must be in the same order as the levels. We will modify the labels to Desk, Mob &amp; Tab for Desktop, Mobile &amp; Tablet respectively. factor(device, levels = c(&quot;Desktop&quot;, &quot;Mobile&quot;, &quot;Tablet&quot;), labels = c(&quot;Desk&quot;, &quot;Mob&quot;, &quot;Tab&quot;)) ## [1] Tab Mob Tab Mob Mob Desk Tab Tab Tab Mob Desk Tab Mob Desk Desk ## [16] Desk Tab Tab Desk Desk Mob Desk Desk Tab Tab ## Levels: Desk Mob Tab You can see that not only the values but the levels are also modified. 10.3.6 Missing Values Let us regenerate the device column but include some missing values (NA) deliberately to see how factor() handles them. # sample with missing values device &lt;- sample(c(&quot;Desktop&quot;, &quot;Mobile&quot;, &quot;Tablet&quot;, NA), size = 25, replace = TRUE) device ## [1] NA &quot;Desktop&quot; NA &quot;Tablet&quot; &quot;Tablet&quot; &quot;Desktop&quot; &quot;Desktop&quot; ## [8] &quot;Tablet&quot; &quot;Tablet&quot; &quot;Desktop&quot; &quot;Mobile&quot; &quot;Desktop&quot; &quot;Mobile&quot; NA ## [15] &quot;Tablet&quot; &quot;Tablet&quot; NA NA &quot;Desktop&quot; &quot;Tablet&quot; &quot;Tablet&quot; ## [22] &quot;Tablet&quot; &quot;Tablet&quot; &quot;Mobile&quot; &quot;Mobile&quot; # store as categorical data factor(device) ## [1] &lt;NA&gt; Desktop &lt;NA&gt; Tablet Tablet Desktop Desktop Tablet Tablet ## [10] Desktop Mobile Desktop Mobile &lt;NA&gt; Tablet Tablet &lt;NA&gt; &lt;NA&gt; ## [19] Desktop Tablet Tablet Tablet Tablet Mobile Mobile ## Levels: Desktop Mobile Tablet NA is not shown as one of the levels. Why does this happen? By default, it will ignore them. If you look at the arguments of the factor() function, the exclude argument is set to NA by default i.e. NA is excluded automatically. What should we do to ensure that NA is also treated as a level? In order to treat NA as a level, set the exclude argument to NULL. factor(device, exclude = NULL) ## [1] &lt;NA&gt; Desktop &lt;NA&gt; Tablet Tablet Desktop Desktop Tablet Tablet ## [10] Desktop Mobile Desktop Mobile &lt;NA&gt; Tablet Tablet &lt;NA&gt; &lt;NA&gt; ## [19] Desktop Tablet Tablet Tablet Tablet Mobile Mobile ## Levels: Desktop Mobile Tablet &lt;NA&gt; As you can see, NA is displayed as one of the levels in the data. 10.3.7 Ordered Factors So far, we have been looking at nominal data. Let us now explore how R handles ordered data. We will generate a new data set of satisfaction ratings to use in this section. Satisfaction ratings are widely used to measure a customers satisfaction with an organization, service or a product. rating &lt;- sample(c(&quot;Dislike&quot;, &quot;Neutral&quot;, &quot;Like&quot;), size = 25, replace = TRUE) rating ## [1] &quot;Dislike&quot; &quot;Neutral&quot; &quot;Like&quot; &quot;Like&quot; &quot;Neutral&quot; &quot;Neutral&quot; &quot;Like&quot; ## [8] &quot;Dislike&quot; &quot;Neutral&quot; &quot;Neutral&quot; &quot;Like&quot; &quot;Like&quot; &quot;Like&quot; &quot;Dislike&quot; ## [15] &quot;Dislike&quot; &quot;Neutral&quot; &quot;Dislike&quot; &quot;Neutral&quot; &quot;Like&quot; &quot;Like&quot; &quot;Neutral&quot; ## [22] &quot;Dislike&quot; &quot;Neutral&quot; &quot;Like&quot; &quot;Dislike&quot; It consists of three values Dislike, Neutral &amp; Like in that order. You can see that there is an intrinsic order here. Like is better than neutral which in turn is better than dislike. While we can order them, we cant quantify the difference between them. We cant say neutral is so many times better than dislike. Membership Testing As we did earlier, let us check if the data is ordered using the membership function is.ordered(). is.ordered(rating) ## [1] FALSE R returns FALSE as the variable rating is not ordered. Let us use as.ordered() to coerce it into an ordered factor. as.ordered(rating) ## [1] Dislike Neutral Like Like Neutral Neutral Like Dislike Neutral ## [10] Neutral Like Like Like Dislike Dislike Neutral Dislike Neutral ## [19] Like Like Neutral Dislike Neutral Like Dislike ## Levels: Dislike &lt; Like &lt; Neutral Look at the last line where the levels are displayed. In case of ordered factors, you will see a &lt; between the labels. This is used to indicate the order of the levels. Now rating is both an ordered but the order of the levels is not correct. It should be Dislike &lt; Neutral &lt; Like but is displayed in order of appearance in the data. Let us use the factor() function since we need more control over how the levels are ranked and set the ordered argument to TRUE. factor(rating, ordered = TRUE) ## [1] Dislike Neutral Like Like Neutral Neutral Like Dislike Neutral ## [10] Neutral Like Like Like Dislike Dislike Neutral Dislike Neutral ## [19] Like Like Neutral Dislike Neutral Like Dislike ## Levels: Dislike &lt; Like &lt; Neutral The ranking of the levels has not changed and is still the same. Why is this happening? If you observe carefully, the ranking follows the alphabetical order (Desktop, Mobile, Table). The factor() function uses the same order for the levels. 10.3.8 Modify Order of Levels To change the order/ranking of the levels, we need to specify it using the levels argument. Let us do that in the next example. factor(rating, levels = c(&quot;Dislike&quot;, &quot;Neutral&quot;, &quot;Like&quot;), ordered = TRUE) ## [1] Dislike Neutral Like Like Neutral Neutral Like Dislike Neutral ## [10] Neutral Like Like Like Dislike Dislike Neutral Dislike Neutral ## [19] Like Like Neutral Dislike Neutral Like Dislike ## Levels: Dislike &lt; Neutral &lt; Like Now, you can see that the levels are ranked correctly. The ordered() function can also be used to create ordered factors. Let us recreate the previous example using the ordered() function. ordered(rating, levels = c(&quot;Dislike&quot;, &quot;Neutral&quot;, &quot;Like&quot;)) ## [1] Dislike Neutral Like Like Neutral Neutral Like Dislike Neutral ## [10] Neutral Like Like Like Dislike Dislike Neutral Dislike Neutral ## [19] Like Like Neutral Dislike Neutral Like Dislike ## Levels: Dislike &lt; Neutral &lt; Like You can specify levels, modify labels and handle missing values using the ordered() function as well. 10.3.9 Key Functions 10.3.10 Summary R uses factor to handle categorical data. Use as.factor() or as_factor() to coerce other data types to factor. Use is.factor() or is.ordered() to identify factor &amp; ordered factor respectively. Use factor() to specify labels modify labels handle missing data create ordered factors specify order of levels Use ordered() to create ordered factors. 10.3.11 Your Turn Use analytics_raw.rds data set to answer the below questions. Check whether the below variables are factor device page_depth landing_page Coerce the following variables to type factor device os browser user_type channel gender landing_page exit_page city country user_type Use only the following levels in the gender column: male female Include NA as a level in the gender column. Change label of NA to missing in the gender column. Change the labels of the levels in user_type column to New Returning Check if the user_rating column is ordered. If not, coerce it to type ordered factor. 10.3.12 Summarize 10.3.13 Introduction Categorical data cannot be summarized in the same way as numeric data. It does not make sense to look at range, standard deviation etc. since data consists of a few distinct values only. So how do we summarize such data? We can look at count/frequency proportion cumulative frequency cross table contingency table etc. In this section, we will explore the above ways of summarizing categorical data. We will also spend some time learning about tables as you will be using them extensively while working with categorical data. R has many packages for tabulating data and we list and explore all of them in the last section of this chapter. 10.3.14 Number of Categories From our case study, we want to know the number of devices used to browse the website, the name of the devices and the proportion of traffic they drive to our website. Let us begin with the number of devices. To view the number of groups/categories in a categorical variable, use nlevels(). nlevels(data$device) ## [1] 3 There are 3 categories of devices used by the visitors to browse the website. This can also be used for data sanitization i.e. as an analyst you know that there are only 3 valid categories of device into which any visitor can be classified into. If you see more than 3 categories, you might want to check if there are any issues in data collection or processing. Now that we know there are 3 categories of devices, let us check if they are valid. The levels() function will return the labels of the groups. 10.3.15 Category Names Knowing the number of levels is useful but not sufficient. levels() is one of the most useful functions when it comes to dealing with categorical data. levels(data$device) ## [1] &quot;Desktop&quot; &quot;Mobile&quot; &quot;Tablet&quot; Other functions that you can use include unique() and fct_unique(). Both these functions will return the unique names/labels along with the levels while levels() returns the labels of the levels. unique(data$device) ## [1] Desktop Mobile Tablet ## Levels: Desktop Mobile Tablet fct_unique(data$device) ## [1] Desktop Mobile Tablet ## Levels: Desktop Mobile Tablet 10.3.16 Names &amp; Counts So we have checked the number of devices and their names. Let us now examine their distribution i.e. count/frequency. table() and summary() will display the levels and their counts while fct_count() will return a tibble with 2 columns (level &amp; count). It is extremely useful for further data processing or visualization (using ggplot2). table(data$device) ## ## Desktop Mobile Tablet ## 177282 63482 3634 fct_count(data$device) ## # A tibble: 3 x 2 ## f n ## &lt;fct&gt; &lt;int&gt; ## 1 Desktop 177282 ## 2 Mobile 63482 ## 3 Tablet 3634 summary(data$device) ## Desktop Mobile Tablet ## 177282 63482 3634 10.3.17 Tables In the previous section, we used the table() function to tabulate categorical data. We will recreate the tabulation for device and store it in a new variable tab. tab &lt;- table(data$device) tab ## ## Desktop Mobile Tablet ## 177282 63482 3634 What does this function return? It is not a vector, list, data.frame or matrix. Let us use the class() function to check the class of the object returned by table(). It returns an object of the class table. This is a new type of object. Let us spend some time understanding tables as they are useful for organizing and summarizing categorical data. table is also the most used object when it comes to dealing with categorical data. The table() function returns the counts of the categories but let us say we want to view the proportion or percentage instead of counts i.e. the proportion or percentage of traffic driven to our website by the different devices. The proportions() or prop.table() function comes in handy in such cases. It takes a table object as input (tab in our case). prop.table(tab) ## ## Desktop Mobile Tablet ## 0.72538237 0.25974844 0.01486919 proportions(tab) ## ## Desktop Mobile Tablet ## 0.72538237 0.25974844 0.01486919 To get the percentages, multiply the output by 100. Use the round() function to round the decimal places according to your requirements. proportions(tab) * 100 ## ## Desktop Mobile Tablet ## 72.538237 25.974844 1.486919 round(proportions(tab) * 100, 2) ## ## Desktop Mobile Tablet ## 72.54 25.97 1.49 So far, we have used table() to tabulate a single categorical variable. It can be used for a lot more than just tabulating data. We can examine the relationship between two categorical variables as well as create multidimensional tables. Let us look at the relationship between gender and device in our case study. Does gender affect the type of device used? To answer this, we will create a two way or cross table. In the table() function, we can specify multiple variables by separating them with a comma. tab2 &lt;- table(data$gender, data$device) tab2 ## ## Desktop Mobile Tablet ## female 32803 7268 494 ## male 46418 14503 696 ## &lt;NA&gt; 98061 41711 2444 Keep in mind that the order of the variables matter. Rows represent the first variable while column represents the second. table(data$device, data$gender) ## ## female male &lt;NA&gt; ## Desktop 32803 46418 98061 ## Mobile 7268 14503 41711 ## Tablet 494 696 2444 The proportions() function works with two way tables as well. proportions(tab2) ## ## Desktop Mobile Tablet ## female 0.134219593 0.029738378 0.002021293 ## male 0.189927904 0.059341729 0.002847814 ## &lt;NA&gt; 0.401234871 0.170668336 0.010000082 proportions(tab2) * 100 ## ## Desktop Mobile Tablet ## female 13.4219593 2.9738378 0.2021293 ## male 18.9927904 5.9341729 0.2847814 ## &lt;NA&gt; 40.1234871 17.0668336 1.0000082 We would like to introduce another function at this point of time, margin.table(). What does this function do? It computes the marginal frequencies i.e. the sum of the rows or columns. It takes a table object as input. The margin argument allows us to specify whether we want the sum of rows or columns. 1 indicates rows and 2 indicates columns. margin.table(tab2, 1) # sum of rows ## ## female male &lt;NA&gt; ## 40565 61617 142216 margin.table(tab2, 2) # sum of columns ## ## Desktop Mobile Tablet ## 177282 63482 3634 If the margin argument is NULL (which it is by default), the function returns the sum of all cells of the table. margin.table(tab2) ## [1] 244398 table() does not display row or column labels. It does display the group labels though. Let us revisit the output from tab2. You can observe that while it includes the group labels, the row and column labels are missing. The output from the dimnames() function shows the group labels of the variables but the row &amp; column labels are absent. dimnames(tab2) ## [[1]] ## [1] &quot;female&quot; &quot;male&quot; NA ## ## [[2]] ## [1] &quot;Desktop&quot; &quot;Mobile&quot; &quot;Tablet&quot; names(tab2) ## NULL names(dimnames(tab2)) ## [1] &quot;&quot; &quot;&quot; The output from names(dimnames(tab2)) is also empty. Let us add the variable names as the row &amp; column labels to tab2. names(dimnames(tab2)) &lt;- c(&quot;Gender&quot;, &quot;Device&quot;) tab2 ## Device ## Gender Desktop Mobile Tablet ## female 32803 7268 494 ## male 46418 14503 696 ## &lt;NA&gt; 98061 41711 2444 Now look at the output from tab2 and you can observe the difference. The same is also visible when we run dimnames(tab2). dimnames(tab2) ## $Gender ## [1] &quot;female&quot; &quot;male&quot; NA ## ## $Device ## [1] &quot;Desktop&quot; &quot;Mobile&quot; &quot;Tablet&quot; To add margin totals to the table, use addmargins(). Like proportions() and margin.table(), it also takes a table object as the input. addmargins(tab2) ## Device ## Gender Desktop Mobile Tablet Sum ## female 32803 7268 494 40565 ## male 46418 14503 696 61617 ## &lt;NA&gt; 98061 41711 2444 142216 ## Sum 177282 63482 3634 244398 rowSums() returns the row total while colSums() returns the column total. They are similar to margin.table(). rowSums(tab2) ## female male &lt;NA&gt; ## 40565 61617 142216 colSums(tab2) ## Desktop Mobile Tablet ## 177282 63482 3634 xtabs() is another way of creating multidimensional tables in R. In comparison to table(), it uses formula notation for input the data argument ensures variable names are referenced instead of using $ i.e. data$variable displays row &amp; column labels by default tabx &lt;- xtabs(~gender+device, data = data) tabx ## device ## gender Desktop Mobile Tablet ## female 32803 7268 494 ## male 46418 14503 696 ## &lt;NA&gt; 98061 41711 2444 The following functions work with xtabs() as well proportions() margin.table() addmargins() proportions(tabx) ## device ## gender Desktop Mobile Tablet ## female 0.134219593 0.029738378 0.002021293 ## male 0.189927904 0.059341729 0.002847814 ## &lt;NA&gt; 0.401234871 0.170668336 0.010000082 margin.table(tabx, 1) ## gender ## female male &lt;NA&gt; ## 40565 61617 142216 margin.table(tabx, 2) ## device ## Desktop Mobile Tablet ## 177282 63482 3634 addmargins(tabx) ## device ## gender Desktop Mobile Tablet Sum ## female 32803 7268 494 40565 ## male 46418 14503 696 61617 ## &lt;NA&gt; 98061 41711 2444 142216 ## Sum 177282 63482 3634 244398 So far, we have been working with one or two dimensional tables. Both the table() and xtabs() functions are capable of creating multidimensional tables. Keep in mind that multidimensional tables are complex and it becomes increasingly difficult to understand or interpret them. tab3 &lt;- xtabs(~gender+device+channel, data = data) tab3 ## , , channel = (Other) ## ## device ## gender Desktop Mobile Tablet ## female 786 258 0 ## male 1063 507 19 ## &lt;NA&gt; 2173 1186 81 ## ## , , channel = Affiliates ## ## device ## gender Desktop Mobile Tablet ## female 1314 60 0 ## male 1714 169 0 ## &lt;NA&gt; 3518 548 65 ## ## , , channel = Direct ## ## device ## gender Desktop Mobile Tablet ## female 4785 977 59 ## male 7010 2381 95 ## &lt;NA&gt; 15824 8292 430 ## ## , , channel = Display ## ## device ## gender Desktop Mobile Tablet ## female 123 753 104 ## male 210 491 73 ## &lt;NA&gt; 554 911 156 ## ## , , channel = Organic Search ## ## device ## gender Desktop Mobile Tablet ## female 17109 4480 282 ## male 25016 9563 448 ## &lt;NA&gt; 54071 27223 1476 ## ## , , channel = Paid Search ## ## device ## gender Desktop Mobile Tablet ## female 645 230 22 ## male 887 478 26 ## &lt;NA&gt; 1274 782 51 ## ## , , channel = Referral ## ## device ## gender Desktop Mobile Tablet ## female 7387 74 0 ## male 9251 185 0 ## &lt;NA&gt; 18052 615 51 ## ## , , channel = Social ## ## device ## gender Desktop Mobile Tablet ## female 654 436 27 ## male 1267 729 35 ## &lt;NA&gt; 2595 2154 134 ftable stands for flat tables and is useful for printing attractive tables. It makes it easy to read and interpret multidimensional tables. In the next example, we will use ftable() to print the tables we have created in the previous examples and compare the outputs. ftable(tabx) ## device Desktop Mobile Tablet ## gender ## female 32803 7268 494 ## male 46418 14503 696 ## NA 98061 41711 2444 ftable(tab2) ## Device Desktop Mobile Tablet ## Gender ## female 32803 7268 494 ## male 46418 14503 696 ## NA 98061 41711 2444 ftable(tab3) ## channel (Other) Affiliates Direct Display Organic Search Paid Search Referral Social ## gender device ## female Desktop 786 1314 4785 123 17109 645 7387 654 ## Mobile 258 60 977 753 4480 230 74 436 ## Tablet 0 0 59 104 282 22 0 27 ## male Desktop 1063 1714 7010 210 25016 887 9251 1267 ## Mobile 507 169 2381 491 9563 478 185 729 ## Tablet 19 0 95 73 448 26 0 35 ## NA Desktop 2173 3518 15824 554 54071 1274 18052 2595 ## Mobile 1186 548 8292 911 27223 782 615 2154 ## Tablet 81 65 430 156 1476 51 51 134 By default, missing values (NAs) are excluded from tables. Let us modify the gender data from our case study a bit and see how the table() function deals with missing values. We wont explicitly specify NA as a level while recreating the gender data. gen &lt;- as.factor(as.character(data$gender)) table(gen) ## gen ## female male ## 40565 61617 As you can see, table() excludes missing values while tabulating the data. In order to ensure that missing values are also counted, we can use the useNA argument. It can take two values: ifany always In the first case, it will show NA as a level and the count only if there are missing values in the data. In the second case, it will always show NA as a level irrespective of whether there are missing values in the data or not. table(gen, useNA = &quot;ifany&quot;) ## gen ## female male &lt;NA&gt; ## 40565 61617 142216 table(data$device, useNA = &quot;always&quot;) ## ## Desktop Mobile Tablet &lt;NA&gt; ## 177282 63482 3634 0 In this final section on tables, we will learn how to select/access the different parts of a table. We will use [ operator to select rows and columns of a table (it is similar to selecting data from a data.frame). Below are a few examples: select first row tab2[1, ] ## Desktop Mobile Tablet ## 32803 7268 494 select first column tab2[, 1] ## female male &lt;NA&gt; ## 32803 46418 98061 select first two rows tab2[1:2, ] ## Device ## Gender Desktop Mobile Tablet ## female 32803 7268 494 ## male 46418 14503 696 select first two columns tab2[, 1:2] ## Device ## Gender Desktop Mobile ## female 32803 7268 ## male 46418 14503 ## &lt;NA&gt; 98061 41711 select nth row tab2[2, ] ## Desktop Mobile Tablet ## 46418 14503 696 select nth column tab2[, 2] ## female male &lt;NA&gt; ## 7268 14503 41711 select row by group label tab2[&quot;female&quot;, ] ## Desktop Mobile Tablet ## 32803 7268 494 select column by group label tab2[, &quot;Mobile&quot;] ## female male &lt;NA&gt; ## 7268 14503 41711 Before we end this section, let us learn how to test if an object is of class table using is.table(). is.table(tab2) ## [1] TRUE Next, we will look at different R packages for two way/contingency tables. 10.3.18 Contingency Table For cross tables with output similar to SAS or SPSS, use any of the below: CrossTable() from the gmodels package ds_cross_table() from the descriptr package gmodels::CrossTable(data$device, data$gender) ## ## ## Cell Contents ## |-------------------------| ## | N | ## | Chi-square contribution | ## | N / Row Total | ## | N / Col Total | ## | N / Table Total | ## |-------------------------| ## ## ## Total Observations in Table: 102182 ## ## ## | data$gender ## data$device | female | male | Row Total | ## -------------|-----------|-----------|-----------| ## Desktop | 32803 | 46418 | 79221 | ## | 58.228 | 38.334 | | ## | 0.414 | 0.586 | 0.775 | ## | 0.809 | 0.753 | | ## | 0.321 | 0.454 | | ## -------------|-----------|-----------|-----------| ## Mobile | 7268 | 14503 | 21771 | ## | 218.694 | 143.975 | | ## | 0.334 | 0.666 | 0.213 | ## | 0.179 | 0.235 | | ## | 0.071 | 0.142 | | ## -------------|-----------|-----------|-----------| ## Tablet | 494 | 696 | 1190 | ## | 0.986 | 0.649 | | ## | 0.415 | 0.585 | 0.012 | ## | 0.012 | 0.011 | | ## | 0.005 | 0.007 | | ## -------------|-----------|-----------|-----------| ## Column Total | 40565 | 61617 | 102182 | ## | 0.397 | 0.603 | | ## -------------|-----------|-----------|-----------| ## ## descriptr::ds_cross_table(data, device, gender) ## Cell Contents ## |---------------| ## | Frequency | ## | Percent | ## | Row Pct | ## | Col Pct | ## |---------------| ## ## Total Observations: 244398 ## ## ---------------------------------------------------------------------------- ## | | gender | ## ---------------------------------------------------------------------------- ## | device | female | male | NA | Row Total | ## ---------------------------------------------------------------------------- ## | Desktop | 32803 | 46418 | 98061 | 177282 | ## | | 0.134 | 0.19 | 0.401 | | ## | | 0.19 | 0.26 | 0.55 | 0.73 | ## | | 0.81 | 0.75 | 0.69 | | ## ---------------------------------------------------------------------------- ## | Mobile | 7268 | 14503 | 41711 | 63482 | ## | | 0.03 | 0.059 | 0.171 | | ## | | 0.11 | 0.23 | 0.66 | 0.26 | ## | | 0.18 | 0.24 | 0.29 | | ## ---------------------------------------------------------------------------- ## | Tablet | 494 | 696 | 2444 | 3634 | ## | | 0.002 | 0.003 | 0.01 | | ## | | 0.14 | 0.19 | 0.67 | 0.01 | ## | | 0.01 | 0.01 | 0.02 | | ## ---------------------------------------------------------------------------- ## | Column Total | 40565 | 61617 | 142216 | 244398 | ## | | 0.166 | 0.252 | 0.582 | | ## ---------------------------------------------------------------------------- We list and explore different R packages for summarizing categorical data in the last section of this chapter. 10.3.19 Key Functions 10.3.20 Your Turn Display the number of levels in browser channel landing_page exit_page Display the categories in os channel browser gender user_type Display the count/frequency of channel user_type Examine the distribution of the following and summarize your observations: channel by user_type device by purchase_flag channel by device channel by purchase_flag user_type by purchase_flag 10.4 Data Manipulation 10.4.1 Introduction In this section, our focus will be on handling the levels of a categorical variable, and exploring the forcats package for the same. We will basically look at 3 key operations or transformations we would like to do when it comes to factors which are: change value of levels add or remove levels change order of levels Before we start working with the value of the levels, let us take a quick look at some of the functions we used in the previous sections. We will store the source of traffic as channel instead of referring to the column in the data.frame every time. channel &lt;- data$channel Let us go back to the function we used for tabulating data, fct_count(). If you observe the result, it is in the same order as displayed by levels(). fct_count(channel) ## # A tibble: 8 x 2 ## f n ## &lt;fct&gt; &lt;int&gt; ## 1 (Other) 6073 ## 2 Affiliates 7388 ## 3 Direct 39853 ## 4 Display 3375 ## 5 Organic Search 139668 ## 6 Paid Search 4395 ## 7 Referral 35615 ## 8 Social 8031 If you want to sort the results by the count i.e. most common level comes at the top, use the sort argument. fct_count(channel, sort = TRUE) ## # A tibble: 8 x 2 ## f n ## &lt;fct&gt; &lt;int&gt; ## 1 Organic Search 139668 ## 2 Direct 39853 ## 3 Referral 35615 ## 4 Social 8031 ## 5 Affiliates 7388 ## 6 (Other) 6073 ## 7 Paid Search 4395 ## 8 Display 3375 If you want to view the proportion along with the count, set the prop argument to TRUE. fct_count(channel, prop = TRUE) ## # A tibble: 8 x 3 ## f n p ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; ## 1 (Other) 6073 0.0248 ## 2 Affiliates 7388 0.0302 ## 3 Direct 39853 0.163 ## 4 Display 3375 0.0138 ## 5 Organic Search 139668 0.571 ## 6 Paid Search 4395 0.0180 ## 7 Referral 35615 0.146 ## 8 Social 8031 0.0329 One of the important steps in data preparation/sanitization is to check if the levels are valid i.e. only levels which should be present in the data are actually present. fct_match() can be used to check validity of levels. It returns a logical vector if the level is present and an error if not. table(fct_match(channel, &quot;Social&quot;)) ## ## FALSE TRUE ## 236367 8031 10.4.2 Your Turn Display the count/frequency of the following variables in the descending order device landing_page exit_page Check if laptop is a level in the device column. 10.4.3 Change Value of Levels In this section, we will learn how to change the value of the levels. In order to keep it interesting, we will state an objective from our case study and then map it into a function from the forcats package. 10.4.4 Combine both Paid &amp; Organic Search into a single level, Search In this case, we want to change the value of two levels, Paid Search &amp; Organic Search and give them the common value Search. You can also look at it as collapsing two levels into one. There are two functions we can use here: fct_collapse() fct_recode() Let us look at fct_collapse() first. After specifying the categorical variable, we specify the new value followed by a character vector of the existing values. Remember, the new value is not enclosed in quotes (single or double) but the existing values must be a character vector. fct_count( fct_collapse( channel, Search = c(&quot;Paid Search&quot;, &quot;Organic Search&quot;) ) ) ## # A tibble: 7 x 2 ## f n ## &lt;fct&gt; &lt;int&gt; ## 1 (Other) 6073 ## 2 Affiliates 7388 ## 3 Direct 39853 ## 4 Display 3375 ## 5 Search 144063 ## 6 Referral 35615 ## 7 Social 8031 In the case of fct_recode(), each value being changed must be specified in a new line. Similar to fct_collapse(), the new value is not enclosed in quotes but the existing values must be. fct_count( fct_recode( channel, Search = &quot;Paid Search&quot;, Search = &quot;Organic Search&quot; ) ) ## # A tibble: 7 x 2 ## f n ## &lt;fct&gt; &lt;int&gt; ## 1 (Other) 6073 ## 2 Affiliates 7388 ## 3 Direct 39853 ## 4 Display 3375 ## 5 Search 144063 ## 6 Referral 35615 ## 7 Social 8031 The dplyr and car packages also have recode functions. Retain only those channels which have driven a minimum traffic of 5000 to the website Instead of having all the channels, we desire to retain only those channels which have driven at least 5000 visits to the website. What about the rest of the channels which have driven less than 5000? We will recategorize them as Other. Keep in mind that we already have a (Other) level in our data. fct_lump_min() will lump together all levels which do not have a minimum count specified. In our case study, only Display drives less than 5000 visits and it will be categorized into Other. fct_count(fct_lump_min(channel, 5000)) ## # A tibble: 7 x 2 ## f n ## &lt;fct&gt; &lt;int&gt; ## 1 (Other) 6073 ## 2 Affiliates 7388 ## 3 Direct 39853 ## 4 Organic Search 139668 ## 5 Referral 35615 ## 6 Social 8031 ## 7 Other 7770 Retain only top 3 referring channels and categorize rest into Other Suppose you decide to retain only the top 3 channels in terms of the traffic driven to the website. In our case study, these are Direct, Organic Search and Referral. We want to retain these 3 levels and categorize the rest as Other. fct_lump_n() will retain top n levels by count/frequency and lump the rest into Other. fct_count(fct_lump_n(channel, 3)) ## # A tibble: 4 x 2 ## f n ## &lt;fct&gt; &lt;int&gt; ## 1 Direct 39853 ## 2 Organic Search 139668 ## 3 Referral 35615 ## 4 Other 29262 In our case, n is 3 and hence the top 3 channels in terms of traffic driven are retained while the rest are lumped into Other. Retain only those channels which have driven at least 2% of the overall traffic In the second scenario above, we retained channels based on minimum traffic driven by them to the website. The criteria was count of visits. If you want to specify the criteria as a percentage or proportion instead of count, use fct_lump_prop(). The criteria is a value between 0 and 1. In our case study, we want to retain channels that have driven at least 2% of the overall traffic. Hence, we have specified the criteria as 0.02. fct_count(fct_lump_prop(channel, 0.02)) ## # A tibble: 7 x 2 ## f n ## &lt;fct&gt; &lt;int&gt; ## 1 (Other) 6073 ## 2 Affiliates 7388 ## 3 Direct 39853 ## 4 Organic Search 139668 ## 5 Referral 35615 ## 6 Social 8031 ## 7 Other 7770 As you can see, only Display drives less than 2% of overall traffic and has been lumped into Other. Retain the following channels and merge the rest into Other Organic Search Direct Referral In the previous scenarios, we have been retaining or lumping channels based on some criteria like count or percentage of traffic driven to the website. In this scenario, we want to retain certain levels by specifying their labels and combine the rest into Other. While we can use fct_collapse() or fct_recode(), a more appropriate function would be fct_other(). We will do a comparison of the three functions in a short while. fct_other() has two arguments, keep and drop. keep is used when we know the levels we want to retain and drop is used when we know the levels we want to drop. In this scenario, we know the levels we want to retain and hence we will use the keep argument and specify them. Organic Search, Direct and Referral will be retained while the rest of the channels will be lumped into Other. fct_count( fct_other( channel, keep = c(&quot;Organic Search&quot;, &quot;Direct&quot;, &quot;Referral&quot;)) ) ## # A tibble: 4 x 2 ## f n ## &lt;fct&gt; &lt;int&gt; ## 1 Direct 39853 ## 2 Organic Search 139668 ## 3 Referral 35615 ## 4 Other 29262 Merge the following channels into Other and retain rest of them: Display Paid Search In this scenario, we know the levels we want to drop and hence we will use the drop argument and specify them. Display and Paid Search will be lumped into Other while the rest of the channels will be retained. fct_count( fct_other( channel, drop = c(&quot;Display&quot;, &quot;Paid Search&quot;) ) ) ## # A tibble: 7 x 2 ## f n ## &lt;fct&gt; &lt;int&gt; ## 1 (Other) 6073 ## 2 Affiliates 7388 ## 3 Direct 39853 ## 4 Organic Search 139668 ## 5 Referral 35615 ## 6 Social 8031 ## 7 Other 7770 In the previous scenario, we said we will compare fct_other() with fct_collapse() and fct_recode(). Let us use the other two functions as well and see the difference. # collapse fct_count( fct_collapse( channel, Other = c(&quot;(Other)&quot;, &quot;Affiliate&quot;, &quot;Display&quot;, &quot;Paid Search&quot;, &quot;Social&quot;) ) ) ## Warning: Unknown levels in `f`: Affiliate ## # A tibble: 5 x 2 ## f n ## &lt;fct&gt; &lt;int&gt; ## 1 Other 21874 ## 2 Affiliates 7388 ## 3 Direct 39853 ## 4 Organic Search 139668 ## 5 Referral 35615 # recode fct_count( fct_recode( channel, Other = &quot;(Other)&quot;, Other = &quot;Affiliate&quot;, Other = &quot;Display&quot;, Other = &quot;Paid Search&quot;, Other = &quot;Social&quot; ) ) ## Warning: Unknown levels in `f`: Affiliate ## # A tibble: 5 x 2 ## f n ## &lt;fct&gt; &lt;int&gt; ## 1 Other 21874 ## 2 Affiliates 7388 ## 3 Direct 39853 ## 4 Organic Search 139668 ## 5 Referral 35615 As you can observe, fct_other() requires less typing and is easier to specify. Anonymize the data set before sharing it with your colleagues Anonymizing data is extremely important when you are sharing sensitive data with others. Here, we want to anonymize the channels which drive traffic to the website so that we can share it with others without divulging the names of the channels. fct_anon() allows us to anonymize the levels in the data. Using the prefix argument, we can specify the prefix to be used while anonymizing the data. fct_count(fct_anon(channel, prefix = &quot;ch_&quot;)) ## # A tibble: 8 x 2 ## f n ## &lt;fct&gt; &lt;int&gt; ## 1 ch_1 7388 ## 2 ch_2 8031 ## 3 ch_3 3375 ## 4 ch_4 4395 ## 5 ch_5 6073 ## 6 ch_6 35615 ## 7 ch_7 39853 ## 8 ch_8 139668 10.4.5 Key Functions Function Description fct_collapse() Collapse factor levels fct_recode() Recode factor levels fct_lump_min() Lump factor levels with count lesser than specified value fct_lump_n() Lump all levels except the top n levels fct_lump_prop() Lump factor levels with count lesser than specified proportion fct_lump_lowfreq() Lump together least frequent levels fct_other() Replace levels with Other level fct_anon() Anonymize factor levels 10.4.6 Your Turn.. Combine the following levels in landing_page into Account My Account Register Sign In Your Info Combine levels in landing_page that drive less than 1000 visits. Get top 10 landing and exit pages. Get landing pages that drive at least 5% of the total traffic to the website. Retain only the following levels in the browser column: Chrome Firefox Safari Edge Anonymize landing and exit page levels. 10.4.7 Add / Remove Levels In this small section, we will learn to: add new levels drop levels make missing values explicit 10.4.8 Add a new level, Blog fct_expand() allows us to add new levels to the data. The label of the new level must be specified after the variable name and must be enclosed in quotes. If the level already exists, it will be ignored. Let us add a new level, Blog. levels(fct_expand(channel, &quot;Blog&quot;)) ## [1] &quot;(Other)&quot; &quot;Affiliates&quot; &quot;Direct&quot; &quot;Display&quot; ## [5] &quot;Organic Search&quot; &quot;Paid Search&quot; &quot;Referral&quot; &quot;Social&quot; ## [9] &quot;Blog&quot; Drop existing level On the other hand, fct_drop() will drop levels which have no values i.e. unused levels. If you want to drop only specific levels, use the only argument and specify the name of the level in quotes. Let us drop the new level we added in the previous example. levels(fct_drop(fct_expand(channel, &quot;Blog&quot;))) ## [1] &quot;(Other)&quot; &quot;Affiliates&quot; &quot;Direct&quot; &quot;Display&quot; ## [5] &quot;Organic Search&quot; &quot;Paid Search&quot; &quot;Referral&quot; &quot;Social&quot; Make missing values explicit In our data set, the gender column has many missing values, and in R, missing values are represented by NA. Suppose you are sharing the data or analysis with someone who is not an R user, and does not know what NA represents. In such a scenario, we can use the fct_explicit_na() function to make the missing values in the gender column explicit i.e. it will appear as (Missing) instead of NA. This will help non R users to understand that there are missing values in the data. fct_count(fct_explicit_na(data$gender)) ## # A tibble: 3 x 2 ## f n ## &lt;fct&gt; &lt;int&gt; ## 1 female 40565 ## 2 male 61617 ## 3 (Missing) 142216 10.4.9 Key Functions Function Description fct_expand() Add additional levels to a factor fct_drop() Drop unused factor levels fct_explicit_na() Make missing values explicit 10.4.10 Change Order of Levels In this last part of this section, we will learn how to change the order of the levels. We will look at the following scenarios from our case study: We want to make Organic Search the first level Referral the third level Display the last level Organic Search is the first level In this scenario, we want the levels to appear in a certain order. In the first case, we want Organic Search to be the first level. fct_relevel() allows us to manually reorder the levels. To move a level to the beginning, specify the label (it must be enclosed in quotes). levels(channel) ## [1] &quot;(Other)&quot; &quot;Affiliates&quot; &quot;Direct&quot; &quot;Display&quot; ## [5] &quot;Organic Search&quot; &quot;Paid Search&quot; &quot;Referral&quot; &quot;Social&quot; levels(fct_relevel(channel, &quot;Organic Search&quot;)) ## [1] &quot;Organic Search&quot; &quot;(Other)&quot; &quot;Affiliates&quot; &quot;Direct&quot; ## [5] &quot;Display&quot; &quot;Paid Search&quot; &quot;Referral&quot; &quot;Social&quot; Referral is the third level The after argument is useful when we want to move the level to the end or anywhere between the beginning and end. In the second case, we want Referral to be the third level. After specifying the label, use the after argument and specify the level after which Referral should appear. Since we want to move it to the third position, we will set the value of after to 2 i.e. Referral should come after the second position. levels(channel) ## [1] &quot;(Other)&quot; &quot;Affiliates&quot; &quot;Direct&quot; &quot;Display&quot; ## [5] &quot;Organic Search&quot; &quot;Paid Search&quot; &quot;Referral&quot; &quot;Social&quot; levels(fct_relevel(channel, &quot;Referral&quot;, after = 2)) ## [1] &quot;(Other)&quot; &quot;Affiliates&quot; &quot;Referral&quot; &quot;Direct&quot; ## [5] &quot;Display&quot; &quot;Organic Search&quot; &quot;Paid Search&quot; &quot;Social&quot; Display is the last level In this last case, we want to move Display to the end. If you know the number of levels, you can specify a value here. In our data, there are eight channels i.e. eight levels, so we can set the value of after to 7. What happens when we do not know the number of levels or if they tend to vary? In such cases, to move a level to the end, set the value of after to Inf. levels(channel) ## [1] &quot;(Other)&quot; &quot;Affiliates&quot; &quot;Direct&quot; &quot;Display&quot; ## [5] &quot;Organic Search&quot; &quot;Paid Search&quot; &quot;Referral&quot; &quot;Social&quot; levels(fct_relevel(channel, &quot;Display&quot;, after = Inf)) ## [1] &quot;(Other)&quot; &quot;Affiliates&quot; &quot;Direct&quot; &quot;Organic Search&quot; ## [5] &quot;Paid Search&quot; &quot;Referral&quot; &quot;Social&quot; &quot;Display&quot; Let us now look at a scenario where we want to order the levels by frequency (largest to smallest) order of appearance (in data) Order levels by frequency In the first case, the levels with the most frequency should appear at the top. fct_infreq() will order the levels by their frequency. # reorder levels levels(channel) ## [1] &quot;(Other)&quot; &quot;Affiliates&quot; &quot;Direct&quot; &quot;Display&quot; ## [5] &quot;Organic Search&quot; &quot;Paid Search&quot; &quot;Referral&quot; &quot;Social&quot; levels(fct_infreq(channel)) ## [1] &quot;Organic Search&quot; &quot;Direct&quot; &quot;Referral&quot; &quot;Social&quot; ## [5] &quot;Affiliates&quot; &quot;(Other)&quot; &quot;Paid Search&quot; &quot;Display&quot; Order levels by appearance In the second case, the order of the levels should be the same as the order of their appearance in the data. fct_inorder() will order the levels according to the order in which they appear in the data. # reorder levels levels(channel) ## [1] &quot;(Other)&quot; &quot;Affiliates&quot; &quot;Direct&quot; &quot;Display&quot; ## [5] &quot;Organic Search&quot; &quot;Paid Search&quot; &quot;Referral&quot; &quot;Social&quot; levels(fct_inorder(channel)) ## [1] &quot;Organic Search&quot; &quot;Direct&quot; &quot;Referral&quot; &quot;Affiliates&quot; ## [5] &quot;(Other)&quot; &quot;Social&quot; &quot;Display&quot; &quot;Paid Search&quot; Reverse the order of the levels The order of the levels can be reversed using fct_rev(). # reorder levels levels(channel) ## [1] &quot;(Other)&quot; &quot;Affiliates&quot; &quot;Direct&quot; &quot;Display&quot; ## [5] &quot;Organic Search&quot; &quot;Paid Search&quot; &quot;Referral&quot; &quot;Social&quot; levels(fct_rev(channel)) ## [1] &quot;Social&quot; &quot;Referral&quot; &quot;Paid Search&quot; &quot;Organic Search&quot; ## [5] &quot;Display&quot; &quot;Direct&quot; &quot;Affiliates&quot; &quot;(Other)&quot; Randomly shuffle the order of the levels The order of the levels can be randomly shuffled using fct_shuffle(). # reorder levels levels(channel) ## [1] &quot;(Other)&quot; &quot;Affiliates&quot; &quot;Direct&quot; &quot;Display&quot; ## [5] &quot;Organic Search&quot; &quot;Paid Search&quot; &quot;Referral&quot; &quot;Social&quot; levels(fct_shuffle(channel)) ## [1] &quot;Affiliates&quot; &quot;Display&quot; &quot;Direct&quot; &quot;Social&quot; ## [5] &quot;Referral&quot; &quot;Paid Search&quot; &quot;(Other)&quot; &quot;Organic Search&quot; 10.4.11 Key Functions Function Description fct_relevel() Reorder factor levels fct_shift() Shift factor levels fct_infreq() Reorder factor levels by frequency fct_rev() Reverse order of factor levels fct_inorder() Reorder factor levels by first appearance fct_shuffle() Randomly shuffle factor levels 10.4.12 Your Turn Make Home first level in the landing_page column. Make Apparel second level in the landing_page column. Make Specials last level in the landing_page column. Order the levels in the browser by frequency: Order the levels in landing page by appearance: Shuffle the levels in os Reverse the levels in browser 10.5 Data Visualization 10.5.1 Introduction In this section, we will learn to visualize categorical data. We will look at the following type of plots: univariate bar plot bivariate bar plot grouped stacked proportional mosaic plot pie chart donut chart We will be using ggplot2 in this chapter. So you should know the basics of data visualization with ggplot2. If you are new or have never used ggplot2, do not worry. We have several tutorials and an ebook on ggplot2, you can go through them first and then come back to this section. 10.5.2 Bar Plot Bar charts provide a visual representation of categorical data. The bars can be plotted either vertically or horizontally. The categories/groups appear along the horizontal X axis and the height of the bar represents a measured value. ggplot(data) + geom_bar(aes(x = device), fill = &quot;blue&quot;) + xlab(&quot;Device&quot;) + ylab(&quot;Count&quot;) In the above example, the bars represent the count/frequency of the categories. If the bars represent continuous data, the value could be mean or sum of the variable being represented. 10.5.3 Grouped Bar Plot A grouped bar chart plots values for two levels of a categorical variable instead of one. You should use grouped bar chart when making comparisons across different categories of data. Use it when you want to look at how the second category variable changes within each level of the first and vice versa. ggplot(data) + geom_bar(aes(x = device, fill = gender), position = &quot;dodge&quot;) + xlab(&quot;Device&quot;) + ylab(&quot;Count&quot;) 10.5.4 Stacked Bar Plot In stacked bar plots, the bars are stacked on top of each other instead of placing them next to each other. Use stacked bar plots while looking at cumulative value. ggplot(data) + geom_bar(aes(x = device, fill = gender)) + xlab(&quot;Device&quot;) + ylab(&quot;Count&quot;) 10.5.5 Proportional Bar Plot Also known as percent stacked plot, the height of all bars in this plot are the same. The distribution of the second categorical variable is scaled to 1 or 100. The length of each bar is determined by its share in the category. Use this when you want to concurrently observe each of several variables as they fluctuate and as their percentage ratios change. data %&gt;% select(device, gender) %&gt;% table() %&gt;% tibble::as_tibble() %&gt;% ggplot(aes(x = device, y = n, fill = gender)) + geom_bar(stat = &quot;identity&quot;, position = &quot;fill&quot;) + xlab(&quot;Device&quot;) + ylab(&quot;Gender&quot;) 10.5.6 Mosaic Plot A mosaic plot is a graphical representation of a two way table or contingency table. It was introduced by Hartigan &amp; Kleiner and is divided into rectangles. Proportions on horizontal axis represents the number of observations for each level of the X variable. The vertical length of each rectangle is proportional to the proportion of Y variable in each level of X variable. ggplot(data = data) + geom_mosaic(aes(x = product(channel, device), fill = channel)) + xlab(&quot;Device&quot;) + ylab(&quot;Channel&quot;) 10.5.7 Pie Chart Pie chart is a circular chart, divided into slices to show relevant sizes of data. It shows the distribution of the different levels of a categorical variable as a circle is divided into radial slices. Each level corresponds with a single slice of the circle and size indicates the proportion of the level. Use it when comparing each groups contribution to the whole as opposed to comparing groups to each other. Base R data %&gt;% pull(device) %&gt;% table() %&gt;% pie() 3D Pie Chart data %&gt;% pull(device) %&gt;% table() %&gt;% pie3D(explode = 0.1) ggplot2 data %&gt;% pull(device) %&gt;% fct_count() %&gt;% rename(device = f, count = n) %&gt;% ggplot() + geom_bar(aes(x = &quot;&quot;, y = count, fill = device), width = 1, stat = &quot;identity&quot;) + coord_polar(&quot;y&quot;, start = 0) 10.5.8 Donut Chart Donut chart is a variation of the pie chart. It has a round hole in the middle which makes it look like a donut. The focus is on the length of the arcs and not the proportions of the slices. Blank spaces inside donut chart can be used to display information inside it. data %&gt;% pull(device) %&gt;% fct_count() %&gt;% rename(device = f, count = n) %&gt;% ggdonutchart(&quot;count&quot;, label = &quot;device&quot;, fill = &quot;device&quot;, color = &quot;white&quot;, palette = c(&quot;#00AFBB&quot;, &quot;#E7B800&quot;, &quot;#FC4E07&quot;)) 10.5.9 Summary Bar charts provide a visual representation of categorical data. Use grouped bar chart to make comparison against different categories of data. Use stacked bar chart while looking at cumulative data. Use proportional bar chart when you want to concurrently observe each of the several variables as they fluctuate. Use mosaic plot to discover associations between two variables. Use pie chart and donut chart when comparing each groups contribution to the whole. 10.5.10 Your Turn Generate all the below plots: Bar plot of channel Display grouped bar plot of user_type by channel Display stacked bar plot of channel by gender Display proportional bar plot of channel by device Display mosaic plot of device by channel Display pie or donut chart of channel 6.1 Pie Chart 6.2 3D Pie Chart 6.3 Pie Chart (ggplot2) 6.4 Donut Chart 10.6 Useful R Packages Below are a list of functions from different R pacakges for summarizing categorical data. 10.6.1 Summarize Variables Hmisc::describe(data$device) ## data$device ## n missing distinct ## 244398 0 3 ## ## Value Desktop Mobile Tablet ## Frequency 177282 63482 3634 ## Proportion 0.725 0.260 0.015 janitor::tabyl(data$device) ## data$device n percent ## Desktop 177282 0.72538237 ## Mobile 63482 0.25974844 ## Tablet 3634 0.01486919 epiDisplay::tab1(data$device) ## data$device : ## Frequency Percent Cum. percent ## Desktop 177282 72.5 72.5 ## Mobile 63482 26.0 98.5 ## Tablet 3634 1.5 100.0 ## Total 244398 100.0 100.0 summarytools::freq(data$device) ## Registered S3 method overwritten by &#39;pryr&#39;: ## method from ## print.bytes Rcpp ## Frequencies ## data$device ## Type: Factor ## ## Freq % Valid % Valid Cum. % Total % Total Cum. ## ------------- -------- --------- -------------- --------- -------------- ## Desktop 177282 72.54 72.54 72.54 72.54 ## Mobile 63482 25.97 98.51 25.97 98.51 ## Tablet 3634 1.49 100.00 1.49 100.00 ## &lt;NA&gt; 0 0.00 100.00 ## Total 244398 100.00 100.00 100.00 100.00 questionr::freq(data$device) ## This version of bslib is designed to work with shiny version 1.6.0 or higher. ## n % val% ## Desktop 177282 72.5 72.5 ## Mobile 63482 26.0 26.0 ## Tablet 3634 1.5 1.5 descriptr::ds_freq_table(data, device) ## Variable: device ## ----------------------------------------------------------------------- ## Levels Frequency Cum Frequency Percent Cum Percent ## ----------------------------------------------------------------------- ## Desktop 177282 177282 72.54 72.54 ## ----------------------------------------------------------------------- ## Mobile 63482 240764 25.97 98.51 ## ----------------------------------------------------------------------- ## Tablet 3634 244398 1.49 100 ## ----------------------------------------------------------------------- ## Total 244398 - 100.00 - ## ----------------------------------------------------------------------- 10.6.2 Summarize Data Set minilytics &lt;- select(data, device, browser, os) psych::describe(minilytics) ## vars n mean sd median trimmed mad min max range skew kurtosis ## device* 1 244398 1.29 0.49 1 1.22 0.0 1 3 2 1.31 0.58 ## browser* 2 244398 8.22 4.84 6 7.14 0.0 1 26 25 1.80 1.43 ## os* 3 244398 8.80 4.47 8 8.99 8.9 1 16 15 -0.08 -1.36 ## se ## device* 0.00 ## browser* 0.01 ## os* 0.01 skimr::skim(minilytics) (#tab:appendix_2)Data summary Name minilytics Number of rows 244398 Number of columns 3 _______________________ Column type frequency: factor 3 ________________________ Group variables None Variable type: factor skim_variable n_missing complete_rate ordered n_unique top_counts device 0 1 FALSE 3 Des: 177282, Mob: 63482, Tab: 3634 browser 0 1 FALSE 26 Chr: 192176, Saf: 35323, Fir: 6420, Edg: 3787 os 0 1 FALSE 16 Win: 91223, Mac: 66171, And: 39055, iOS: 27866 summarytools::dfSummary(minilytics) ## Data Frame Summary ## minilytics ## Dimensions: 244398 x 3 ## Duplicates: 244317 ## ## ------------------------------------------------------------------------------------------------------- ## No Variable Stats / Values Freqs (% of Valid) Graph Valid Missing ## ---- ---------- ------------------------- -------------------- -------------------- --------- --------- ## 1 device 1. Desktop 177282 (72.5%) IIIIIIIIIIIIII 244398 0 ## [factor] 2. Mobile 63482 (26.0%) IIIII (100%) (0%) ## 3. Tablet 3634 ( 1.5%) ## ## 2 browser 1. Amazon Silk 168 ( 0.1%) 244398 0 ## [factor] 2. Android Browser 101 ( 0.0%) (100%) (0%) ## 3. Android Webview 1215 ( 0.5%) ## 4. APKPure 1 ( 0.0%) ## 5. BlackBerry 7 ( 0.0%) ## 6. Chrome 192176 (78.6%) IIIIIIIIIIIIIII ## 7. Coc Coc 45 ( 0.0%) ## 8. Edge 3787 ( 1.5%) ## 9. Firefox 6420 ( 2.6%) ## 10. Internet Explorer 700 ( 0.3%) ## [ 16 others ] 39778 (16.3%) III ## ## 3 os 1. (not set) 179 ( 0.1%) 244398 0 ## [factor] 2. Android 39055 (16.0%) III (100%) (0%) ## 3. BlackBerry 16 ( 0.0%) ## 4. Chrome OS 14038 ( 5.7%) I ## 5. Firefox OS 4 ( 0.0%) ## 6. iOS 27866 (11.4%) II ## 7. Linux 5781 ( 2.4%) ## 8. Macintosh 66171 (27.1%) IIIII ## 9. OS/2 3 ( 0.0%) ## 10. Playstation 4 15 ( 0.0%) ## [ 6 others ] 91270 (37.3%) IIIIIII ## ------------------------------------------------------------------------------------------------------- tableone::CreateTableOne(vars = c(&quot;device&quot;, &quot;os&quot;), data = minilytics) ## ## Overall ## n 244398 ## device (%) ## Desktop 177282 (72.5) ## Mobile 63482 (26.0) ## Tablet 3634 ( 1.5) ## os (%) ## (not set) 179 ( 0.1) ## Android 39055 (16.0) ## BlackBerry 16 ( 0.0) ## Chrome OS 14038 ( 5.7) ## Firefox OS 4 ( 0.0) ## iOS 27866 (11.4) ## Linux 5781 ( 2.4) ## Macintosh 66171 (27.1) ## OS/2 3 ( 0.0) ## Playstation 4 15 ( 0.0) ## Playstation Vita 1 ( 0.0) ## Samsung 5 ( 0.0) ## Tizen 20 ( 0.0) ## Windows 91223 (37.3) ## Windows Phone 13 ( 0.0) ## Xbox 8 ( 0.0) desctable::desctable(minilytics) ##   N % ## 1 device 244398 NA ## 2 device: Desktop 177282 7.253824e+01 ## 3 device: Mobile 63482 2.597484e+01 ## 4 device: Tablet 3634 1.486919e+00 ## 5 browser 244398 NA ## 6 browser: Amazon Silk 168 6.874033e-02 ## 7 browser: Android Browser 101 4.132603e-02 ## 8 browser: Android Webview 1215 4.971399e-01 ## 9 browser: APKPure 1 4.091687e-04 ## 10 browser: BlackBerry 7 2.864181e-03 ## 11 browser: Chrome 192176 7.863239e+01 ## 12 browser: Coc Coc 45 1.841259e-02 ## 13 browser: Edge 3787 1.549522e+00 ## 14 browser: Firefox 6420 2.626863e+00 ## 15 browser: Internet Explorer 700 2.864181e-01 ## 16 browser: Maxthon 5 2.045843e-03 ## 17 browser: Mozilla Compatible Agent 13 5.319192e-03 ## 18 browser: MRCHROME 1 4.091687e-04 ## 19 browser: Opera 1145 4.684981e-01 ## 20 browser: Opera Mini 52 2.127677e-02 ## 21 browser: Playstation 4 15 6.137530e-03 ## 22 browser: Playstation Vita Browser 1 4.091687e-04 ## 23 browser: Puffin 18 7.365036e-03 ## 24 browser: Safari 35323 1.445306e+01 ## 25 browser: Safari (in-app) 658 2.692330e-01 ## 26 browser: Samsung Internet 2065 8.449333e-01 ## 27 browser: SeaMonkey 3 1.227506e-03 ## 28 browser: Seznam 1 4.091687e-04 ## 29 browser: UC Browser 174 7.119535e-02 ## 30 browser: User-Agent:Mozilla 28 1.145672e-02 ## 31 browser: YaBrowser 276 1.129305e-01 ## 32 os 244398 NA ## 33 os: (not set) 179 7.324119e-02 ## 34 os: Android 39055 1.598008e+01 ## 35 os: BlackBerry 16 6.546698e-03 ## 36 os: Chrome OS 14038 5.743910e+00 ## 37 os: Firefox OS 4 1.636675e-03 ## 38 os: iOS 27866 1.140189e+01 ## 39 os: Linux 5781 2.365404e+00 ## 40 os: Macintosh 66171 2.707510e+01 ## 41 os: OS/2 3 1.227506e-03 ## 42 os: Playstation 4 15 6.137530e-03 ## 43 os: Playstation Vita 1 4.091687e-04 ## 44 os: Samsung 5 2.045843e-03 ## 45 os: Tizen 20 8.183373e-03 ## 46 os: Windows 91223 3.732559e+01 ## 47 os: Windows Phone 13 5.319192e-03 ## 48 os: Xbox 8 3.273349e-03 "]]
