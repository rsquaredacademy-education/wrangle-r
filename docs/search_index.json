[
["index.html", "Data Wrangling with R Preface Structure of the book Software information", " Data Wrangling with R Aravind Hebbali 2020-06-20 Preface This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. Structure of the book Chapters 1 and 2 focus on reading data from flat/delimited files and spreadsheets. Chapters 3, 4 and 5 focus on wrangling data using the dplyr package. Chapter 6 introduces the pipe operator from the magrittr package. Chapter 7 explores tibble(), an alternative for data.frame(). Chapters 8, 9 and 10 explore ways to handle text, date/time and categorical data. Software information The R session information when compiling this book is shown below: sessionInfo() ## R version 4.0.0 (2020-04-24) ## Platform: x86_64-w64-mingw32/x64 (64-bit) ## Running under: Windows 10 x64 (build 18362) ## ## Matrix products: default ## ## locale: ## [1] LC_COLLATE=English_India.1252 LC_CTYPE=English_India.1252 ## [3] LC_MONETARY=English_India.1252 LC_NUMERIC=C ## [5] LC_TIME=English_India.1252 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## loaded via a namespace (and not attached): ## [1] compiler_4.0.0 magrittr_1.5 bookdown_0.18 tools_4.0.0 ## [5] htmltools_0.4.0 rstudioapi_0.11 yaml_2.2.1 Rcpp_1.0.4.6 ## [9] stringi_1.4.6 rmarkdown_2.1 knitr_1.27 stringr_1.4.0 ## [13] xfun_0.13 digest_0.6.25 rlang_0.4.6 evaluate_0.14 We do not add prompts (&gt; and +) to R source code in this book, and we comment out the text output with two hashes ## by default, as you can see from the R session information above. This is for your convenience when you want to copy and run the code (the text output will be ignored since it is commented out). Package names are in bold text (e.g., rmarkdown), and function names are followed by parentheses (e.g., bookdown::render_book()). The double-colon operator :: means accessing an object from a package. "],
["about-the-author.html", "About the Author", " About the Author Aravind Hebbali is the founder of Rsquared Academy. He earned his Masters in Economics from Madras School of Economics. As an active R user, he has authored several R packages such as olsrr rfm descriptr blorr xplorerr In 2015, he founded Rsquared Academy, a free and open source education initiative with focus on data science and analytics. Apart from self paced online courses, Rsquared Academy offers customized learning modules for corporates and universities. You can find him on GitHub. "],
["import1.html", "Chapter 1 Import Data - 1 1.1 Introduction 1.2 Delimiters 1.3 Read Data 1.4 Column Names 1.5 Skip Lines 1.6 Maximum Lines 1.7 Column Types 1.8 Select Columns 1.9 Summary", " Chapter 1 Import Data - 1 1.1 Introduction In this chapter, we will learn to: read data from flat or delimited files handle column names/header skip text/info present before data specify column/variable types read specific columns/variables We will use the following R packages: library(readr) 1.2 Delimiters Before we start reading data from files, let us take a quick look at the different types of delimiters we have to deal with while reading or importing data. In general, it is a good practice to take a quick look at as you will clearly know the delimiter used in the file. 1.2.1 Comma Separated Values 1.2.2 Semi Colon Separated Values 1.2.3 Space Separated Values 1.2.4 Tab Separated Values 1.3 Read Data Let us begin by reading data from a csv file using read_csv(). read_csv(&#39;hsb2.csv&#39;) ## Parsed with column specification: ## cols( ## id = col_double(), ## female = col_double(), ## race = col_double(), ## ses = col_double(), ## schtyp = col_double(), ## prog = col_double(), ## read = col_double(), ## write = col_double(), ## math = col_double(), ## science = col_double(), ## socst = col_double() ## ) ## # A tibble: 200 x 11 ## id female race ses schtyp prog read write math science socst ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 70 0 4 1 1 1 57 52 41 47 57 ## 2 121 1 4 2 1 3 68 59 53 63 61 ## 3 86 0 4 3 1 1 44 33 54 58 31 ## 4 141 0 4 3 1 3 63 44 47 53 56 ## 5 172 0 4 2 1 2 47 52 57 53 61 ## 6 113 0 4 2 1 2 44 52 51 63 61 ## 7 50 0 3 2 1 1 50 59 42 53 61 ## 8 11 0 1 2 1 2 34 46 45 39 36 ## 9 84 0 4 2 1 1 63 57 54 58 51 ## 10 48 0 3 2 1 2 57 55 52 50 51 ## # ... with 190 more rows Great! If you see the above output, you have successfully read data into R. If you see an error message (which most of us see when we are trying to read data for the first time), follow the below instructions: check the separator in the file and ensure it is a comma check the file name check the file path i.e. location of the file ensure that the file name or path is enclosed in single or double quotes When you read data using readr, it will display the data type detected for each column/variable in the data set. If you want to check the data types before reading the data, use spec_csv(). We will learn to specify the column types in the next section. spec_csv(&#39;hsb2.csv&#39;) ## cols( ## id = col_double(), ## female = col_double(), ## race = col_double(), ## ses = col_double(), ## schtyp = col_double(), ## prog = col_double(), ## read = col_double(), ## write = col_double(), ## math = col_double(), ## science = col_double(), ## socst = col_double() ## ) 1.4 Column Names In some cases, files do not include column names or headers. If we do not indicate the absence of column names, readr will treat the first row from the data as the column name. Like we said before, it is a good practice to take a quick look at the data to check for the presence/absence of column names. We will first read the data set without indicating the presence or absence of column names. read_csv(&#39;hsb3.csv&#39;) ## Warning: Duplicated column names deduplicated: &#39;1&#39; =&gt; &#39;1_1&#39; [5], &#39;1&#39; =&gt; ## &#39;1_2&#39; [6], &#39;57&#39; =&gt; &#39;57_1&#39; [11] ## # A tibble: 199 x 11 ## `70` `0` `4` `1` `1_1` `1_2` `57` `52` `41` `47` `57_1` ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 121 1 4 2 1 3 68 59 53 63 61 ## 2 86 0 4 3 1 1 44 33 54 58 31 ## 3 141 0 4 3 1 3 63 44 47 53 56 ## 4 172 0 4 2 1 2 47 52 57 53 61 ## 5 113 0 4 2 1 2 44 52 51 63 61 ## 6 50 0 3 2 1 1 50 59 42 53 61 ## 7 11 0 1 2 1 2 34 46 45 39 36 ## 8 84 0 4 2 1 1 63 57 54 58 51 ## 9 48 0 3 2 1 2 57 55 52 50 51 ## 10 75 0 4 2 1 3 60 46 51 53 61 ## # ... with 189 more rows As you can see, in the absence of column names, readr has converted the first row of the data into the column names. As a result, the data is not read properly and there are lots of missing values and warnings. If the column names are absent (i.e. the column names are provided in a separate file), use the col_names argument and set it to FALSE. Now readr will not convert the first row of data into column name and instead it will generate new column names. read_csv(&#39;hsb3.csv&#39;, col_names = FALSE) ## # A tibble: 200 x 11 ## X1 X2 X3 X4 X5 X6 X7 X8 X9 X10 X11 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 70 0 4 1 1 1 57 52 41 47 57 ## 2 121 1 4 2 1 3 68 59 53 63 61 ## 3 86 0 4 3 1 1 44 33 54 58 31 ## 4 141 0 4 3 1 3 63 44 47 53 56 ## 5 172 0 4 2 1 2 47 52 57 53 61 ## 6 113 0 4 2 1 2 44 52 51 63 61 ## 7 50 0 3 2 1 1 50 59 42 53 61 ## 8 11 0 1 2 1 2 34 46 45 39 36 ## 9 84 0 4 2 1 1 63 57 54 58 51 ## 10 48 0 3 2 1 2 57 55 52 50 51 ## # ... with 190 more rows We may not always want to use the column names generated by readr and instead specify new column names. In such cases, we can use col_names to supply column names as shown in the below example. Let us reread hsb3 and specify column names. cnames &lt;- c(&quot;id&quot;, &quot;gender&quot;, &quot;race&quot;, &quot;socio_economic_status&quot;, &quot;school_type&quot;, &quot;program&quot;, &quot;read&quot;, &quot;write&quot;, &quot;math&quot;, &quot;science&quot;, &quot;socst&quot;) read_csv(&#39;hsb3.csv&#39;, col_names = cnames) ## # A tibble: 200 x 11 ## id gender race socio_economic_~ school_type program read write math ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 70 0 4 1 1 1 57 52 41 ## 2 121 1 4 2 1 3 68 59 53 ## 3 86 0 4 3 1 1 44 33 54 ## 4 141 0 4 3 1 3 63 44 47 ## 5 172 0 4 2 1 2 47 52 57 ## 6 113 0 4 2 1 2 44 52 51 ## 7 50 0 3 2 1 1 50 59 42 ## 8 11 0 1 2 1 2 34 46 45 ## 9 84 0 4 2 1 1 63 57 54 ## 10 48 0 3 2 1 2 57 55 52 ## # ... with 190 more rows, and 2 more variables: science &lt;dbl&gt;, socst &lt;dbl&gt; 1.5 Skip Lines In certain files, you will find information related to the data such as: the data source column names column description copyright etc. The data will appear after/below such text/information. While reading data from such files, we need to skip all the rows where the text is present. If we do not skip them, readr will consider them as part of the data. Let us read the data without skipping any lines/rows and observe the result. read_csv(&#39;hsb4.csv&#39;) ## Warning: 201 parsing failures. ## row col expected actual file ## 3 -- 1 columns 11 columns &#39;hsb4.csv&#39; ## 4 -- 1 columns 11 columns &#39;hsb4.csv&#39; ## 5 -- 1 columns 11 columns &#39;hsb4.csv&#39; ## 6 -- 1 columns 11 columns &#39;hsb4.csv&#39; ## 7 -- 1 columns 11 columns &#39;hsb4.csv&#39; ## ... ... ......... .......... .......... ## See problems(...) for more details. ## # A tibble: 203 x 1 ## `# A dataset containing demographic information and standardized` ## &lt;chr&gt; ## 1 # test scores of high school students. ## 2 # http://www.ats.ucla.edu/stat/spss/whatstat/whatstat.htm ## 3 id ## 4 70 ## 5 121 ## 6 86 ## 7 141 ## 8 172 ## 9 113 ## 10 50 ## # ... with 193 more rows Use skip argument to indicate the number of lines/rows to be skipped while reading data from a file. For example, if the file has contents other than data in the first few lines, we need to skip them before reading the data. In the below example, we will skip the first 3 lines as they contain information about the data set which we do not need. read_csv(&#39;hsb4.csv&#39;, skip = 3) ## # A tibble: 200 x 11 ## id female race ses schtyp prog read write math science socst ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 70 0 4 1 1 1 57 52 41 47 57 ## 2 121 1 4 2 1 3 68 59 53 63 61 ## 3 86 0 4 3 1 1 44 33 54 58 31 ## 4 141 0 4 3 1 3 63 44 47 53 56 ## 5 172 0 4 2 1 2 47 52 57 53 61 ## 6 113 0 4 2 1 2 44 52 51 63 61 ## 7 50 0 3 2 1 1 50 59 42 53 61 ## 8 11 0 1 2 1 2 34 46 45 39 36 ## 9 84 0 4 2 1 1 63 57 54 58 51 ## 10 48 0 3 2 1 2 57 55 52 50 51 ## # ... with 190 more rows 1.6 Maximum Lines Suppose the data file contains several thousands of rows of data and we do not want to read all of it. What can we do in such cases? readr allows us to specify the maximum number of rows to be read using the n_max argument. Suppose we want to read only 100 rows of data from a file, we can set n_max equal to 100. In the next example, we will read the first 120 rows from the hsb2 file. If you observe the last row in the output, it says # ... with 110 more rows, indicating that only 120 rows of data has been read from the file. read_csv(&#39;hsb2.csv&#39;, n_max = 120) ## # A tibble: 120 x 11 ## id female race ses schtyp prog read write math science socst ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 70 0 4 1 1 1 57 52 41 47 57 ## 2 121 1 4 2 1 3 68 59 53 63 61 ## 3 86 0 4 3 1 1 44 33 54 58 31 ## 4 141 0 4 3 1 3 63 44 47 53 56 ## 5 172 0 4 2 1 2 47 52 57 53 61 ## 6 113 0 4 2 1 2 44 52 51 63 61 ## 7 50 0 3 2 1 1 50 59 42 53 61 ## 8 11 0 1 2 1 2 34 46 45 39 36 ## 9 84 0 4 2 1 1 63 57 54 58 51 ## 10 48 0 3 2 1 2 57 55 52 50 51 ## # ... with 110 more rows 1.7 Column Types If you have observed carefully, when you read data using readr, it displays the column names and column types followed by the first 10 rows of data. readr determines the data type for each column based on the first 1000 rows of data. The data can be of the following types: integer double (decimal point) logical (TRUE/FALSE) character (text/string) factor (categorical/qualitative) date/time Before you read data from a file, use spec_csv() to see the data types as determined by readr. If it determines the data types correctly, you can go ahead and read the data else we will have to specify the data types and we will have to do that for all the columns we want to read and not just for those columns whose data type was wrongly determined by readr. To specify the data types, we will use the col_types argument and supply it a list of data types. The data types can be specified using: col_integer() col_double() col_factor() col_logical() col_character() col_date() col_time() col_datetime() While specifying the data types we also need to specify the categories of the categorical/qualitative variable. To do that, we use the levels argument within col_factor(). Let us read data from the hsb2.csv file to understand data type specification. read_csv(&#39;hsb2.csv&#39;, col_types = list( col_integer(), col_factor(levels = c(0, 1)), col_factor(levels = c(1, 2, 3, 4)), col_factor(levels = c(1, 2, 3)), col_factor(levels = c(1, 2)), col_factor(levels = c(1, 2, 3)), col_integer(), col_integer(), col_integer(), col_integer(), col_integer()) ) ## # A tibble: 200 x 11 ## id female race ses schtyp prog read write math science socst ## &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 70 0 4 1 1 1 57 52 41 47 57 ## 2 121 1 4 2 1 3 68 59 53 63 61 ## 3 86 0 4 3 1 1 44 33 54 58 31 ## 4 141 0 4 3 1 3 63 44 47 53 56 ## 5 172 0 4 2 1 2 47 52 57 53 61 ## 6 113 0 4 2 1 2 44 52 51 63 61 ## 7 50 0 3 2 1 1 50 59 42 53 61 ## 8 11 0 1 2 1 2 34 46 45 39 36 ## 9 84 0 4 2 1 1 63 57 54 58 51 ## 10 48 0 3 2 1 2 57 55 52 50 51 ## # ... with 190 more rows If we do not specify the data type for all columns, readr will return an error which leads to the following questions: What if I want to skip a few columns? What if I want to read certain columns only? 1.8 Select Columns For the first scenario, we can use col_skip() i.e. instead of specifying the data type, we indicate to readr to skip that particular column while reading the data. In case of the second scenario, we will use cols_only() to specify the columns to be read i.e. instead of using list() to supply the data types, we will use cols_only() and provide the following details: column name column type using col_types argument read_csv(&#39;hsb2.csv&#39;, col_types = cols_only(id = col_integer(), prog = col_factor(levels = c(1, 2, 3)), read = col_integer()) ) ## # A tibble: 200 x 3 ## id prog read ## &lt;int&gt; &lt;fct&gt; &lt;int&gt; ## 1 70 1 57 ## 2 121 3 68 ## 3 86 1 44 ## 4 141 3 63 ## 5 172 2 47 ## 6 113 2 44 ## 7 50 1 50 ## 8 11 2 34 ## 9 84 1 63 ## 10 48 2 57 ## # ... with 190 more rows If you have a data set with 10 columns and plan to skip only a couple of columns, use col_skip() instead if you plan to read only a couple of columns, use cols_only(). 1.9 Summary "],
["import2.html", "Chapter 2 Import Data - 2 2.1 Introduction 2.2 List Sheets 2.3 Read Sheet 2.4 Read Specific Cells 2.5 Read Specific Rows 2.6 Read Single Column 2.7 Read Multiple Columns 2.8 Statistical Softwares 2.9 Summary", " Chapter 2 Import Data - 2 2.1 Introduction In this chapter, we will: list sheets in an excel file read data from an excel sheet read specific cells from an excel sheet read specific rows read specific columns read data from - SAS - SPSS - STATA We will use the following R packages: library(readxl) library(haven) 2.2 List Sheets An excel file may contain several sheets. Let us see how many sheets are present in sample.xls file and their respective names using excel_sheets(). excel_sheets(&#39;sample.xls&#39;) ## [1] &quot;ecom&quot; 2.3 Read Sheet Now that we know the number of sheets and their names, let us read data from the ecom sheet of the sample.xls file using read_excel(). We will specify the file name, and the sheet name or sheet number. 2.3.1 Case 1: Specify the sheet number read_excel(&#39;sample.xls&#39;, sheet = 1) ## # A tibble: 7 x 5 ## channel users new_users sessions bounce_rate ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Organic Search 43296 40238 50810 48.72% ## 2 Direct 12916 12311 16419 49.27% ## 3 Referral 10983 7636 18105 22.26% ## 4 Social 10346 10029 11101 61.92% ## 5 Display 5564 4790 7220 83.30% ## 6 Paid Search 2687 2205 3438 38.02% ## 7 Affiliates 1773 1585 2167 55.75% 2.3.2 Case 2: Specify the sheet name read_excel(&#39;sample.xls&#39;, sheet = &#39;ecom&#39;) ## # A tibble: 7 x 5 ## channel users new_users sessions bounce_rate ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Organic Search 43296 40238 50810 48.72% ## 2 Direct 12916 12311 16419 49.27% ## 3 Referral 10983 7636 18105 22.26% ## 4 Social 10346 10029 11101 61.92% ## 5 Display 5564 4790 7220 83.30% ## 6 Paid Search 2687 2205 3438 38.02% ## 7 Affiliates 1773 1585 2167 55.75% Notice when you use the sheet name, the name should be enclosed in single/double quotes. 2.4 Read Specific Cells You may not always want to read all the columns or rows from the excel sheet. In such cases, you can specify the cells from which the data must be read which can be achieved using the range argument. So how do we specify the cells from which to read data? There are different ways of specifying the cell range and we will look at them one by one: 2.4.1 Method 1 The first method uses the cell names along with : to specify the cell range. For example, to read data from first 4 rows of columns B and C, we will specify the range as \"B1:C4\". read_excel(&#39;sample.xls&#39;, sheet = 1, range = &quot;B1:C4&quot;) ## # A tibble: 3 x 2 ## users new_users ## &lt;dbl&gt; &lt;dbl&gt; ## 1 43296 40238 ## 2 12916 12311 ## 3 10983 7636 To read data from first 5 rows of columns A, B and C, we will specify the range as \"A1:C5\". read_excel(&#39;sample.xls&#39;, sheet = 1, range = &quot;A1:C5&quot;) ## # A tibble: 4 x 3 ## channel users new_users ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Organic Search 43296 40238 ## 2 Direct 12916 12311 ## 3 Referral 10983 7636 ## 4 Social 10346 10029 2.4.2 Method 2 In the second method, we start from a particular cell and specify the number of rows and columns to be covered keeping the initial cell as anchorage. In the below example, we want to read 3 rows and 2 columns starting from the cell A4. read_excel(&#39;sample.xls&#39;, sheet = 1, col_names = FALSE, range = anchored(&quot;A4&quot;, dim = c(3, 2))) ## # A tibble: 3 x 2 ## ...1 ...2 ## &lt;chr&gt; &lt;dbl&gt; ## 1 Referral 10983 ## 2 Social 10346 ## 3 Display 5564 2.4.3 Method 3 In this method, we use the cell_limit() and specify the location of two ends of a rectangle covering the cells we want to read. For example, to read data from the first 6 rows and 4 columns, we will specify the range as following: start from the first row of the first column cover all cells upto the 6th row of the 4th column read_excel(&#39;sample.xls&#39;, sheet = 1, range = cell_limits(c(1, 1), c(6, 4))) ## # A tibble: 5 x 4 ## channel users new_users sessions ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Organic Search 43296 40238 50810 ## 2 Direct 12916 12311 16419 ## 3 Referral 10983 7636 18105 ## 4 Social 10346 10029 11101 ## 5 Display 5564 4790 7220 You can use NA to indicate the first and last row/column. For example, to read data from all the rows from the second column onwards: read_excel(&#39;sample.xls&#39;, sheet = 1, range = cell_limits(c(1, 2), c(NA, NA))) ## # A tibble: 7 x 4 ## users new_users sessions bounce_rate ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 43296 40238 50810 48.72% ## 2 12916 12311 16419 49.27% ## 3 10983 7636 18105 22.26% ## 4 10346 10029 11101 61.92% ## 5 5564 4790 7220 83.30% ## 6 2687 2205 3438 38.02% ## 7 1773 1585 2167 55.75% Let us quickly look at how we will specify range of cells using the above 3 methods when we want to read data from the first 4 rows of columns B and C: 2.4.4 Method 1 read_excel(&#39;sample.xls&#39;, sheet = 1, range = &quot;B1:C4&quot;) ## # A tibble: 3 x 2 ## users new_users ## &lt;dbl&gt; &lt;dbl&gt; ## 1 43296 40238 ## 2 12916 12311 ## 3 10983 7636 2.4.5 Method 2 read_excel(&#39;sample.xls&#39;, sheet = 1, range = anchored(&quot;B1&quot;, dim = c(4, 2))) ## # A tibble: 3 x 2 ## users new_users ## &lt;dbl&gt; &lt;dbl&gt; ## 1 43296 40238 ## 2 12916 12311 ## 3 10983 7636 2.4.6 Method 3 read_excel(&#39;sample.xls&#39;, sheet = 1, range = cell_limits(c(1, 2), c(4, 3))) ## # A tibble: 3 x 2 ## users new_users ## &lt;dbl&gt; &lt;dbl&gt; ## 1 43296 40238 ## 2 12916 12311 ## 3 10983 7636 2.5 Read Specific Rows When you want to read a subset of rows from the data, use cell_rows() and specify the row numbers or the range. In the below example, we want to read the first 4 rows of data from the file. read_excel(&#39;sample.xls&#39;, sheet = 1, range = cell_rows(1:4)) ## # A tibble: 3 x 5 ## channel users new_users sessions bounce_rate ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Organic Search 43296 40238 50810 48.72% ## 2 Direct 12916 12311 16419 49.27% ## 3 Referral 10983 7636 18105 22.26% 2.6 Read Single Column If you want to read a single column from the data, use cell_cols() and specify the column number. In the below example, we read the second column from the sample.xls file. read_excel(&#39;sample.xls&#39;, sheet = 1, range = cell_cols(2)) ## # A tibble: 7 x 1 ## users ## &lt;dbl&gt; ## 1 43296 ## 2 12916 ## 3 10983 ## 4 10346 ## 5 5564 ## 6 2687 ## 7 1773 2.7 Read Multiple Columns In case of multiple columns, we need to specify the column numbers or the column range. In the below example, we want to read the 2nd, 4th and 6th column from the sample.xls file. read_excel(&#39;sample.xls&#39;, sheet = 1, range = cell_cols(c(2, 4, 6))) ## # A tibble: 7 x 5 ## users new_users sessions bounce_rate ...5 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;lgl&gt; ## 1 43296 40238 50810 48.72% NA ## 2 12916 12311 16419 49.27% NA ## 3 10983 7636 18105 22.26% NA ## 4 10346 10029 11101 61.92% NA ## 5 5564 4790 7220 83.30% NA ## 6 2687 2205 3438 38.02% NA ## 7 1773 1585 2167 55.75% NA In the next example, we want to read data from the 2nd column upto and including the 6th column. read_excel(&#39;sample.xls&#39;, sheet = 1, range = cell_cols(c(2:6))) ## # A tibble: 7 x 5 ## users new_users sessions bounce_rate ...5 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;lgl&gt; ## 1 43296 40238 50810 48.72% NA ## 2 12916 12311 16419 49.27% NA ## 3 10983 7636 18105 22.26% NA ## 4 10346 10029 11101 61.92% NA ## 5 5564 4790 7220 83.30% NA ## 6 2687 2205 3438 38.02% NA ## 7 1773 1585 2167 55.75% NA 2.7.1 Summary 2.8 Statistical Softwares We will use the haven package to read data from files of other statistical softwares such as: SAS SPSS STATA 2.8.1 STATA read_stata(&#39;airline.dta&#39;) ## # A tibble: 32 x 6 ## year y w r l k ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1948 1.21 0.243 0.145 1.41 0.612 ## 2 1949 1.35 0.260 0.218 1.38 0.559 ## 3 1950 1.57 0.278 0.316 1.39 0.573 ## 4 1951 1.95 0.297 0.394 1.55 0.564 ## 5 1952 2.27 0.310 0.356 1.80 0.574 ## 6 1953 2.73 0.322 0.359 1.93 0.711 ## 7 1954 3.03 0.335 0.403 1.96 0.776 ## 8 1955 3.56 0.350 0.396 2.12 0.827 ## 9 1956 3.98 0.361 0.382 2.43 0.800 ## 10 1957 4.42 0.379 0.305 2.71 0.921 ## # ... with 22 more rows 2.8.2 SPSS read_spss(&#39;employee.sav&#39;) ## # A tibble: 474 x 9 ## id gender educ jobcat salary salbegin jobtime prevexp minority ## &lt;dbl&gt; &lt;chr+lb&gt; &lt;dbl+lb&gt; &lt;dbl+lbl&gt; &lt;dbl+l&gt; &lt;dbl+lb&gt; &lt;dbl+l&gt; &lt;dbl+lbl&gt; &lt;dbl+lb&gt; ## 1 1 m [Male] 15 [15] 3 [Manag~ 57000 27000 98 144 0 [No] ## 2 2 m [Male] 16 [16] 1 [Cleri~ 40200 18750 98 36 0 [No] ## 3 3 f [Fema~ 12 [12] 1 [Cleri~ 21450 12000 98 381 0 [No] ## 4 4 f [Fema~ 8 [8] 1 [Cleri~ 21900 13200 98 190 0 [No] ## 5 5 m [Male] 15 [15] 1 [Cleri~ 45000 21000 98 138 0 [No] ## 6 6 m [Male] 15 [15] 1 [Cleri~ 32100 13500 98 67 0 [No] ## 7 7 m [Male] 15 [15] 1 [Cleri~ 36000 18750 98 114 0 [No] ## 8 8 f [Fema~ 12 [12] 1 [Cleri~ 21900 9750 98 0 [mis~ 0 [No] ## 9 9 f [Fema~ 15 [15] 1 [Cleri~ 27900 12750 98 115 0 [No] ## 10 10 f [Fema~ 12 [12] 1 [Cleri~ 24000 13500 98 244 0 [No] ## # ... with 464 more rows 2.8.3 SAS read_sas(&#39;airline.sas7bdat&#39;) ## # A tibble: 32 x 6 ## YEAR Y W R L K ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1948 1.21 0.243 0.145 1.41 0.612 ## 2 1949 1.35 0.260 0.218 1.38 0.559 ## 3 1950 1.57 0.278 0.316 1.39 0.573 ## 4 1951 1.95 0.297 0.394 1.55 0.564 ## 5 1952 2.27 0.310 0.356 1.80 0.574 ## 6 1953 2.73 0.322 0.359 1.93 0.711 ## 7 1954 3.03 0.335 0.403 1.96 0.776 ## 8 1955 3.56 0.350 0.396 2.12 0.827 ## 9 1956 3.98 0.361 0.382 2.43 0.800 ## 10 1957 4.42 0.379 0.305 2.71 0.921 ## # ... with 22 more rows 2.9 Summary "],
["dplyr1.html", "Chapter 3 Data Wrangling - 1 3.1 Introduction 3.2 dplyr Verbs 3.3 Data 3.4 Case Study 3.5 Average Order Value 3.6 AOV by Devices 3.7 Syntax 3.8 Filter Rows 3.9 Select Columns 3.10 Grouping Data 3.11 Summarise Data 3.12 Create Columns 3.13 Arrange Data 3.14 AOV by Devices 3.15 Your Turn", " Chapter 3 Data Wrangling - 1 3.1 Introduction According to a survey by CrowdFlower, data scientists spend most of their time cleaning and manipulating data rather than mining or modeling them for insights. As such, it becomes important to have tools that make data manipulation faster and easier. In today’s chapter, we introduce you to dplyr, a grammar of data manipulation. We will use the following R packages: library(dplyr) library(readr) 3.2 dplyr Verbs dplyr provides a set of verbs that help us solve the most common data manipulation challenges while working with tabular data (dataframes, tibbles): select filter arrange mutate summarise 3.3 Data ecom &lt;- read_csv(&#39;https://raw.githubusercontent.com/rsquaredacademy/datasets/master/web.csv&#39;, col_types = cols_only(device = col_factor(levels = c(&quot;laptop&quot;, &quot;tablet&quot;, &quot;mobile&quot;)), referrer = col_factor(levels = c(&quot;bing&quot;, &quot;direct&quot;, &quot;social&quot;, &quot;yahoo&quot;, &quot;google&quot;)), purchase = col_logical(), n_pages = col_double(), n_visit = col_double(), duration = col_double(), order_value = col_double(), order_items = col_double() ) ) ecom ## # A tibble: 1,000 x 8 ## referrer device n_visit n_pages duration purchase order_items order_value ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 google laptop 10 1 693 FALSE 0 0 ## 2 yahoo tablet 9 1 459 FALSE 0 0 ## 3 direct laptop 0 1 996 FALSE 0 0 ## 4 bing tablet 3 18 468 TRUE 6 434 ## 5 yahoo mobile 9 1 955 FALSE 0 0 ## 6 yahoo laptop 5 5 135 FALSE 0 0 ## 7 yahoo mobile 10 1 75 FALSE 0 0 ## 8 direct mobile 10 1 908 FALSE 0 0 ## 9 bing mobile 3 19 209 FALSE 0 0 ## 10 google mobile 6 1 208 FALSE 0 0 ## # ... with 990 more rows 3.3.1 Data Dictionary Below is the description of the data set: referrer: referrer website/search engine device: device used to visit the website n_pages: number of pages visited duration: time spent on the website (in seconds) purchase: whether visitor purchased order_value: order value of visitor (in dollars) n_visit: number of visits 3.4 Case Study We will use dplyr to answer the following: what is the average order value by device types? what is the average number of pages visited by purchasers and non-purchasers? what is the average time on site for purchasers vs non-purchasers? what is the average number of pages visited by purchasers and non-purchasers using mobile? 3.5 Average Order Value 3.6 AOV by Devices ecom %&gt;% filter(purchase) %&gt;% select(device, order_value) %&gt;% group_by(device) %&gt;% summarise_all(funs(revenue = sum, orders = n())) %&gt;% mutate( aov = revenue / orders ) %&gt;% select(device, aov) %&gt;% arrange(aov) ## Warning: `funs()` is deprecated as of dplyr 0.8.0. ## Please use a list of either functions or lambdas: ## ## # Simple named list: ## list(mean = mean, median = median) ## ## # Auto named with `tibble::lst()`: ## tibble::lst(mean, median) ## ## # Using lambdas ## list(~ mean(., trim = .2), ~ median(., na.rm = TRUE)) ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_warnings()` to see where this warning was generated. ## # A tibble: 3 x 2 ## device aov ## &lt;fct&gt; &lt;dbl&gt; ## 1 tablet 1426. ## 2 mobile 1431. ## 3 laptop 1824. 3.7 Syntax Before we start exploring the dplyr verbs, let us look at their syntax: the first argument is always a data.frame or tibble the subsequent arguments provide the information required for the verbs to take action the name of columns in the data need not be surrounded by quotes 3.8 Filter Rows In order to compute the AOV, we must first separate the purchasers from non-purchasers. We will do this by filtering the data related to purchasers using the filter() function. It allows us to filter rows that meet a specific criteria/condition. The first argument is the name of the data frame and the rest of the arguments are expressions for filtering the data. Let us look at a few examples: The first example we will look at filters all visits from device mobile. As we had learnt in the previous section, the first argument is our data set ecom and the next argument is the condition for filtering rows. filter(ecom, device == &quot;mobile&quot;) ## # A tibble: 344 x 8 ## referrer device n_visit n_pages duration purchase order_items order_value ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 yahoo mobile 9 1 955 FALSE 0 0 ## 2 yahoo mobile 10 1 75 FALSE 0 0 ## 3 direct mobile 10 1 908 FALSE 0 0 ## 4 bing mobile 3 19 209 FALSE 0 0 ## 5 google mobile 6 1 208 FALSE 0 0 ## 6 direct mobile 9 14 406 TRUE 3 651 ## 7 yahoo mobile 7 1 19 FALSE 7 2423 ## 8 google mobile 5 1 147 FALSE 0 0 ## 9 bing mobile 0 7 196 FALSE 4 237 ## 10 google mobile 10 1 338 FALSE 0 0 ## # ... with 334 more rows We can specify multiple filtering conditions as well. In the below example, we specify two filter conditions: visit from device mobile resulted in a purchase or conversion filter(ecom, device == &quot;mobile&quot;, purchase) ## # A tibble: 36 x 8 ## referrer device n_visit n_pages duration purchase order_items order_value ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 direct mobile 9 14 406 TRUE 3 651 ## 2 bing mobile 4 20 440 TRUE 3 184 ## 3 bing mobile 3 18 288 TRUE 6 764 ## 4 social mobile 10 11 242 TRUE 4 287 ## 5 yahoo mobile 6 14 322 TRUE 3 1443 ## 6 google mobile 1 18 252 TRUE 3 2449 ## 7 social mobile 7 16 352 TRUE 10 2824 ## 8 direct mobile 4 18 324 TRUE 3 1670 ## 9 social mobile 1 20 520 TRUE 5 1021 ## 10 yahoo mobile 0 13 351 TRUE 10 288 ## # ... with 26 more rows Here is another example where we specify multiple conditions: visit from device tablet made a purchase browsed less than 15 pages filter(ecom, device == &quot;tablet&quot;, purchase, n_pages &lt; 15) ## # A tibble: 12 x 8 ## referrer device n_visit n_pages duration purchase order_items order_value ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 social tablet 7 10 290 TRUE 9 1304 ## 2 yahoo tablet 2 14 364 TRUE 6 1667 ## 3 google tablet 7 12 324 TRUE 2 1358 ## 4 direct tablet 3 12 324 TRUE 10 1257 ## 5 yahoo tablet 0 13 390 TRUE 5 1748 ## 6 social tablet 2 12 300 TRUE 2 2754 ## 7 direct tablet 6 13 338 TRUE 5 683 ## 8 yahoo tablet 2 10 280 TRUE 4 293 ## 9 social tablet 10 10 290 TRUE 9 37 ## 10 direct tablet 3 10 260 TRUE 7 980 ## 11 google tablet 9 14 308 TRUE 7 2436 ## 12 social tablet 10 11 330 TRUE 1 2171 3.8.1 Case Study Let us apply what we have learnt to the case study. We want to filter all visits that resulted in a purchase. filter(ecom, purchase) ## # A tibble: 103 x 8 ## referrer device n_visit n_pages duration purchase order_items order_value ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 bing tablet 3 18 468 TRUE 6 434 ## 2 direct mobile 9 14 406 TRUE 3 651 ## 3 bing tablet 5 16 368 TRUE 6 1049 ## 4 social tablet 7 10 290 TRUE 9 1304 ## 5 direct tablet 2 19 342 TRUE 5 622 ## 6 social tablet 9 20 420 TRUE 7 1613 ## 7 bing mobile 4 20 440 TRUE 3 184 ## 8 yahoo tablet 2 16 480 TRUE 9 286 ## 9 bing mobile 3 18 288 TRUE 6 764 ## 10 yahoo tablet 2 14 364 TRUE 6 1667 ## # ... with 93 more rows 3.9 Select Columns After filtering the data, we need to select relevent variables to compute the AOV. Remember, we do not need all the columns in the data to compute a required metric (in our case, AOV). The select() function allows us to select a subset of columns. The first argument is the name of the data frame and the subsequent arguments specify the columns by name or position. To select the device and duration columns, we specify the data set i.e. ecom followed by the name of the columns. select(ecom, device, duration) ## # A tibble: 1,000 x 2 ## device duration ## &lt;fct&gt; &lt;dbl&gt; ## 1 laptop 693 ## 2 tablet 459 ## 3 laptop 996 ## 4 tablet 468 ## 5 mobile 955 ## 6 laptop 135 ## 7 mobile 75 ## 8 mobile 908 ## 9 mobile 209 ## 10 mobile 208 ## # ... with 990 more rows We can select a set of columns using :. In the below example, we select all the columns starting from referrer up to order_items. Remember that we can use : only when the columns are adjacent to each other in the data set. select(ecom, referrer:order_items) ## # A tibble: 1,000 x 7 ## referrer device n_visit n_pages duration purchase order_items ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;dbl&gt; ## 1 google laptop 10 1 693 FALSE 0 ## 2 yahoo tablet 9 1 459 FALSE 0 ## 3 direct laptop 0 1 996 FALSE 0 ## 4 bing tablet 3 18 468 TRUE 6 ## 5 yahoo mobile 9 1 955 FALSE 0 ## 6 yahoo laptop 5 5 135 FALSE 0 ## 7 yahoo mobile 10 1 75 FALSE 0 ## 8 direct mobile 10 1 908 FALSE 0 ## 9 bing mobile 3 19 209 FALSE 0 ## 10 google mobile 6 1 208 FALSE 0 ## # ... with 990 more rows What if you want to select all columns except a few? Typing the name of many columns can be cumbersome and may also result in spelling errors. We may use : only if the columns are adjacent to each other but that may not always be the case. dplyr allows us to specify columns that need not be selected using -. In the below example, we select all columns except n_pages and duration. Notice the - before both of them. select(ecom, -n_pages, -duration) ## # A tibble: 1,000 x 6 ## referrer device n_visit purchase order_items order_value ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 google laptop 10 FALSE 0 0 ## 2 yahoo tablet 9 FALSE 0 0 ## 3 direct laptop 0 FALSE 0 0 ## 4 bing tablet 3 TRUE 6 434 ## 5 yahoo mobile 9 FALSE 0 0 ## 6 yahoo laptop 5 FALSE 0 0 ## 7 yahoo mobile 10 FALSE 0 0 ## 8 direct mobile 10 FALSE 0 0 ## 9 bing mobile 3 FALSE 0 0 ## 10 google mobile 6 FALSE 0 0 ## # ... with 990 more rows 3.9.1 Case Study For our case study, we need to select the column order_value to calculate the AOV. We also need to select the device column as we are computing the AOV for each device type. select(ecom, device, order_value) ## # A tibble: 1,000 x 2 ## device order_value ## &lt;fct&gt; &lt;dbl&gt; ## 1 laptop 0 ## 2 tablet 0 ## 3 laptop 0 ## 4 tablet 434 ## 5 mobile 0 ## 6 laptop 0 ## 7 mobile 0 ## 8 mobile 0 ## 9 mobile 0 ## 10 mobile 0 ## # ... with 990 more rows But we want the above data only for purchasers. Let us combine filter() and select() functions to extract order_value and order_items only for those visis that resulted in a purchase. # filter all visits that resulted in a purchase ecom1 &lt;- filter(ecom, purchase) # select the relevant columns ecom2 &lt;- select(ecom1, device, order_value) # view data ecom2 ## # A tibble: 103 x 2 ## device order_value ## &lt;fct&gt; &lt;dbl&gt; ## 1 tablet 434 ## 2 mobile 651 ## 3 tablet 1049 ## 4 tablet 1304 ## 5 tablet 622 ## 6 tablet 1613 ## 7 mobile 184 ## 8 tablet 286 ## 9 mobile 764 ## 10 tablet 1667 ## # ... with 93 more rows 3.10 Grouping Data We need to compute the total order value and total order items for each device in order to compute their AOV. To achieve this, we need to group the selected order_value and order_items by device type. group_by() allows us to group or split data based on particular (discrete) variable. The first argument is the name of the data set and the second argument is the name of the column based on which the data will be split. To split the data by referrer type, we use group_by and specify the data set i.e. ecom and the column based on which to split the data i.e. referrer. group_by(ecom, referrer) ## # A tibble: 1,000 x 8 ## # Groups: referrer [5] ## referrer device n_visit n_pages duration purchase order_items order_value ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 google laptop 10 1 693 FALSE 0 0 ## 2 yahoo tablet 9 1 459 FALSE 0 0 ## 3 direct laptop 0 1 996 FALSE 0 0 ## 4 bing tablet 3 18 468 TRUE 6 434 ## 5 yahoo mobile 9 1 955 FALSE 0 0 ## 6 yahoo laptop 5 5 135 FALSE 0 0 ## 7 yahoo mobile 10 1 75 FALSE 0 0 ## 8 direct mobile 10 1 908 FALSE 0 0 ## 9 bing mobile 3 19 209 FALSE 0 0 ## 10 google mobile 6 1 208 FALSE 0 0 ## # ... with 990 more rows 3.10.1 Case Study In the second line in the previous output, you can observe Groups: referrer [5] . The data is split into 5 groups as the referrer variable has 5 distinct values. For our case study, we need to group the data by device type. # split ecom2 by device type ecom3 &lt;- group_by(ecom2, device) ecom3 ## # A tibble: 103 x 2 ## # Groups: device [3] ## device order_value ## &lt;fct&gt; &lt;dbl&gt; ## 1 tablet 434 ## 2 mobile 651 ## 3 tablet 1049 ## 4 tablet 1304 ## 5 tablet 622 ## 6 tablet 1613 ## 7 mobile 184 ## 8 tablet 286 ## 9 mobile 764 ## 10 tablet 1667 ## # ... with 93 more rows 3.11 Summarise Data The next step is to compute the total order value and total order items for each device. i.e. we need to reduce the order value and order items data to a single summary. We can achieve this using summarise(). As usual, the first argument is the name of a data set and the subsequent arguments are functions that can summarise data. For example, we can use min, max, sum, mean etc. Let us compute the average number of pages browsed by referrer type: split data by referrer type compute the average number of pages using mean # split data by referrer type step_1 &lt;- group_by(ecom, referrer) # compute average number of pages step_2 &lt;- summarise(step_1, mean(n_pages)) ## `summarise()` ungrouping (override with `.groups` argument) step_2 ## # A tibble: 5 x 2 ## referrer `mean(n_pages)` ## * &lt;fct&gt; &lt;dbl&gt; ## 1 bing 6.13 ## 2 direct 6.38 ## 3 social 5.42 ## 4 yahoo 5.99 ## 5 google 5.73 Now let us compute both the mean and the median. # split data by referrer type step_1 &lt;- group_by(ecom, referrer) # compute average number of pages step_2 &lt;- summarise(step_1, mean(n_pages), median(n_pages)) ## `summarise()` ungrouping (override with `.groups` argument) step_2 ## # A tibble: 5 x 3 ## referrer `mean(n_pages)` `median(n_pages)` ## * &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 bing 6.13 1 ## 2 direct 6.38 1 ## 3 social 5.42 1 ## 4 yahoo 5.99 2 ## 5 google 5.73 1 Another way to achieve the above result is to use the summarise_all() function. How does that work? It generates the specified summary for all the columns in the data set except for the column based on which the data has been grouped or split. So we need to ensure that the data does not have any irrelevant columns. split data by referrer type select order_value and order_items compute the average number of pages by applying the mean function to all the columns # select relevant columns step_1 &lt;- select(ecom, referrer, order_value) # split data by referrer type step_2 &lt;- group_by(step_1, referrer) # compute average number of pages step_3 &lt;- summarise_all(step_2, funs(mean)) step_3 ## # A tibble: 5 x 2 ## referrer order_value ## * &lt;fct&gt; &lt;dbl&gt; ## 1 bing 316. ## 2 direct 441. ## 3 social 380. ## 4 yahoo 470. ## 5 google 328. Let us compute mean and median number of pages for each referre type using summarise_all. # select relevant columns step_1 &lt;- select(ecom, referrer, order_value) # split data by referrer type step_2 &lt;- group_by(step_1, referrer) # compute mean and median number of pages step_3 &lt;- summarise_all(step_2, funs(mean, median)) step_3 ## # A tibble: 5 x 3 ## referrer mean median ## * &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 bing 316. 0 ## 2 direct 441. 0 ## 3 social 380. 0 ## 4 yahoo 470. 0 ## 5 google 328. 0 3.11.1 Case Study So far, we have split the data based on the device type and we have selected 2 columns, order_value and order_items. We need the sum of order value and order items. What function can we use to obtain them? The sum() function will generate the sum of the values and hence we will use it inside the summarise() function. Remember, we need to provide a name to the summary being generated. ecom4 &lt;- summarise(ecom3, revenue = sum(order_value), orders = n()) ## `summarise()` ungrouping (override with `.groups` argument) ecom4 ## # A tibble: 3 x 3 ## device revenue orders ## * &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; ## 1 laptop 56531 31 ## 2 tablet 51321 36 ## 3 mobile 51504 36 There you go, we have the total order value and total order items for each device type. If we use summarise_all(), it will generate the summary for the selected columns based on the function specified. To specify the functions, we need to use another argument funs and it can take any number of valid functions. ecom4 &lt;- summarise_all(ecom3, funs(revenue = sum, orders = n())) ecom4 ## # A tibble: 3 x 3 ## device revenue orders ## * &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; ## 1 laptop 56531 31 ## 2 tablet 51321 36 ## 3 mobile 51504 36 3.12 Create Columns To create a new column, we will use mutate(). The first argument is the name of the data set and the subsequent arguments are expressions for creating new columns based out of existing columns. Let us add a new column avg_page_time i.e. time on site divided by number of pages visited. # select duration and n_pages from ecom mutate_1 &lt;- select(ecom, n_pages, duration) mutate(mutate_1, avg_page_time = duration / n_pages) ## # A tibble: 1,000 x 3 ## n_pages duration avg_page_time ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 693 693 ## 2 1 459 459 ## 3 1 996 996 ## 4 18 468 26 ## 5 1 955 955 ## 6 5 135 27 ## 7 1 75 75 ## 8 1 908 908 ## 9 19 209 11 ## 10 1 208 208 ## # ... with 990 more rows We can create new columns based on other columns created using mutate. Let us create another column sqrt_avg_page_time i.e. square root of the average time on page using avg_page_time. mutate(mutate_1, avg_page_time = duration / n_pages, sqrt_avg_page_time = sqrt(avg_page_time)) ## # A tibble: 1,000 x 4 ## n_pages duration avg_page_time sqrt_avg_page_time ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 693 693 26.3 ## 2 1 459 459 21.4 ## 3 1 996 996 31.6 ## 4 18 468 26 5.10 ## 5 1 955 955 30.9 ## 6 5 135 27 5.20 ## 7 1 75 75 8.66 ## 8 1 908 908 30.1 ## 9 19 209 11 3.32 ## 10 1 208 208 14.4 ## # ... with 990 more rows 3.12.1 Case Study Back to our case study, from the last step we have the total order value and total order items for each device category and can compute the AOV. We will create a new column to store AOV. ecom5 &lt;- mutate(ecom4, aov = revenue / orders) ecom5 ## # A tibble: 3 x 4 ## device revenue orders aov ## * &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 laptop 56531 31 1824. ## 2 tablet 51321 36 1426. ## 3 mobile 51504 36 1431. The last step is to select the relevant columns. We will select the device type and the corresponding aov while getting rid of other columns. Use select() to extract the relevant columns. ecom6 &lt;- select(ecom5, device, aov) ecom6 ## # A tibble: 3 x 2 ## device aov ## &lt;fct&gt; &lt;dbl&gt; ## 1 laptop 1824. ## 2 tablet 1426. ## 3 mobile 1431. 3.13 Arrange Data Arranging data in ascending or descending order is one of the most common tasks in data manipulation. We can use arrange to arrange data by different columns. Let us say we want to arrange data by the number of pages browsed. arrange(ecom, n_pages) ## # A tibble: 1,000 x 8 ## referrer device n_visit n_pages duration purchase order_items order_value ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 google laptop 10 1 693 FALSE 0 0 ## 2 yahoo tablet 9 1 459 FALSE 0 0 ## 3 direct laptop 0 1 996 FALSE 0 0 ## 4 yahoo mobile 9 1 955 FALSE 0 0 ## 5 yahoo mobile 10 1 75 FALSE 0 0 ## 6 direct mobile 10 1 908 FALSE 0 0 ## 7 google mobile 6 1 208 FALSE 0 0 ## 8 direct laptop 9 1 738 FALSE 0 0 ## 9 yahoo mobile 7 1 19 FALSE 7 2423 ## 10 bing laptop 1 1 995 FALSE 0 0 ## # ... with 990 more rows If we want to arrange the data in descending order, we can use desc(). Let us arrange the data in descending order. arrange(ecom , desc(n_pages)) ## # A tibble: 1,000 x 8 ## referrer device n_visit n_pages duration purchase order_items order_value ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 social tablet 9 20 420 TRUE 7 1613 ## 2 bing mobile 4 20 440 TRUE 3 184 ## 3 yahoo tablet 0 20 200 FALSE 0 0 ## 4 direct tablet 6 20 580 TRUE 5 1155 ## 5 social mobile 1 20 520 TRUE 5 1021 ## 6 google mobile 8 20 300 TRUE 7 2091 ## 7 social laptop 4 20 200 FALSE 0 0 ## 8 yahoo mobile 3 20 480 FALSE 0 0 ## 9 social laptop 10 20 280 TRUE 1 2011 ## 10 yahoo mobile 2 20 240 FALSE 0 0 ## # ... with 990 more rows Data can be arranged by multiple variables as well. Let us arrange data first by number of visits and then by number of pages in a descending order. arrange(ecom, n_visit, desc(n_pages)) ## # A tibble: 1,000 x 8 ## referrer device n_visit n_pages duration purchase order_items order_value ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 yahoo tablet 0 20 200 FALSE 0 0 ## 2 google laptop 0 19 418 TRUE 2 996 ## 3 bing laptop 0 18 180 FALSE 0 0 ## 4 yahoo laptop 0 18 522 TRUE 8 1523 ## 5 direct tablet 0 18 252 FALSE 0 0 ## 6 social laptop 0 17 204 FALSE 0 0 ## 7 bing laptop 0 17 272 TRUE 9 1384 ## 8 bing mobile 0 16 272 FALSE 0 0 ## 9 yahoo mobile 0 15 255 FALSE 0 0 ## 10 direct laptop 0 15 255 FALSE 0 0 ## # ... with 990 more rows 3.13.1 Case Study If you observe ecom6, the aov column is arranged in descending order. arrange(ecom6, aov) ## # A tibble: 3 x 2 ## device aov ## &lt;fct&gt; &lt;dbl&gt; ## 1 tablet 1426. ## 2 mobile 1431. ## 3 laptop 1824. 3.14 AOV by Devices Let us combine all the code from the above steps: ecom1 &lt;- filter(ecom, purchase) ecom2 &lt;- select(ecom1, device, order_value) ecom3 &lt;- group_by(ecom2, device) ecom4 &lt;- summarise_all(ecom3, funs(revenue = sum, orders = n())) ecom5 &lt;- mutate(ecom4, aov = revenue / orders) ecom6 &lt;- select(ecom5, device, aov) ecom7 &lt;- arrange(ecom6, aov) ecom7 ## # A tibble: 3 x 2 ## device aov ## &lt;fct&gt; &lt;dbl&gt; ## 1 tablet 1426. ## 2 mobile 1431. ## 3 laptop 1824. If you observe, at each step we create a new variable(data frame) and then use it as an input in the next step i.e. the output from one step becomes the input for the next. Can we achieve the final outcome i.e. ecom7 without creating the intermediate data (ecom1 - ecom6)? Yes, we can. We will use the %&gt;% operator to chain the steps and get rid of the intermediate data. ecom %&gt;% filter(purchase) %&gt;% select(device, order_value) %&gt;% group_by(device) %&gt;% summarise_all(funs(revenue = sum, orders = n())) %&gt;% mutate( aov = revenue / orders ) %&gt;% select(device, aov) %&gt;% arrange(aov) ## # A tibble: 3 x 2 ## device aov ## &lt;fct&gt; &lt;dbl&gt; ## 1 tablet 1426. ## 2 mobile 1431. ## 3 laptop 1824. Below we map the description of each step to dplyr verbs. 3.15 Your Turn what is the average number of pages visited by purchasers and non-purchasers? what is the average time on site for purchasers vs non-purchasers? what is the average number of pages visited by purchasers and non-purchasers using mobile? "],
["dplyr2.html", "Chapter 4 Data Wrangling - 2 4.1 Introduction 4.2 Case Study 4.3 Example Data 4.4 Inner Join 4.5 Left Join 4.6 Case Study: Details of customers and their orders irrespective of whether a customer has 4.7 Right Join 4.8 Semi Join 4.9 Anti Join 4.10 Full Join", " Chapter 4 Data Wrangling - 2 4.1 Introduction In this chapter, we will learn to combine tables using different *_join functions provided in dplyr. We will use the following R packages: library(dplyr) library(readr) options(tibble.width = Inf) 4.2 Case Study For our case study, we will use two data sets. The first one, order, contains details of orders placed by different customers. The second data set, customer contains details of each customer. The below table displays the details stored in each data set. Let us import both the data sets using read_csv. 4.2.1 Data: Orders order &lt;- read_delim(&#39;https://raw.githubusercontent.com/rsquaredacademy/datasets/master/order.csv&#39;, delim = &#39;;&#39;) order ## # A tibble: 300 x 3 ## id order_date amount ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 368 7/2/2016 365. ## 2 286 11/2/2016 2064. ## 3 28 2/22/2017 432. ## 4 309 3/5/2017 480. ## 5 2 12/28/2016 235. ## 6 31 12/30/2016 2745. ## 7 179 12/21/2016 2358. ## 8 484 11/24/2016 1031. ## 9 115 9/9/2016 1218. ## 10 340 5/6/2017 1184. ## # ... with 290 more rows 4.2.2 Data: Customers customer &lt;- read_delim(&#39;https://raw.githubusercontent.com/rsquaredacademy/datasets/master/customer.csv&#39;, delim = &#39;;&#39;) customer ## # A tibble: 91 x 3 ## id first_name city ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 Elbertine California ## 2 2 Marcella Colorado ## 3 3 Daria Florida ## 4 4 Sherilyn Distric... ## 5 5 Ketty Texas ## 6 6 Jethro California ## 7 7 Jeremiah California ## 8 8 Constancia Texas ## 9 9 Muire Idaho ## 10 10 Abigail Texas ## # ... with 81 more rows We will explore the following in the case study: details of customers who have placed orders and their order details details of customers and their orders irrespective of whether a customer has placed orders or not customer details for each order details of customers who have placed orders details of customers who have not placed orders details of all customers and all orders 4.3 Example Data We will use another data set to illustrate how the different joins work. You can view the example data sets below. 4.4 Inner Join Inner join return all rows from Age where there are matching values in Height, and all columns from Age and Height. If there are multiple matches between Age and Height, all combination of the matches are returned. 4.4.1 Case Study: Details of customers who have placed orders and their order details To get data for all those customers who have placed orders in the past let us join the order data with the customer data using inner_join. inner_join(customer, order, by = &quot;id&quot;) ## # A tibble: 55 x 5 ## id first_name city order_date amount ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 2 Marcella Colorado 12/28/2016 235. ## 2 2 Marcella Colorado 8/31/2016 1150. ## 3 5 Ketty Texas 1/17/2017 346. ## 4 6 Jethro California 1/27/2017 2317. ## 5 7 Jeremiah California 6/21/2016 136. ## 6 7 Jeremiah California 2/13/2017 1407. ## 7 7 Jeremiah California 7/8/2016 1914. ## 8 8 Constancia Texas 11/5/2016 2461. ## 9 8 Constancia Texas 5/19/2017 2714. ## 10 9 Muire Idaho 12/28/2016 187. ## # ... with 45 more rows 4.5 Left Join Left join return all rows from Age, and all columns from Age and Height. Rows in Age with no match in Height will have NA values in the new columns. If there are multiple matches between Age and Height, all combinations of the matches are returned. 4.6 Case Study: Details of customers and their orders irrespective of whether a customer has placed orders or not. To get data for all those customers and their orders irrespective of whether a customer has placed orders or not let us join the order data with the customer data using left_join. left_join(customer, order, by = &quot;id&quot;) ## # A tibble: 104 x 5 ## id first_name city order_date amount ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 Elbertine California &lt;NA&gt; NA ## 2 2 Marcella Colorado 12/28/2016 235. ## 3 2 Marcella Colorado 8/31/2016 1150. ## 4 3 Daria Florida &lt;NA&gt; NA ## 5 4 Sherilyn Distric... &lt;NA&gt; NA ## 6 5 Ketty Texas 1/17/2017 346. ## 7 6 Jethro California 1/27/2017 2317. ## 8 7 Jeremiah California 6/21/2016 136. ## 9 7 Jeremiah California 2/13/2017 1407. ## 10 7 Jeremiah California 7/8/2016 1914. ## # ... with 94 more rows 4.7 Right Join Right join return all rows from Height, and all columns from Age and Height. Rows in Height with no match in Age will have NA values in the new columns. If there are multiple matches between Age and Height, all combinations of the matches are returned. 4.7.1 Case Study: Customer details for each order To get customer data for all orders, let us join the order data with the customer data using right_join. right_join(customer, order, by = &quot;id&quot;) ## # A tibble: 300 x 5 ## id first_name city order_date amount ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 2 Marcella Colorado 12/28/2016 235. ## 2 2 Marcella Colorado 8/31/2016 1150. ## 3 5 Ketty Texas 1/17/2017 346. ## 4 6 Jethro California 1/27/2017 2317. ## 5 7 Jeremiah California 6/21/2016 136. ## 6 7 Jeremiah California 2/13/2017 1407. ## 7 7 Jeremiah California 7/8/2016 1914. ## 8 8 Constancia Texas 11/5/2016 2461. ## 9 8 Constancia Texas 5/19/2017 2714. ## 10 9 Muire Idaho 12/28/2016 187. ## # ... with 290 more rows 4.8 Semi Join Semi join return all rows from Age where there are matching values in Height, keeping just columns from Age. A semi join differs from an inner join because an inner join will return one row of Age for each matching row of Height, where a semi join will never duplicate rows of Age. 4.8.1 Case Study: Details of customers who have placed orders To get customer data for all orders where customer data exists, let us join the order data with the customer data using semi_join. You can observe that data is returned only for those cases where customer data is present. semi_join(customer, order, by = &quot;id&quot;) ## # A tibble: 42 x 3 ## id first_name city ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 2 Marcella Colorado ## 2 5 Ketty Texas ## 3 6 Jethro California ## 4 7 Jeremiah California ## 5 8 Constancia Texas ## 6 9 Muire Idaho ## 7 15 Valentijn California ## 8 16 Monique Missouri ## 9 20 Colette Texas ## 10 28 Avrit Texas ## # ... with 32 more rows 4.9 Anti Join Anti join return all rows from Age where there are not matching values in Height, keeping just columns from Age. 4.9.1 Case Study: Details of customers who have not placed orders To get details of customers who have not placed orders, let us join the order data with the customer data using anti_join. anti_join(customer, order, by = &quot;id&quot;) ## # A tibble: 49 x 3 ## id first_name city ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 Elbertine California ## 2 3 Daria Florida ## 3 4 Sherilyn Distric... ## 4 10 Abigail Texas ## 5 11 Wynne Georgia ## 6 12 Pietra Minnesota ## 7 13 Bram Iowa ## 8 14 Rees New York ## 9 17 Orazio Louisiana ## 10 18 Mason Texas ## # ... with 39 more rows 4.10 Full Join Full join return all rows and all columns from both Age and Height. Where there are not matching values, returns NA for the one missing. 4.10.1 Case Study: Details of all customers and all orders To get details of all customers and all orders, let us join the order data with the customer data using full_join. full_join(customer, order, by = &quot;id&quot;) ## # A tibble: 349 x 5 ## id first_name city order_date amount ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 Elbertine California &lt;NA&gt; NA ## 2 2 Marcella Colorado 12/28/2016 235. ## 3 2 Marcella Colorado 8/31/2016 1150. ## 4 3 Daria Florida &lt;NA&gt; NA ## 5 4 Sherilyn Distric... &lt;NA&gt; NA ## 6 5 Ketty Texas 1/17/2017 346. ## 7 6 Jethro California 1/27/2017 2317. ## 8 7 Jeremiah California 6/21/2016 136. ## 9 7 Jeremiah California 2/13/2017 1407. ## 10 7 Jeremiah California 7/8/2016 1914. ## # ... with 339 more rows "],
["dplyr3.html", "Chapter 5 Data Wrangling - 3 5.1 Introduction 5.2 Case Study 5.3 Data Sanitization 5.4 Rename Columns 5.5 Data Tabulation 5.6 Sampling Data 5.7 Data Extraction 5.8 Between 5.9 Case When", " Chapter 5 Data Wrangling - 3 5.1 Introduction In this chapter, we will explore a set of helper functions in order to: extract unique rows rename columns sample data extract columns slice rows arrange rows compare tables extract/mutate data using predicate functions count observations for different levels of a variable We will use the following R packages: library(dplyr) library(readr) 5.2 Case Study Let us look at a case study (e-commerce data) and see how we can use dplyr helper functions to answer questions we have about and to modify/transform the underlying data set. 5.2.1 Data ecom &lt;- read_csv(&#39;https://raw.githubusercontent.com/rsquaredacademy/datasets/master/web.csv&#39;, col_types = cols_only(device = col_factor(levels = c(&quot;laptop&quot;, &quot;tablet&quot;, &quot;mobile&quot;)), referrer = col_factor(levels = c(&quot;bing&quot;, &quot;direct&quot;, &quot;social&quot;, &quot;yahoo&quot;, &quot;google&quot;)), purchase = col_logical(), bouncers = col_logical(), duration = col_double(), n_visit = col_double(), n_pages = col_double() ) ) ecom ## # A tibble: 1,000 x 7 ## referrer device bouncers n_visit n_pages duration purchase ## &lt;fct&gt; &lt;fct&gt; &lt;lgl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 google laptop TRUE 10 1 693 FALSE ## 2 yahoo tablet TRUE 9 1 459 FALSE ## 3 direct laptop TRUE 0 1 996 FALSE ## 4 bing tablet FALSE 3 18 468 TRUE ## 5 yahoo mobile TRUE 9 1 955 FALSE ## 6 yahoo laptop FALSE 5 5 135 FALSE ## 7 yahoo mobile TRUE 10 1 75 FALSE ## 8 direct mobile TRUE 10 1 908 FALSE ## 9 bing mobile FALSE 3 19 209 FALSE ## 10 google mobile TRUE 6 1 208 FALSE ## # ... with 990 more rows 5.2.2 Data Dictionary referrer: referrer website/search engine device: device used to visit the website bouncers: whether a visit bounced (exited from landing page) duration: time spent on the website (in seconds) purchase: whether visitor purchased n_visit: number of visits n_pages: number of pages visited/browsed 5.3 Data Sanitization Let us ensure that the data is sanitized by checking the sources of traffic and devices used to visit the site. We will use distinct to examine the values in the referrer column distinct(ecom, referrer) ## # A tibble: 5 x 1 ## referrer ## &lt;fct&gt; ## 1 google ## 2 yahoo ## 3 direct ## 4 bing ## 5 social and the device column as well. distinct(ecom, device) ## # A tibble: 3 x 1 ## device ## &lt;fct&gt; ## 1 laptop ## 2 tablet ## 3 mobile 5.4 Rename Columns Columns can be renamed using rename(). rename(ecom, time_on_site = duration) ## # A tibble: 1,000 x 7 ## referrer device bouncers n_visit n_pages time_on_site purchase ## &lt;fct&gt; &lt;fct&gt; &lt;lgl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 google laptop TRUE 10 1 693 FALSE ## 2 yahoo tablet TRUE 9 1 459 FALSE ## 3 direct laptop TRUE 0 1 996 FALSE ## 4 bing tablet FALSE 3 18 468 TRUE ## 5 yahoo mobile TRUE 9 1 955 FALSE ## 6 yahoo laptop FALSE 5 5 135 FALSE ## 7 yahoo mobile TRUE 10 1 75 FALSE ## 8 direct mobile TRUE 10 1 908 FALSE ## 9 bing mobile FALSE 3 19 209 FALSE ## 10 google mobile TRUE 6 1 208 FALSE ## # ... with 990 more rows 5.5 Data Tabulation Let us now look at the proportion or share of visits driven by different sources of traffic. ecom %&gt;% group_by(referrer) %&gt;% tally() ## # A tibble: 5 x 2 ## referrer n ## * &lt;fct&gt; &lt;int&gt; ## 1 bing 194 ## 2 direct 191 ## 3 social 200 ## 4 yahoo 207 ## 5 google 208 We would also like to know the number of bouncers driven by the different sources of traffic. ecom %&gt;% group_by(referrer, bouncers) %&gt;% tally() ## # A tibble: 10 x 3 ## # Groups: referrer [5] ## referrer bouncers n ## &lt;fct&gt; &lt;lgl&gt; &lt;int&gt; ## 1 bing FALSE 104 ## 2 bing TRUE 90 ## 3 direct FALSE 98 ## 4 direct TRUE 93 ## 5 social FALSE 93 ## 6 social TRUE 107 ## 7 yahoo FALSE 110 ## 8 yahoo TRUE 97 ## 9 google FALSE 101 ## 10 google TRUE 107 Let us look at how many conversions happen across different devices. ecom %&gt;% group_by(device, purchase) %&gt;% tally() %&gt;% filter(purchase) ## # A tibble: 3 x 3 ## # Groups: device [3] ## device purchase n ## &lt;fct&gt; &lt;lgl&gt; &lt;int&gt; ## 1 laptop TRUE 31 ## 2 tablet TRUE 36 ## 3 mobile TRUE 36 Another way to extract the above information is by using count ecom %&gt;% count(referrer, purchase) %&gt;% filter(purchase) ## # A tibble: 5 x 3 ## referrer purchase n ## &lt;fct&gt; &lt;lgl&gt; &lt;int&gt; ## 1 bing TRUE 17 ## 2 direct TRUE 25 ## 3 social TRUE 20 ## 4 yahoo TRUE 22 ## 5 google TRUE 19 5.6 Sampling Data dplyr offers sampling functions which allow us to specify either the number or percentage of observations. sample_n() allows sampling a specific number of observations. sample_n(ecom, 700) ## # A tibble: 700 x 7 ## referrer device bouncers n_visit n_pages duration purchase ## &lt;fct&gt; &lt;fct&gt; &lt;lgl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 yahoo laptop TRUE 5 1 994 FALSE ## 2 direct laptop FALSE 2 16 400 FALSE ## 3 direct mobile TRUE 9 1 242 FALSE ## 4 bing mobile FALSE 4 2 22 FALSE ## 5 google mobile FALSE 9 17 221 FALSE ## 6 bing tablet TRUE 0 1 697 FALSE ## 7 bing laptop TRUE 0 1 897 FALSE ## 8 google laptop TRUE 5 1 376 FALSE ## 9 google laptop FALSE 0 19 418 TRUE ## 10 direct tablet FALSE 10 18 324 TRUE ## # ... with 690 more rows We can combine the sampling functions with other dplyr functions as shown below where we sample observation after grouping them according to the source of traffic. ecom %&gt;% group_by(referrer) %&gt;% sample_n(100) ## # A tibble: 500 x 7 ## # Groups: referrer [5] ## referrer device bouncers n_visit n_pages duration purchase ## &lt;fct&gt; &lt;fct&gt; &lt;lgl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 bing tablet FALSE 1 17 170 FALSE ## 2 bing mobile TRUE 5 1 541 FALSE ## 3 bing tablet TRUE 7 1 489 FALSE ## 4 bing laptop TRUE 6 1 802 FALSE ## 5 bing laptop TRUE 10 1 777 FALSE ## 6 bing mobile FALSE 7 1 22 FALSE ## 7 bing tablet FALSE 3 15 390 TRUE ## 8 bing laptop TRUE 0 1 973 FALSE ## 9 bing mobile FALSE 10 20 560 FALSE ## 10 bing tablet FALSE 4 15 210 FALSE ## # ... with 490 more rows sample_frac() allows a specific percentage of observations. sample_frac(ecom, size = 0.7) ## # A tibble: 700 x 7 ## referrer device bouncers n_visit n_pages duration purchase ## &lt;fct&gt; &lt;fct&gt; &lt;lgl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 yahoo tablet TRUE 4 1 326 FALSE ## 2 google laptop TRUE 2 1 154 FALSE ## 3 google tablet TRUE 5 1 169 FALSE ## 4 google tablet FALSE 9 3 57 FALSE ## 5 bing mobile TRUE 3 1 479 FALSE ## 6 direct mobile TRUE 1 1 423 FALSE ## 7 social tablet TRUE 8 1 502 FALSE ## 8 google laptop TRUE 7 1 79 FALSE ## 9 yahoo laptop TRUE 2 1 83 FALSE ## 10 google tablet TRUE 0 1 866 FALSE ## # ... with 690 more rows 5.7 Data Extraction In the first chapter, we had observed that dplyr verbs always returned a tibble. What if you want to extract a specific column or a bunch of rows but not as a tibble? Use pull to extract columns either by name or position. It will return a vector. In the below example, we extract the device column as a vector. I am using head in addition to limit the output printed. 5.7.1 Sample Data ecom_mini &lt;- sample_n(ecom, size = 10) pull(ecom_mini, device) ## [1] tablet tablet mobile tablet mobile tablet mobile mobile tablet mobile ## Levels: laptop tablet mobile Let us extract the first column from ecom using column position instead of name. pull(ecom_mini, 1) ## [1] direct social direct google google social yahoo google bing direct ## Levels: bing direct social yahoo google You can use - before the column position to indicate the position in reverse. The below example extracts data from the last column. pull(ecom_mini, -1) ## [1] FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE Let us now look at extracting rows using slice(). In the below example, we extract data starting from the 5th row and upto the 15th row. slice(ecom, 5:15) ## # A tibble: 11 x 7 ## referrer device bouncers n_visit n_pages duration purchase ## &lt;fct&gt; &lt;fct&gt; &lt;lgl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 yahoo mobile TRUE 9 1 955 FALSE ## 2 yahoo laptop FALSE 5 5 135 FALSE ## 3 yahoo mobile TRUE 10 1 75 FALSE ## 4 direct mobile TRUE 10 1 908 FALSE ## 5 bing mobile FALSE 3 19 209 FALSE ## 6 google mobile TRUE 6 1 208 FALSE ## 7 direct laptop TRUE 9 1 738 FALSE ## 8 direct tablet FALSE 6 12 132 FALSE ## 9 direct mobile FALSE 9 14 406 TRUE ## 10 yahoo tablet FALSE 5 8 80 FALSE ## 11 yahoo mobile FALSE 7 1 19 FALSE Use n() inside slice() to extract the last row. slice(ecom, n()) ## # A tibble: 1 x 7 ## referrer device bouncers n_visit n_pages duration purchase ## &lt;fct&gt; &lt;fct&gt; &lt;lgl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 google mobile TRUE 9 1 269 FALSE 5.8 Between between() allows us to test if the values in a column lie between two specific values. In the below example, we check how many visits browsed pages between 5 and 15. ecom_sample &lt;- sample_n(ecom, 30) ecom_sample %&gt;% pull(n_pages) %&gt;% between(5, 15) ## [1] FALSE FALSE FALSE TRUE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE ## [13] TRUE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE ## [25] FALSE FALSE FALSE FALSE FALSE FALSE 5.9 Case When case_when() is an alternative to if else. It allows us to lay down the conditions clearly and makes the code more readable. In the below example, we create a new column repeat_visit from n_visit (the number of previous visits). ecom %&gt;% mutate( repeat_visit = case_when( n_visit &gt; 0 ~ TRUE, TRUE ~ FALSE ) ) %&gt;% select(n_visit, repeat_visit) ## # A tibble: 1,000 x 2 ## n_visit repeat_visit ## &lt;dbl&gt; &lt;lgl&gt; ## 1 10 TRUE ## 2 9 TRUE ## 3 0 FALSE ## 4 3 TRUE ## 5 9 TRUE ## 6 5 TRUE ## 7 10 TRUE ## 8 10 TRUE ## 9 3 TRUE ## 10 6 TRUE ## # ... with 990 more rows "],
["pipes.html", "Chapter 6 Pipes 6.1 Introduction 6.2 Pipes 6.3 Data 6.4 head() 6.5 Square Root 6.6 Visualization 6.7 Correlation 6.8 Regression 6.9 String Manipulation 6.10 Data Extraction 6.11 Arithmetic Operations 6.12 Logical Operators", " Chapter 6 Pipes 6.1 Introduction R code contain a lot of parentheses in case of a sequence of multiple operations. When you are dealing with complex code, it results in nested function calls which are hard to read and maintain. The magrittr package by Stefan Milton Bache provides pipes enabling us to write R code that is readable. Pipes allow us to clearly express a sequence of multiple operations by: structuring operations from left to right avoiding nested function calls intermediate steps overwriting of original data minimizing creation of local variables 6.2 Pipes If you are using tidyverse, magrittr will be automatically loaded. We will look at 3 different types of pipes: %&gt;% : pipe a value forward into an expression or function call %&lt;&gt;%: result assigned to left hand side object instead of returning it %$% : expose names within left hand side objects to right hand side expressions We will use the following R packages: library(magrittr) library(readr) library(dplyr) library(stringr) library(purrr) 6.3 Data ecom &lt;- read_csv(&#39;https://raw.githubusercontent.com/rsquaredacademy/datasets/master/web.csv&#39;, col_types = cols_only( referrer = col_factor(levels = c(&quot;bing&quot;, &quot;direct&quot;, &quot;social&quot;, &quot;yahoo&quot;, &quot;google&quot;)), n_pages = col_double(), duration = col_double(), purchase = col_logical() ) ) ecom ## # A tibble: 1,000 x 4 ## referrer n_pages duration purchase ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 google 1 693 FALSE ## 2 yahoo 1 459 FALSE ## 3 direct 1 996 FALSE ## 4 bing 18 468 TRUE ## 5 yahoo 1 955 FALSE ## 6 yahoo 5 135 FALSE ## 7 yahoo 1 75 FALSE ## 8 direct 1 908 FALSE ## 9 bing 19 209 FALSE ## 10 google 1 208 FALSE ## # ... with 990 more rows We will create a smaller data set from the above data to be used in some examples: ecom_mini &lt;- sample_n(ecom, size = 10) ecom_mini ## # A tibble: 10 x 4 ## referrer n_pages duration purchase ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 direct 1 141 FALSE ## 2 social 14 406 FALSE ## 3 direct 1 896 FALSE ## 4 bing 9 261 FALSE ## 5 direct 11 154 FALSE ## 6 direct 1 296 FALSE ## 7 yahoo 1 594 FALSE ## 8 social 12 300 TRUE ## 9 bing 10 260 TRUE ## 10 bing 19 209 FALSE 6.3.1 Data Dictionary referrer: referrer website/search engine n_pages: number of pages visited duration: time spent on the website (in seconds) purchase: whether visitor purchased 6.4 head() Let us start with a simple example. You must be aware of head(). If not, do not worry. It returns the first few observations/rows of data. We can specify the number of observations it should return as well. Let us use it to view the first 10 rows of our data set. head(ecom, 10) ## # A tibble: 10 x 4 ## referrer n_pages duration purchase ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 google 1 693 FALSE ## 2 yahoo 1 459 FALSE ## 3 direct 1 996 FALSE ## 4 bing 18 468 TRUE ## 5 yahoo 1 955 FALSE ## 6 yahoo 5 135 FALSE ## 7 yahoo 1 75 FALSE ## 8 direct 1 908 FALSE ## 9 bing 19 209 FALSE ## 10 google 1 208 FALSE 6.4.1 Using Pipe Now let us do the same but with %&gt;%. ecom %&gt;% head(10) ## # A tibble: 10 x 4 ## referrer n_pages duration purchase ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 google 1 693 FALSE ## 2 yahoo 1 459 FALSE ## 3 direct 1 996 FALSE ## 4 bing 18 468 TRUE ## 5 yahoo 1 955 FALSE ## 6 yahoo 5 135 FALSE ## 7 yahoo 1 75 FALSE ## 8 direct 1 908 FALSE ## 9 bing 19 209 FALSE ## 10 google 1 208 FALSE 6.5 Square Root Time to try a slightly more challenging example. We want the square root of n_pages column from the data set. y &lt;- sqrt(ecom_mini$n_pages) Let us break down the above computation into small steps: select/expose the n_pages column from ecom data compute the square root assign the first few observations to y Let us reproduce y using pipes. # select n_pages variable and assign it to y y &lt;- ecom_mini %$% n_pages # compute square root of y and assign it to y y %&lt;&gt;% sqrt Another way to compute the square root of y is shown below. y &lt;- ecom_mini %$% n_pages %&gt;% sqrt() 6.6 Visualization Let us look at a data visualization example. We will create a bar plot to visualize the frequency of different referrer types that drove purchasers to the website. Let us look at the steps involved in creating the bar plot: extract rows where purchase is TRUE select/expose referrer column tabulate referrer data using table() use the tabulated data to create bar plot using barplot() barplot(table(subset(ecom, purchase)$referrer)) 6.6.1 Using pipe ecom %&gt;% subset(purchase) %&gt;% extract(&#39;referrer&#39;) %&gt;% table() %&gt;% barplot() 6.7 Correlation Correlation is a statistical measure that indicates the extent to which two or more variables fluctuate together. In R, correlation is computed using cor(). Let us look at the correlation between the number of pages browsed and time spent on the site for visitors who purchased some product. Below are the steps for computing correlation: extract rows where purchase is TRUE select/expose n_pages and duration columns use cor() to compute the correlation # without pipe ecom1 &lt;- subset(ecom, purchase) cor(ecom1$n_pages, ecom1$duration) ## [1] 0.4290905 # with pipe ecom %&gt;% subset(purchase) %$% cor(n_pages, duration) ## [1] 0.4290905 # with pipe ecom %&gt;% filter(purchase) %$% cor(n_pages, duration) ## [1] 0.4290905 6.8 Regression Let us look at a regression example. We regress time spent on the site on number of pages visited. Below are the steps involved in running the regression: use duration and n_pages columns from ecom data pass the above data to lm() pass the output from lm() to summary() summary(lm(duration ~ n_pages, data = ecom)) ## ## Call: ## lm(formula = duration ~ n_pages, data = ecom) ## ## Residuals: ## Min 1Q Median 3Q Max ## -386.45 -213.03 -38.93 179.31 602.55 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 404.803 11.323 35.750 &lt; 2e-16 *** ## n_pages -8.355 1.296 -6.449 1.76e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 263.3 on 998 degrees of freedom ## Multiple R-squared: 0.04, Adjusted R-squared: 0.03904 ## F-statistic: 41.58 on 1 and 998 DF, p-value: 1.756e-10 6.8.1 Using pipe ecom %$% lm(duration ~ n_pages) %&gt;% summary() ## ## Call: ## lm(formula = duration ~ n_pages) ## ## Residuals: ## Min 1Q Median 3Q Max ## -386.45 -213.03 -38.93 179.31 602.55 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 404.803 11.323 35.750 &lt; 2e-16 *** ## n_pages -8.355 1.296 -6.449 1.76e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 263.3 on 998 degrees of freedom ## Multiple R-squared: 0.04, Adjusted R-squared: 0.03904 ## F-statistic: 41.58 on 1 and 998 DF, p-value: 1.756e-10 6.9 String Manipulation We want to extract the first name (jovial) from the below email id and convert it to upper case. Below are the steps to achieve this: split the email id using the pattern @ using str_split() extract the first element from the resulting list using extract2() extract the first element from the character vector using extract() extract the first six characters using str_sub() convert to upper case using str_to_upper() email &lt;- &#39;jovialcann@anymail.com&#39; # without pipe str_to_upper(str_sub(str_split(email, &#39;@&#39;)[[1]][1], start = 1, end = 6)) ## [1] &quot;JOVIAL&quot; # with pipe email %&gt;% str_split(pattern = &#39;@&#39;) %&gt;% extract2(1) %&gt;% extract(1) %&gt;% str_sub(start = 1, end = 6) %&gt;% str_to_upper() ## [1] &quot;JOVIAL&quot; Another method that uses map_chr() from the purrr package. email %&gt;% str_split(pattern = &#39;@&#39;) %&gt;% map_chr(1) %&gt;% str_sub(start = 1, end = 6) %&gt;% str_to_upper() ## [1] &quot;JOVIAL&quot; 6.10 Data Extraction Let us turn our attention towards data extraction. magrittr provides alternatives to $, [ and [[. extract() extract2() use_series() 6.10.1 Extract Column To extract a specific column using the column name, we mention the name of the column in single/double quotes within [ or [[. In case of $, we do not use quotes. # base ecom_mini[&#39;n_pages&#39;] ## # A tibble: 10 x 1 ## n_pages ## &lt;dbl&gt; ## 1 1 ## 2 14 ## 3 1 ## 4 9 ## 5 11 ## 6 1 ## 7 1 ## 8 12 ## 9 10 ## 10 19 # magrittr extract(ecom_mini, &#39;n_pages&#39;) ## # A tibble: 10 x 1 ## n_pages ## &lt;dbl&gt; ## 1 1 ## 2 14 ## 3 1 ## 4 9 ## 5 11 ## 6 1 ## 7 1 ## 8 12 ## 9 10 ## 10 19 We can extract columns using their index position. Keep in mind that index position starts from 1 in R. In the below example, we show how to extract n_pages column but instead of using the column name, we use the column position. # base ecom_mini[2] ## # A tibble: 10 x 1 ## n_pages ## &lt;dbl&gt; ## 1 1 ## 2 14 ## 3 1 ## 4 9 ## 5 11 ## 6 1 ## 7 1 ## 8 12 ## 9 10 ## 10 19 # magrittr extract(ecom_mini, 2) ## # A tibble: 10 x 1 ## n_pages ## &lt;dbl&gt; ## 1 1 ## 2 14 ## 3 1 ## 4 9 ## 5 11 ## 6 1 ## 7 1 ## 8 12 ## 9 10 ## 10 19 One important differentiator between [ and [[ is that [[ will return a atomic vector and not a data.frame. $ will also return a atomic vector. In magrittr, we can use use_series() in place of $. # base ecom_mini$n_pages ## [1] 1 14 1 9 11 1 1 12 10 19 # magrittr use_series(ecom_mini, &#39;n_pages&#39;) ## [1] 1 14 1 9 11 1 1 12 10 19 6.10.2 Extract List Element Let us convert ecom_mini into a list using as.list() as shown below: ecom_list &lt;- as.list(ecom_mini) To extract elements of a list, we can use extract2(). It is an alternative for [[. # base ecom_list[[&#39;n_pages&#39;]] ## [1] 1 14 1 9 11 1 1 12 10 19 # magrittr extract2(ecom_list, &#39;n_pages&#39;) ## [1] 1 14 1 9 11 1 1 12 10 19 # base ecom_list[[1]] ## [1] direct social direct bing direct direct yahoo social bing bing ## Levels: bing direct social yahoo google # magrittr extract2(ecom_list, 1) ## [1] direct social direct bing direct direct yahoo social bing bing ## Levels: bing direct social yahoo google We can extract the elements of a list using use_series() as well. # base ecom_list$n_pages ## [1] 1 14 1 9 11 1 1 12 10 19 # magrittr use_series(ecom_list, n_pages) ## [1] 1 14 1 9 11 1 1 12 10 19 6.11 Arithmetic Operations magrittr offer alternatives for arithemtic operations as well. We will look at a few examples below. add() subtract() multiply_by() multiply_by_matrix() divide_by() divide_by_int() mod() raise_to_power() 6.11.1 Addition 1:10 + 1 ## [1] 2 3 4 5 6 7 8 9 10 11 add(1:10, 1) ## [1] 2 3 4 5 6 7 8 9 10 11 `+`(1:10, 1) ## [1] 2 3 4 5 6 7 8 9 10 11 6.11.2 Multiplication 1:10 * 3 ## [1] 3 6 9 12 15 18 21 24 27 30 multiply_by(1:10, 3) ## [1] 3 6 9 12 15 18 21 24 27 30 `*`(1:10, 3) ## [1] 3 6 9 12 15 18 21 24 27 30 6.11.3 Division 1:10 / 2 ## [1] 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 divide_by(1:10, 2) ## [1] 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 `/`(1:10, 2) ## [1] 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 6.11.4 Power 1:10 ^ 2 ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## [19] 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 ## [37] 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 ## [55] 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 ## [73] 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 ## [91] 91 92 93 94 95 96 97 98 99 100 raise_to_power(1:10, 2) ## [1] 1 4 9 16 25 36 49 64 81 100 `^`(1:10, 2) ## [1] 1 4 9 16 25 36 49 64 81 100 6.12 Logical Operators There are alternatives for logical operators as well. We will look at a few examples below. and() or() equals() not() is_greater_than() is_weakly_greater_than() is_less_than() is_weakly_less_than() 6.12.1 Greater Than 1:10 &gt; 5 ## [1] FALSE FALSE FALSE FALSE FALSE TRUE TRUE TRUE TRUE TRUE is_greater_than(1:10, 5) ## [1] FALSE FALSE FALSE FALSE FALSE TRUE TRUE TRUE TRUE TRUE `&gt;`(1:10, 5) ## [1] FALSE FALSE FALSE FALSE FALSE TRUE TRUE TRUE TRUE TRUE 6.12.2 Weakly Greater Than 1:10 &gt;= 5 ## [1] FALSE FALSE FALSE FALSE TRUE TRUE TRUE TRUE TRUE TRUE is_weakly_greater_than(1:10, 5) ## [1] FALSE FALSE FALSE FALSE TRUE TRUE TRUE TRUE TRUE TRUE `&gt;=`(1:10, 5) ## [1] FALSE FALSE FALSE FALSE TRUE TRUE TRUE TRUE TRUE TRUE "],
["tibbles.html", "Chapter 7 tibbles 7.1 Introduction 7.2 Creating tibbles 7.3 tibble features 7.4 Membership Testing 7.5 Tribble 7.6 Column Names 7.7 Add Rows 7.8 Add Columns 7.9 Rownames 7.10 Glimpse 7.11 Check Column 7.12 Summary", " Chapter 7 tibbles 7.1 Introduction A tibble, or tbl_df, is a modern reimagining of the data.frame, keeping what time has proven to be effective, and throwing out what is not. Tibbles are data.frames that are lazy and surly: they do less (i.e. they don’t change variable names or types, and don’t do partial matching) and complain more (e.g. when a variable does not exist). This forces you to confront problems earlier, typically leading to cleaner, more expressive code. Tibbles also have an enhanced print method() which makes them easier to use with large datasets containing complex objects. Source: https://tibble.tidyverse.org/ In this chapter, we will explore tibbles. To be more precise, we will learn: how tibbles are different from data frames? how to create tibbles? how to manipulate tibbles? We will use the following R packages: library(tibble) library(dplyr) 7.2 Creating tibbles tibble can be created using any of the following: tibble() as_tibble() tribble() Let us start with tibble(). tibble(x = letters, y = 1:26, z = sample(100, 26)) ## # A tibble: 26 x 3 ## x y z ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 a 1 17 ## 2 b 2 40 ## 3 c 3 18 ## 4 d 4 78 ## 5 e 5 61 ## 6 f 6 90 ## 7 g 7 67 ## 8 h 8 68 ## 9 i 9 22 ## 10 j 10 45 ## # ... with 16 more rows We mentioned the column names followed by the data. If you do not specify the column names, tibble() will supply them. Ensure that the length of each column is same. 7.3 tibble features 7.3.1 never changes input’s types tibble() will never alter the input’s type. For example, if you supply a character vector it will not be converted to factor unlike data.frame where you need to set stringsAsFactors to FALSE. tibble(x = letters, y = 1:26, z = sample(100, 26)) ## # A tibble: 26 x 3 ## x y z ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 a 1 96 ## 2 b 2 18 ## 3 c 3 6 ## 4 d 4 55 ## 5 e 5 93 ## 6 f 6 3 ## 7 g 7 35 ## 8 h 8 66 ## 9 i 9 95 ## 10 j 10 49 ## # ... with 16 more rows 7.3.2 never adjusts variable names tibble() will never modify the column names. In the below example, you can observe that while data.frame adds a ., tibble() retains the column names as is. names(data.frame(`order value` = 10)) ## [1] &quot;order.value&quot; names(tibble(`order value` = 10)) ## [1] &quot;order value&quot; 7.3.3 never prints all rows tibble() will never print all the rows and clutter your console. It will only print the first 10 rows and only as many columns that fit the width of the console. x &lt;- 1:100 y &lt;- letters[1] z &lt;- sample(c(TRUE, FALSE), 100, replace = TRUE) tibble(x, y, z) ## # A tibble: 100 x 3 ## x y z ## &lt;int&gt; &lt;chr&gt; &lt;lgl&gt; ## 1 1 a FALSE ## 2 2 a FALSE ## 3 3 a FALSE ## 4 4 a FALSE ## 5 5 a TRUE ## 6 6 a FALSE ## 7 7 a FALSE ## 8 8 a FALSE ## 9 9 a TRUE ## 10 10 a FALSE ## # ... with 90 more rows 7.3.4 never recycles vector of length greater than 1 Recycling vectors of length greater than 1 often leads to errors and as such tibble() will only recycle vectors of length 1. x &lt;- 1:100 y &lt;- letters z &lt;- sample(c(TRUE, FALSE), 100, replace = TRUE) tibble(x, y, z) Error in overscope_eval_next(overscope, expr) : object &#39;y&#39; not found 7.4 Membership Testing We can test if an object is a tibble using is_tibble(). is_tibble(mtcars) ## [1] FALSE is_tibble(as_tibble(mtcars)) ## [1] TRUE 7.5 Tribble Another way to create tibbles is using tribble(): it is short for transposed tibbles it is customized for data entry in code column names start with ~ and values are separated by commas tribble( ~x, ~y, ~z, #--|--|---- 1, TRUE, &#39;a&#39;, 2, FALSE, &#39;b&#39; ) ## # A tibble: 2 x 3 ## x y z ## &lt;dbl&gt; &lt;lgl&gt; &lt;chr&gt; ## 1 1 TRUE a ## 2 2 FALSE b 7.6 Column Names Names of the columns in tibbles need not be valid R variable names. They can contain unusual characters like a space or a smiley but must be enclosed in ticks. tibble( ` ` = &#39;space&#39;, `2` = &#39;integer&#39;, `:)` = &#39;smiley&#39; ) ## # A tibble: 1 x 3 ## ` ` `2` `:)` ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 space integer smiley 7.7 Add Rows Let us add data related to Safari browser to the web traffic data using add_row(). browsers &lt;- enframe(c(chrome = 40, firefox = 20, edge = 30)) browsers ## # A tibble: 3 x 2 ## name value ## &lt;chr&gt; &lt;dbl&gt; ## 1 chrome 40 ## 2 firefox 20 ## 3 edge 30 add_row(browsers, name = &#39;safari&#39;, value = 10) ## # A tibble: 4 x 2 ## name value ## &lt;chr&gt; &lt;dbl&gt; ## 1 chrome 40 ## 2 firefox 20 ## 3 edge 30 ## 4 safari 10 If we want to add the data at a particular row, we can specify the row number using the .before argument. Let us add the data related to Safari browser in the second row instead of the last row. add_row(browsers, name = &#39;safari&#39;, value = 10, .before = 2) ## # A tibble: 4 x 2 ## name value ## &lt;chr&gt; &lt;dbl&gt; ## 1 chrome 40 ## 2 safari 10 ## 3 firefox 20 ## 4 edge 30 7.8 Add Columns add_column() adds a new column to tibbles. browsers &lt;- enframe(c(chrome = 40, firefox = 20, edge = 30, safari = 10)) add_column(browsers, visits = c(4000, 2000, 3000, 1000)) ## # A tibble: 4 x 3 ## name value visits ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 chrome 40 4000 ## 2 firefox 20 2000 ## 3 edge 30 3000 ## 4 safari 10 1000 7.9 Rownames The tibble package provides a set of functions to deal with rownames. Remember, tibble does not have rownames unlike data.frame. To check whether a data set has rownames, use has_rownames(). has_rownames(mtcars) ## [1] TRUE 7.9.1 Remove Rownames remove_rownames(mtcars) ## mpg cyl disp hp drat wt qsec vs am gear carb ## 1 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 4 ## 2 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 4 ## 3 22.8 4 108.0 93 3.85 2.320 18.61 1 1 4 1 ## 4 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1 ## 5 18.7 8 360.0 175 3.15 3.440 17.02 0 0 3 2 ## 6 18.1 6 225.0 105 2.76 3.460 20.22 1 0 3 1 ## 7 14.3 8 360.0 245 3.21 3.570 15.84 0 0 3 4 ## 8 24.4 4 146.7 62 3.69 3.190 20.00 1 0 4 2 ## 9 22.8 4 140.8 95 3.92 3.150 22.90 1 0 4 2 ## 10 19.2 6 167.6 123 3.92 3.440 18.30 1 0 4 4 ## 11 17.8 6 167.6 123 3.92 3.440 18.90 1 0 4 4 ## 12 16.4 8 275.8 180 3.07 4.070 17.40 0 0 3 3 ## 13 17.3 8 275.8 180 3.07 3.730 17.60 0 0 3 3 ## 14 15.2 8 275.8 180 3.07 3.780 18.00 0 0 3 3 ## 15 10.4 8 472.0 205 2.93 5.250 17.98 0 0 3 4 ## 16 10.4 8 460.0 215 3.00 5.424 17.82 0 0 3 4 ## 17 14.7 8 440.0 230 3.23 5.345 17.42 0 0 3 4 ## 18 32.4 4 78.7 66 4.08 2.200 19.47 1 1 4 1 ## 19 30.4 4 75.7 52 4.93 1.615 18.52 1 1 4 2 ## 20 33.9 4 71.1 65 4.22 1.835 19.90 1 1 4 1 ## 21 21.5 4 120.1 97 3.70 2.465 20.01 1 0 3 1 ## 22 15.5 8 318.0 150 2.76 3.520 16.87 0 0 3 2 ## 23 15.2 8 304.0 150 3.15 3.435 17.30 0 0 3 2 ## 24 13.3 8 350.0 245 3.73 3.840 15.41 0 0 3 4 ## 25 19.2 8 400.0 175 3.08 3.845 17.05 0 0 3 2 ## 26 27.3 4 79.0 66 4.08 1.935 18.90 1 1 4 1 ## 27 26.0 4 120.3 91 4.43 2.140 16.70 0 1 5 2 ## 28 30.4 4 95.1 113 3.77 1.513 16.90 1 1 5 2 ## 29 15.8 8 351.0 264 4.22 3.170 14.50 0 1 5 4 ## 30 19.7 6 145.0 175 3.62 2.770 15.50 0 1 5 6 ## 31 15.0 8 301.0 335 3.54 3.570 14.60 0 1 5 8 ## 32 21.4 4 121.0 109 4.11 2.780 18.60 1 1 4 2 7.9.2 Rownames to Column head(rownames_to_column(mtcars)) ## rowname mpg cyl disp hp drat wt qsec vs am gear carb ## 1 Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## 2 Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## 3 Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 ## 4 Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 ## 5 Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 ## 6 Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 7.9.3 Column to Rownames To convert the first column in the data set to rownames, use column_to_rownames(): mtcars_tbl &lt;- rownames_to_column(mtcars) column_to_rownames(mtcars_tbl) ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108.0 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360.0 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225.0 105 2.76 3.460 20.22 1 0 3 1 ## Duster 360 14.3 8 360.0 245 3.21 3.570 15.84 0 0 3 4 ## Merc 240D 24.4 4 146.7 62 3.69 3.190 20.00 1 0 4 2 ## Merc 230 22.8 4 140.8 95 3.92 3.150 22.90 1 0 4 2 ## Merc 280 19.2 6 167.6 123 3.92 3.440 18.30 1 0 4 4 ## Merc 280C 17.8 6 167.6 123 3.92 3.440 18.90 1 0 4 4 ## Merc 450SE 16.4 8 275.8 180 3.07 4.070 17.40 0 0 3 3 ## Merc 450SL 17.3 8 275.8 180 3.07 3.730 17.60 0 0 3 3 ## Merc 450SLC 15.2 8 275.8 180 3.07 3.780 18.00 0 0 3 3 ## Cadillac Fleetwood 10.4 8 472.0 205 2.93 5.250 17.98 0 0 3 4 ## Lincoln Continental 10.4 8 460.0 215 3.00 5.424 17.82 0 0 3 4 ## Chrysler Imperial 14.7 8 440.0 230 3.23 5.345 17.42 0 0 3 4 ## Fiat 128 32.4 4 78.7 66 4.08 2.200 19.47 1 1 4 1 ## Honda Civic 30.4 4 75.7 52 4.93 1.615 18.52 1 1 4 2 ## Toyota Corolla 33.9 4 71.1 65 4.22 1.835 19.90 1 1 4 1 ## Toyota Corona 21.5 4 120.1 97 3.70 2.465 20.01 1 0 3 1 ## Dodge Challenger 15.5 8 318.0 150 2.76 3.520 16.87 0 0 3 2 ## AMC Javelin 15.2 8 304.0 150 3.15 3.435 17.30 0 0 3 2 ## Camaro Z28 13.3 8 350.0 245 3.73 3.840 15.41 0 0 3 4 ## Pontiac Firebird 19.2 8 400.0 175 3.08 3.845 17.05 0 0 3 2 ## Fiat X1-9 27.3 4 79.0 66 4.08 1.935 18.90 1 1 4 1 ## Porsche 914-2 26.0 4 120.3 91 4.43 2.140 16.70 0 1 5 2 ## Lotus Europa 30.4 4 95.1 113 3.77 1.513 16.90 1 1 5 2 ## Ford Pantera L 15.8 8 351.0 264 4.22 3.170 14.50 0 1 5 4 ## Ferrari Dino 19.7 6 145.0 175 3.62 2.770 15.50 0 1 5 6 ## Maserati Bora 15.0 8 301.0 335 3.54 3.570 14.60 0 1 5 8 ## Volvo 142E 21.4 4 121.0 109 4.11 2.780 18.60 1 1 4 2 7.10 Glimpse Use glimpse() to get an overview of the data. glimpse(mtcars) ## Rows: 32 ## Columns: 11 ## $ mpg &lt;dbl&gt; 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17... ## $ cyl &lt;dbl&gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4,... ## $ disp &lt;dbl&gt; 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 140.8,... ## $ hp &lt;dbl&gt; 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 180, ... ## $ drat &lt;dbl&gt; 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.92, 3.... ## $ wt &lt;dbl&gt; 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3.150,... ## $ qsec &lt;dbl&gt; 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 22.90,... ## $ vs &lt;dbl&gt; 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,... ## $ am &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,... ## $ gear &lt;dbl&gt; 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3,... ## $ carb &lt;dbl&gt; 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, 1, 1,... 7.11 Check Column has_name() can be used to check if a tibble has a specific column. has_name(mtcars, &#39;cyl&#39;) ## [1] TRUE has_name(mtcars, &#39;gears&#39;) ## [1] FALSE 7.12 Summary 7.12.1 Creating tibbles use tibble() to create tibbles use as_tibble() to coerce other objects to tibble use enframe() to coerce vector to tibble use tribble() to create tibble using data entry 7.12.2 Modifying tibbles use add_row() to add a new row use add_column() to add a new column use remove_rownames() to remove rownames from data use rownames_to_colum() to coerce rowname to first column use column_to_rownames() to coerce first column to rownames 7.12.3 Testing tibbles use is_tibble() to test if an object is a tibble use has_rownames() to check whether a data set has rownames use has_name() to check if tibble has a specific column use glimpse() to get an overview of data "],
["stringr.html", "Chapter 8 Handling String Data 8.1 Introduction 8.2 Case Study 8.3 Overview 8.4 Extract domain name from email ids 8.5 Extract Domain Extension 8.6 Extract image type from URL 8.7 Extract Image Dimesion from URL 8.8 Extract HTTP Protocol from URL 8.9 Extract file type", " Chapter 8 Handling String Data 8.1 Introduction In this chapter, we will learn to work with string data in R using stringr. As we did in the other chapters, we will use a case study to explore the various features of the stringr package. We will use the following R packages: library(stringr) library(tibble) library(magrittr) library(purrr) library(dplyr) library(readr) 8.2 Case Study extract domain name from random email ids extract image type from url extract image dimension from url extract extension from domain name extract http protocol from url extract file type from url 8.2.1 Data mockstring &lt;- read_csv(&#39;https://raw.githubusercontent.com/rsquaredacademy/datasets/master/mock_strings.csv&#39;) mockstring ## # A tibble: 1,000 x 12 ## id ## &lt;dbl&gt; ## 1 1 ## 2 2 ## 3 3 ## 4 4 ## 5 5 ## 6 6 ## 7 7 ## 8 8 ## 9 9 ## 10 10 ## image_url ## &lt;chr&gt; ## 1 https://robohash.org/providentassumendaexplicabo.jpg?size=50x50&amp;set=set1 ## 2 https://robohash.org/etillumvoluptate.jpg?size=50x50&amp;set=set1 ## 3 https://robohash.org/nonoptiovoluptatibus.jpg?size=50x50&amp;set=set1 ## 4 https://robohash.org/voluptatumauthic.jpg?size=50x50&amp;set=set1 ## 5 https://robohash.org/placeaterrorqui.jpg?size=50x50&amp;set=set1 ## 6 https://robohash.org/temporeutea.jpg?size=50x50&amp;set=set1 ## 7 https://robohash.org/maximesaepequi.bmp?size=50x50&amp;set=set1 ## 8 https://robohash.org/nemoautesse.png?size=50x50&amp;set=set1 ## 9 https://robohash.org/odiorerumaut.png?size=50x50&amp;set=set1 ## 10 https://robohash.org/omnismolestiaearchitecto.png?size=50x50&amp;set=set1 ## domain imageurl ## &lt;chr&gt; &lt;chr&gt; ## 1 addtoany.com http://dummyimage.com/130x183.jpg/dddddd/000000 ## 2 gmpg.org http://dummyimage.com/106x217.bmp/dddddd/000000 ## 3 samsung.com http://dummyimage.com/146x127.bmp/cc0000/ffffff ## 4 spotify.com http://dummyimage.com/181x194.png/5fa2dd/ffffff ## 5 wunderground.com http://dummyimage.com/220x123.jpg/ff4444/ffffff ## 6 alexa.com http://dummyimage.com/118x176.bmp/dddddd/000000 ## 7 google.it http://dummyimage.com/185x202.jpg/ff4444/ffffff ## 8 ed.gov http://dummyimage.com/223x163.jpg/ff4444/ffffff ## 9 jigsy.com http://dummyimage.com/145x113.jpg/5fa2dd/ffffff ## 10 jugem.jp http://dummyimage.com/238x214.png/cc0000/ffffff ## email filename phone ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 mnewburn0@fastcompany.com PedeMalesuada.xls 66-(777)902-6181 ## 2 mdankersley1@digg.com LobortisVel.mp3 351-(422)736-6807 ## 3 hgirhard2@altervista.org CongueDiamId.pdf 33-(371)684-5114 ## 4 pmcmenamy3@sciencedirect.com EleifendQuam.avi 86-(410)823-6712 ## 5 drisbrough4@bandcamp.com PurusPhasellus.mp3 223-(518)814-6361 ## 6 cphlippi5@surveymonkey.com ElementumInHac.avi 420-(760)354-8671 ## 7 kdodswell6@un.org Mattis.doc 1-(712)615-2879 ## 8 vhourihane7@ovh.net PurusEu.tiff 62-(437)705-1118 ## 9 rdike8@timesonline.co.uk JustoEtiamPretium.xls 1-(683)965-1323 ## 10 tdudbridge9@clickbank.net Ante.tiff 30-(553)559-7448 ## address ## &lt;chr&gt; ## 1 8 Anhalt Crossing ## 2 697 East Avenue ## 3 89 Dottie Circle ## 4 98135 Blue Bill Park Drive ## 5 7814 Pennsylvania Street ## 6 4897 Little Fleur Drive ## 7 53541 Morrow Center ## 8 4819 Hermina Parkway ## 9 68096 Monument Park ## 10 9595 Spaight Avenue ## url ## &lt;chr&gt; ## 1 https://engadget.com/nascetur/ridiculus/mus/vivamus/vestibulum.jsp?eu=est&amp;ti~ ## 2 http://delicious.com/phasellus/in/felis/donec.json?interdum=risus&amp;mauris=dap~ ## 3 https://w3.org/sed/augue/aliquam/erat/volutpat.json?dictumst=mi&amp;morbi=sit&amp;ve~ ## 4 http://indiatimes.com/pede/lobortis/ligula/sit/amet.jpg?quam=nullam&amp;sollicit~ ## 5 https://tumblr.com/id/mauris/vulputate/elementum.png?tincidunt=maecenas&amp;eget~ ## 6 https://unblog.fr/est/quam/pharetra.jpg?amet=phasellus&amp;erat=sit&amp;nulla=amet&amp;t~ ## 7 http://vinaora.com/posuere.jpg?convallis=in&amp;nulla=faucibus&amp;neque=orci&amp;libero~ ## 8 https://globo.com/accumsan.png?elementum=eu&amp;pellentesque=mi&amp;quisque=nulla&amp;po~ ## 9 https://xing.com/elementum/eu/interdum/eu/tincidunt.html?sit=proin&amp;amet=eu&amp;s~ ## 10 https://bigcartel.com/tortor/quis/turpis/sed/ante/vivamus.html?in=lorem&amp;elei~ ## full_name currency passwords ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Mufi Ruit ¥34.37 VybPYpEXUjJh6nQk ## 2 Leese Furmagier $67.37 mxET3n6dz42X8YUv ## 3 Blakelee Wilshire €33,85 Z9f4WeNVQ28FwKML ## 4 Terencio McIllrick €42,89 Ndbm8nwCps6jUze3 ## 5 Debee McErlaine €13,19 U3Lj9xJw8NHzB5Sg ## 6 Fran Painten ¥87.35 KEhVAC3QNvjWDFJ7 ## 7 Frasco Bowich $34.89 jydGPCW7fa2bZpU4 ## 8 Car Ponten ¥41.66 pytVHesNZjAL8WKc ## 9 Tades Checcucci €70,80 Rsw4EQGk9tKTnzDp ## 10 Wilton Kemmey €62,76 KvrNGQ7yL3pfsaZA ## # ... with 990 more rows 8.2.2 Data Dictionary domain: dummy website domain imageurl: url of an image email: dummy email id filename: dummy file name with different extensions phone: dummy phone number address: dummy address with door and street names url: randomyly generated urls full_name: dummy first and last names currency: different currencies passwords: dummy passwords 8.3 Overview Before we start with the case study, let us take a quick tour of stringr and introduce ourselves to some of the functions we will be using later in the case study. One of the columns in the case study data is email. It contains random email ids. We want to ensure that the email ids adher to a particular format .i.e they contain @ they contain only one @ Let us first detect if the email ids contain @. Since the data set has 1000 rows, we will use a smaller sample in the examples. mockdata &lt;- slice(mockstring, 1:10) mockdata ## # A tibble: 10 x 12 ## id ## &lt;dbl&gt; ## 1 1 ## 2 2 ## 3 3 ## 4 4 ## 5 5 ## 6 6 ## 7 7 ## 8 8 ## 9 9 ## 10 10 ## image_url ## &lt;chr&gt; ## 1 https://robohash.org/providentassumendaexplicabo.jpg?size=50x50&amp;set=set1 ## 2 https://robohash.org/etillumvoluptate.jpg?size=50x50&amp;set=set1 ## 3 https://robohash.org/nonoptiovoluptatibus.jpg?size=50x50&amp;set=set1 ## 4 https://robohash.org/voluptatumauthic.jpg?size=50x50&amp;set=set1 ## 5 https://robohash.org/placeaterrorqui.jpg?size=50x50&amp;set=set1 ## 6 https://robohash.org/temporeutea.jpg?size=50x50&amp;set=set1 ## 7 https://robohash.org/maximesaepequi.bmp?size=50x50&amp;set=set1 ## 8 https://robohash.org/nemoautesse.png?size=50x50&amp;set=set1 ## 9 https://robohash.org/odiorerumaut.png?size=50x50&amp;set=set1 ## 10 https://robohash.org/omnismolestiaearchitecto.png?size=50x50&amp;set=set1 ## domain imageurl ## &lt;chr&gt; &lt;chr&gt; ## 1 addtoany.com http://dummyimage.com/130x183.jpg/dddddd/000000 ## 2 gmpg.org http://dummyimage.com/106x217.bmp/dddddd/000000 ## 3 samsung.com http://dummyimage.com/146x127.bmp/cc0000/ffffff ## 4 spotify.com http://dummyimage.com/181x194.png/5fa2dd/ffffff ## 5 wunderground.com http://dummyimage.com/220x123.jpg/ff4444/ffffff ## 6 alexa.com http://dummyimage.com/118x176.bmp/dddddd/000000 ## 7 google.it http://dummyimage.com/185x202.jpg/ff4444/ffffff ## 8 ed.gov http://dummyimage.com/223x163.jpg/ff4444/ffffff ## 9 jigsy.com http://dummyimage.com/145x113.jpg/5fa2dd/ffffff ## 10 jugem.jp http://dummyimage.com/238x214.png/cc0000/ffffff ## email filename phone ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 mnewburn0@fastcompany.com PedeMalesuada.xls 66-(777)902-6181 ## 2 mdankersley1@digg.com LobortisVel.mp3 351-(422)736-6807 ## 3 hgirhard2@altervista.org CongueDiamId.pdf 33-(371)684-5114 ## 4 pmcmenamy3@sciencedirect.com EleifendQuam.avi 86-(410)823-6712 ## 5 drisbrough4@bandcamp.com PurusPhasellus.mp3 223-(518)814-6361 ## 6 cphlippi5@surveymonkey.com ElementumInHac.avi 420-(760)354-8671 ## 7 kdodswell6@un.org Mattis.doc 1-(712)615-2879 ## 8 vhourihane7@ovh.net PurusEu.tiff 62-(437)705-1118 ## 9 rdike8@timesonline.co.uk JustoEtiamPretium.xls 1-(683)965-1323 ## 10 tdudbridge9@clickbank.net Ante.tiff 30-(553)559-7448 ## address ## &lt;chr&gt; ## 1 8 Anhalt Crossing ## 2 697 East Avenue ## 3 89 Dottie Circle ## 4 98135 Blue Bill Park Drive ## 5 7814 Pennsylvania Street ## 6 4897 Little Fleur Drive ## 7 53541 Morrow Center ## 8 4819 Hermina Parkway ## 9 68096 Monument Park ## 10 9595 Spaight Avenue ## url ## &lt;chr&gt; ## 1 https://engadget.com/nascetur/ridiculus/mus/vivamus/vestibulum.jsp?eu=est&amp;ti~ ## 2 http://delicious.com/phasellus/in/felis/donec.json?interdum=risus&amp;mauris=dap~ ## 3 https://w3.org/sed/augue/aliquam/erat/volutpat.json?dictumst=mi&amp;morbi=sit&amp;ve~ ## 4 http://indiatimes.com/pede/lobortis/ligula/sit/amet.jpg?quam=nullam&amp;sollicit~ ## 5 https://tumblr.com/id/mauris/vulputate/elementum.png?tincidunt=maecenas&amp;eget~ ## 6 https://unblog.fr/est/quam/pharetra.jpg?amet=phasellus&amp;erat=sit&amp;nulla=amet&amp;t~ ## 7 http://vinaora.com/posuere.jpg?convallis=in&amp;nulla=faucibus&amp;neque=orci&amp;libero~ ## 8 https://globo.com/accumsan.png?elementum=eu&amp;pellentesque=mi&amp;quisque=nulla&amp;po~ ## 9 https://xing.com/elementum/eu/interdum/eu/tincidunt.html?sit=proin&amp;amet=eu&amp;s~ ## 10 https://bigcartel.com/tortor/quis/turpis/sed/ante/vivamus.html?in=lorem&amp;elei~ ## full_name currency passwords ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Mufi Ruit ¥34.37 VybPYpEXUjJh6nQk ## 2 Leese Furmagier $67.37 mxET3n6dz42X8YUv ## 3 Blakelee Wilshire €33,85 Z9f4WeNVQ28FwKML ## 4 Terencio McIllrick €42,89 Ndbm8nwCps6jUze3 ## 5 Debee McErlaine €13,19 U3Lj9xJw8NHzB5Sg ## 6 Fran Painten ¥87.35 KEhVAC3QNvjWDFJ7 ## 7 Frasco Bowich $34.89 jydGPCW7fa2bZpU4 ## 8 Car Ponten ¥41.66 pytVHesNZjAL8WKc ## 9 Tades Checcucci €70,80 Rsw4EQGk9tKTnzDp ## 10 Wilton Kemmey €62,76 KvrNGQ7yL3pfsaZA Use str_detect() to detect @ and str_count() to count the number of times @ appears in the email ids. # detect @ str_detect(mockdata$email, pattern = &quot;@&quot;) ## [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE # count @ str_count(mockdata$email, pattern = &quot;@&quot;) ## [1] 1 1 1 1 1 1 1 1 1 1 We can use str_c() to concatenate strings. Let us add the string email id: before each email id in the data set. str_c(&quot;email id:&quot;, mockdata$email) ## [1] &quot;email id:mnewburn0@fastcompany.com&quot; ## [2] &quot;email id:mdankersley1@digg.com&quot; ## [3] &quot;email id:hgirhard2@altervista.org&quot; ## [4] &quot;email id:pmcmenamy3@sciencedirect.com&quot; ## [5] &quot;email id:drisbrough4@bandcamp.com&quot; ## [6] &quot;email id:cphlippi5@surveymonkey.com&quot; ## [7] &quot;email id:kdodswell6@un.org&quot; ## [8] &quot;email id:vhourihane7@ovh.net&quot; ## [9] &quot;email id:rdike8@timesonline.co.uk&quot; ## [10] &quot;email id:tdudbridge9@clickbank.net&quot; If we want to split a string into two parts using a particular pattern, we use str_split(). Let us split the domain name and extension from the domain column in the data. The domain name and extension are separated by . and we will use it to split the domain column. Since . is a special character, we will use two slashes to escape the special character. str_split(mockdata$domain, pattern = &quot;\\\\.&quot;) ## [[1]] ## [1] &quot;addtoany&quot; &quot;com&quot; ## ## [[2]] ## [1] &quot;gmpg&quot; &quot;org&quot; ## ## [[3]] ## [1] &quot;samsung&quot; &quot;com&quot; ## ## [[4]] ## [1] &quot;spotify&quot; &quot;com&quot; ## ## [[5]] ## [1] &quot;wunderground&quot; &quot;com&quot; ## ## [[6]] ## [1] &quot;alexa&quot; &quot;com&quot; ## ## [[7]] ## [1] &quot;google&quot; &quot;it&quot; ## ## [[8]] ## [1] &quot;ed&quot; &quot;gov&quot; ## ## [[9]] ## [1] &quot;jigsy&quot; &quot;com&quot; ## ## [[10]] ## [1] &quot;jugem&quot; &quot;jp&quot; We can truncate a string using str_trunc(). The default truncation happens at the beggining of the string but we can truncate the central part or the end of the string as well. str_trunc(mockdata$email, width = 10) ## [1] &quot;mnewbur...&quot; &quot;mdanker...&quot; &quot;hgirhar...&quot; &quot;pmcmena...&quot; &quot;drisbro...&quot; ## [6] &quot;cphlipp...&quot; &quot;kdodswe...&quot; &quot;vhourih...&quot; &quot;rdike8@...&quot; &quot;tdudbri...&quot; str_trunc(mockdata$email, width = 10, side = &quot;left&quot;) ## [1] &quot;...any.com&quot; &quot;...igg.com&quot; &quot;...sta.org&quot; &quot;...ect.com&quot; &quot;...amp.com&quot; ## [6] &quot;...key.com&quot; &quot;...@un.org&quot; &quot;...ovh.net&quot; &quot;...e.co.uk&quot; &quot;...ank.net&quot; str_trunc(mockdata$email, width = 10, side = &quot;center&quot;) ## [1] &quot;mnew...com&quot; &quot;mdan...com&quot; &quot;hgir...org&quot; &quot;pmcm...com&quot; &quot;dris...com&quot; ## [6] &quot;cphl...com&quot; &quot;kdod...org&quot; &quot;vhou...net&quot; &quot;rdik....uk&quot; &quot;tdud...net&quot; Strings can be sorted using str_sort(). Let us quickly sort the emails in both ascending and descending orders. str_sort(mockdata$email) ## [1] &quot;cphlippi5@surveymonkey.com&quot; &quot;drisbrough4@bandcamp.com&quot; ## [3] &quot;hgirhard2@altervista.org&quot; &quot;kdodswell6@un.org&quot; ## [5] &quot;mdankersley1@digg.com&quot; &quot;mnewburn0@fastcompany.com&quot; ## [7] &quot;pmcmenamy3@sciencedirect.com&quot; &quot;rdike8@timesonline.co.uk&quot; ## [9] &quot;tdudbridge9@clickbank.net&quot; &quot;vhourihane7@ovh.net&quot; str_sort(mockdata$email, decreasing = TRUE) ## [1] &quot;vhourihane7@ovh.net&quot; &quot;tdudbridge9@clickbank.net&quot; ## [3] &quot;rdike8@timesonline.co.uk&quot; &quot;pmcmenamy3@sciencedirect.com&quot; ## [5] &quot;mnewburn0@fastcompany.com&quot; &quot;mdankersley1@digg.com&quot; ## [7] &quot;kdodswell6@un.org&quot; &quot;hgirhard2@altervista.org&quot; ## [9] &quot;drisbrough4@bandcamp.com&quot; &quot;cphlippi5@surveymonkey.com&quot; The case of a string can be changed to upper, lower or title case as shown below. str_to_upper(mockdata$full_name) ## [1] &quot;MUFI RUIT&quot; &quot;LEESE FURMAGIER&quot; &quot;BLAKELEE WILSHIRE&quot; ## [4] &quot;TERENCIO MCILLRICK&quot; &quot;DEBEE MCERLAINE&quot; &quot;FRAN PAINTEN&quot; ## [7] &quot;FRASCO BOWICH&quot; &quot;CAR PONTEN&quot; &quot;TADES CHECCUCCI&quot; ## [10] &quot;WILTON KEMMEY&quot; str_to_lower(mockdata$full_name) ## [1] &quot;mufi ruit&quot; &quot;leese furmagier&quot; &quot;blakelee wilshire&quot; ## [4] &quot;terencio mcillrick&quot; &quot;debee mcerlaine&quot; &quot;fran painten&quot; ## [7] &quot;frasco bowich&quot; &quot;car ponten&quot; &quot;tades checcucci&quot; ## [10] &quot;wilton kemmey&quot; Parts of a string can be replaced using str_replace(). In the address column of the data set, let us replace: Street with ST Road with RD str_replace(mockdata$address, &quot;Street&quot;, &quot;ST&quot;) ## [1] &quot;8 Anhalt Crossing&quot; &quot;697 East Avenue&quot; ## [3] &quot;89 Dottie Circle&quot; &quot;98135 Blue Bill Park Drive&quot; ## [5] &quot;7814 Pennsylvania ST&quot; &quot;4897 Little Fleur Drive&quot; ## [7] &quot;53541 Morrow Center&quot; &quot;4819 Hermina Parkway&quot; ## [9] &quot;68096 Monument Park&quot; &quot;9595 Spaight Avenue&quot; str_replace(mockdata$address, &quot;Road&quot;, &quot;RD&quot;) ## [1] &quot;8 Anhalt Crossing&quot; &quot;697 East Avenue&quot; ## [3] &quot;89 Dottie Circle&quot; &quot;98135 Blue Bill Park Drive&quot; ## [5] &quot;7814 Pennsylvania Street&quot; &quot;4897 Little Fleur Drive&quot; ## [7] &quot;53541 Morrow Center&quot; &quot;4819 Hermina Parkway&quot; ## [9] &quot;68096 Monument Park&quot; &quot;9595 Spaight Avenue&quot; We can extract parts of the string that match a particular pattern using str_extract(). str_extract(mockdata$email, pattern = &quot;org&quot;) ## [1] NA NA &quot;org&quot; NA NA NA &quot;org&quot; NA NA NA Before we extract, we need to know whether the string contains text that match our pattern. Use str_match() to see if the pattern is present in the string. str_match(mockdata$email, pattern = &quot;org&quot;) ## [,1] ## [1,] NA ## [2,] NA ## [3,] &quot;org&quot; ## [4,] NA ## [5,] NA ## [6,] NA ## [7,] &quot;org&quot; ## [8,] NA ## [9,] NA ## [10,] NA If we are dealing with a character vector and know that the pattern we are looking at is present in the vector, we might want to know the index of the strings in which it is present. Use str_which() to identify the index of the strings that match our pattern. str_which(mockdata$email, pattern = &quot;org&quot;) ## [1] 3 7 Another objective might be to locate the position of the pattern we are looking for in the string. For example, if we want to know the position of @ in the email ids, we can use str_locate(). str_locate(mockdata$email, pattern = &quot;@&quot;) ## start end ## [1,] 10 10 ## [2,] 13 13 ## [3,] 10 10 ## [4,] 11 11 ## [5,] 12 12 ## [6,] 10 10 ## [7,] 11 11 ## [8,] 12 12 ## [9,] 7 7 ## [10,] 12 12 The length of the string can be computed using str_length(). Let us ensure that the length of the strings in the password column is 16. str_length(mockdata$passwords) ## [1] 16 16 16 16 16 16 16 16 16 16 We can extract parts of a string by specifying the starting and ending position using str_sub(). Let us extract the currency type from the currency column. str_sub(mockdata$currency, start = 1, end = 1) ## [1] &quot;¥&quot; &quot;$&quot; &quot;\\200&quot; &quot;\\200&quot; &quot;\\200&quot; &quot;¥&quot; &quot;$&quot; &quot;¥&quot; &quot;\\200&quot; &quot;\\200&quot; One final function that we will look at before the case study is word(). It extracts word(s) from sentences. We do not have any sentences in the data set, but let us use it to extract the first and last name from the full_name column. word(mockdata$full_name, 1) ## [1] &quot;Mufi&quot; &quot;Leese&quot; &quot;Blakelee&quot; &quot;Terencio&quot; &quot;Debee&quot; &quot;Fran&quot; ## [7] &quot;Frasco&quot; &quot;Car&quot; &quot;Tades&quot; &quot;Wilton&quot; word(mockdata$full_name, 2) ## [1] &quot;Ruit&quot; &quot;Furmagier&quot; &quot;Wilshire&quot; &quot;McIllrick&quot; &quot;McErlaine&quot; &quot;Painten&quot; ## [7] &quot;Bowich&quot; &quot;Ponten&quot; &quot;Checcucci&quot; &quot;Kemmey&quot; Alright, now let us apply what we have learned so far to our case study. 8.4 Extract domain name from email ids 8.4.1 Steps split email using pattern @ extract the second element from the resulting list split the above using pattern \\\\. extract the first element from the resulting list Let us take a look at the emails before we extract the domain names. emails &lt;- mockstring %&gt;% pull(email) %&gt;% head() emails ## [1] &quot;mnewburn0@fastcompany.com&quot; &quot;mdankersley1@digg.com&quot; ## [3] &quot;hgirhard2@altervista.org&quot; &quot;pmcmenamy3@sciencedirect.com&quot; ## [5] &quot;drisbrough4@bandcamp.com&quot; &quot;cphlippi5@surveymonkey.com&quot; 8.4.1.1 Step 1: Split email using pattern @. We will split the email using str_split. It will split a string using the pattern supplied. In our case the pattern is @. str_split(emails, pattern = &#39;@&#39;) ## [[1]] ## [1] &quot;mnewburn0&quot; &quot;fastcompany.com&quot; ## ## [[2]] ## [1] &quot;mdankersley1&quot; &quot;digg.com&quot; ## ## [[3]] ## [1] &quot;hgirhard2&quot; &quot;altervista.org&quot; ## ## [[4]] ## [1] &quot;pmcmenamy3&quot; &quot;sciencedirect.com&quot; ## ## [[5]] ## [1] &quot;drisbrough4&quot; &quot;bandcamp.com&quot; ## ## [[6]] ## [1] &quot;cphlippi5&quot; &quot;surveymonkey.com&quot; 8.4.1.2 Step 2: Extract the second element from the resulting list. Step 1 returned a list. Each element of the list has two values. The first one is the username and the second is the domain name. Since we are extracting the domain name, we want the second value from each element of the list. We will use map_chr() from purrr to extract the domain names. It will return the second value from each element in the list. Since the domain name is a string, map_chr() will return a character vector. emails %&gt;% str_split(pattern = &#39;@&#39;) %&gt;% map_chr(2) ## [1] &quot;fastcompany.com&quot; &quot;digg.com&quot; &quot;altervista.org&quot; ## [4] &quot;sciencedirect.com&quot; &quot;bandcamp.com&quot; &quot;surveymonkey.com&quot; 8.4.1.3 Step 3: Split the above using pattern \\\\.. We want the domain name and not the extension. Step 2 returned a character vector and we need to split the domain name and the domain extension. They are separated by .. Since . is a special character, we will use \\\\ before . to escape it. Let us split the domain name and domain extension using str_split and \\\\. as the pattern. emails %&gt;% str_split(pattern = &#39;@&#39;) %&gt;% map_chr(2) %&gt;% str_split(pattern = &#39;\\\\.&#39;) ## [[1]] ## [1] &quot;fastcompany&quot; &quot;com&quot; ## ## [[2]] ## [1] &quot;digg&quot; &quot;com&quot; ## ## [[3]] ## [1] &quot;altervista&quot; &quot;org&quot; ## ## [[4]] ## [1] &quot;sciencedirect&quot; &quot;com&quot; ## ## [[5]] ## [1] &quot;bandcamp&quot; &quot;com&quot; ## ## [[6]] ## [1] &quot;surveymonkey&quot; &quot;com&quot; 8.4.1.4 Step 4: Extract the first element from the resulting list. Now that we have separated the domain name from its extension, let us extract the first value from each element in the list returned in step 3. We will again use map_chr to achieve this. emails %&gt;% str_split(pattern = &#39;@&#39;) %&gt;% map_chr(2) %&gt;% str_split(pattern = &#39;\\\\.&#39;) %&gt;% map_chr(extract(1)) ## [1] &quot;fastcompany&quot; &quot;digg&quot; &quot;altervista&quot; &quot;sciencedirect&quot; ## [5] &quot;bandcamp&quot; &quot;surveymonkey&quot; 8.5 Extract Domain Extension The below code extracts the domain extension instead of the domain name. emails %&gt;% str_split(pattern = &#39;@&#39;) %&gt;% map_chr(2) %&gt;% str_split(pattern = &#39;\\\\.&#39;, simplify = TRUE) %&gt;% extract(, 2) ## [1] &quot;com&quot; &quot;com&quot; &quot;org&quot; &quot;com&quot; &quot;com&quot; &quot;com&quot; 8.6 Extract image type from URL 8.6.1 Steps split imageurl using pattern \\\\. extract the third value from each element of the resulting list subset the string using the index position Let us take a look at the URL of the image. img &lt;- mockstring %&gt;% pull(imageurl) %&gt;% head() img ## [1] &quot;http://dummyimage.com/130x183.jpg/dddddd/000000&quot; ## [2] &quot;http://dummyimage.com/106x217.bmp/dddddd/000000&quot; ## [3] &quot;http://dummyimage.com/146x127.bmp/cc0000/ffffff&quot; ## [4] &quot;http://dummyimage.com/181x194.png/5fa2dd/ffffff&quot; ## [5] &quot;http://dummyimage.com/220x123.jpg/ff4444/ffffff&quot; ## [6] &quot;http://dummyimage.com/118x176.bmp/dddddd/000000&quot; 8.6.1.1 Step 1: Split imageurl using pattern \\\\. Let us split imageurl using str_split and the pattern \\\\.. str_split(img, pattern = &#39;\\\\.&#39;) ## [[1]] ## [1] &quot;http://dummyimage&quot; &quot;com/130x183&quot; &quot;jpg/dddddd/000000&quot; ## ## [[2]] ## [1] &quot;http://dummyimage&quot; &quot;com/106x217&quot; &quot;bmp/dddddd/000000&quot; ## ## [[3]] ## [1] &quot;http://dummyimage&quot; &quot;com/146x127&quot; &quot;bmp/cc0000/ffffff&quot; ## ## [[4]] ## [1] &quot;http://dummyimage&quot; &quot;com/181x194&quot; &quot;png/5fa2dd/ffffff&quot; ## ## [[5]] ## [1] &quot;http://dummyimage&quot; &quot;com/220x123&quot; &quot;jpg/ff4444/ffffff&quot; ## ## [[6]] ## [1] &quot;http://dummyimage&quot; &quot;com/118x176&quot; &quot;bmp/dddddd/000000&quot; 8.6.1.2 Step 2: Extract the third value from each element of the resulting list Step 1 returned a list the elements of which have 3 values each. If you observe the list, the image type is in the 3rd value. We will now extract the third value from each element of the list using map_chr. img %&gt;% str_split(pattern = &#39;\\\\.&#39;) %&gt;% map_chr(extract(3)) ## [1] &quot;jpg/dddddd/000000&quot; &quot;bmp/dddddd/000000&quot; &quot;bmp/cc0000/ffffff&quot; ## [4] &quot;png/5fa2dd/ffffff&quot; &quot;jpg/ff4444/ffffff&quot; &quot;bmp/dddddd/000000&quot; 8.6.1.3 Step 3: Subset the string using the index position We can now extract the image type in two ways: subset the first 3 characters of the string split the string using pattern / and extract the first value from the elements of the resulting list Below is the first method. We know that the image type is 3 characters. So we use str_sub to subset the first 3 characters. The index positions are mentioned using start and stop. img %&gt;% str_split(pattern = &#39;\\\\.&#39;) %&gt;% map_chr(extract(3)) %&gt;% str_sub(start = 1, end = 3) ## [1] &quot;jpg&quot; &quot;bmp&quot; &quot;bmp&quot; &quot;png&quot; &quot;jpg&quot; &quot;bmp&quot; In case you are not sure about the length of the image type. In such cases, we will split the string using pattern / and then use map_chr to extract the first value of each element of the resulting list. img %&gt;% str_split(pattern = &#39;\\\\.&#39;) %&gt;% map_chr(extract(3)) %&gt;% str_split(pattern = &#39;/&#39;) %&gt;% map_chr(extract(1)) ## [1] &quot;jpg&quot; &quot;bmp&quot; &quot;bmp&quot; &quot;png&quot; &quot;jpg&quot; &quot;bmp&quot; 8.7 Extract Image Dimesion from URL 8.7.1 Steps locate numbers between 0 and 9 extract part of url starting with image dimension split the string using the pattern \\\\. extract the first element 8.7.1.1 Step 1: Locate numbers between 0 and 9. Let us inspect the image url. The dimension of the image appears after the domain extension and there are no numbers in the url before. We will locate the position or index of the first number in the url using str_locate() and using the pattern [0-9] which instructs to look for any number between and including 0 and 9. str_locate(img, pattern = &quot;[0-9]&quot;) ## start end ## [1,] 23 23 ## [2,] 23 23 ## [3,] 23 23 ## [4,] 23 23 ## [5,] 23 23 ## [6,] 23 23 8.7.1.2 Step 2: Extract url We know where the dimension is located in the url. Let us extract the part of the url that contains the image dimension using str_sub(). str_sub(img, start = 23) ## [1] &quot;130x183.jpg/dddddd/000000&quot; &quot;106x217.bmp/dddddd/000000&quot; ## [3] &quot;146x127.bmp/cc0000/ffffff&quot; &quot;181x194.png/5fa2dd/ffffff&quot; ## [5] &quot;220x123.jpg/ff4444/ffffff&quot; &quot;118x176.bmp/dddddd/000000&quot; 8.7.1.3 Step 3: Split the string using the pattern \\\\.. From the previous step, we have the part of the url that contains the image dimension. To extract the dimension, we will split it from the rest of the url using str_split() and using the pattern \\\\. as it separates the dimension and the image extension. img %&gt;% str_sub(start = 23) %&gt;% str_split(pattern = &#39;\\\\.&#39;) ## [[1]] ## [1] &quot;130x183&quot; &quot;jpg/dddddd/000000&quot; ## ## [[2]] ## [1] &quot;106x217&quot; &quot;bmp/dddddd/000000&quot; ## ## [[3]] ## [1] &quot;146x127&quot; &quot;bmp/cc0000/ffffff&quot; ## ## [[4]] ## [1] &quot;181x194&quot; &quot;png/5fa2dd/ffffff&quot; ## ## [[5]] ## [1] &quot;220x123&quot; &quot;jpg/ff4444/ffffff&quot; ## ## [[6]] ## [1] &quot;118x176&quot; &quot;bmp/dddddd/000000&quot; 8.7.1.4 Step 4: Extract the first element. The above step resulted in a list which contains the image dimension and the rest of the url. Each element of the list is a character vector. We want to extract the first value in the character vector. Let us use map_chr() to extract the first value from each element of the list. img %&gt;% str_sub(start = 23) %&gt;% str_split(pattern = &#39;\\\\.&#39;) %&gt;% map_chr(extract(1)) ## [1] &quot;130x183&quot; &quot;106x217&quot; &quot;146x127&quot; &quot;181x194&quot; &quot;220x123&quot; &quot;118x176&quot; 8.8 Extract HTTP Protocol from URL url1 &lt;- mockstring %&gt;% pull(url) %&gt;% first() url1 ## [1] &quot;https://engadget.com/nascetur/ridiculus/mus/vivamus/vestibulum.jsp?eu=est&amp;tincidunt=risus&amp;in=auctor&amp;leo=sed&amp;maecenas=tristique&amp;pulvinar=in&amp;lobortis=tempus&amp;est=sit&amp;phasellus=amet&amp;sit=sem&amp;amet=fusce&amp;erat=consequat&amp;nulla=nulla&amp;tempus=nisl&amp;vivamus=nunc&amp;in=nisl&amp;felis=duis&amp;eu=bibendum&amp;sapien=felis&amp;cursus=sed&amp;vestibulum=interdum&amp;proin=venenatis&amp;eu=turpis&amp;mi=enim&amp;nulla=blandit&amp;ac=mi&amp;enim=in&amp;in=porttitor&amp;tempor=pede&amp;turpis=justo&amp;nec=eu&amp;euismod=massa&amp;scelerisque=donec&amp;quam=dapibus&amp;turpis=duis&amp;adipiscing=at&amp;lorem=velit&amp;vitae=eu&amp;mattis=est&amp;nibh=congue&amp;ligula=elementum&amp;nec=in&amp;sem=hac&amp;duis=habitasse&amp;aliquam=platea&amp;convallis=dictumst&amp;nunc=morbi&amp;proin=vestibulum&amp;at=velit&amp;turpis=id&amp;a=pretium&amp;pede=iaculis&amp;posuere=diam&amp;nonummy=erat&amp;integer=fermentum&amp;non=justo&amp;velit=nec&amp;donec=condimentum&amp;diam=neque&amp;neque=sapien&amp;vestibulum=placerat&amp;eget=ante&amp;vulputate=nulla&amp;ut=justo&amp;ultrices=aliquam&amp;vel=quis&amp;augue=turpis&amp;vestibulum=eget&amp;ante=elit&amp;ipsum=sodales&amp;primis=scelerisque&amp;in=mauris&amp;faucibus=sit&amp;orci=amet&amp;luctus=eros&amp;et=suspendisse&amp;ultrices=accumsan&amp;posuere=tortor&amp;cubilia=quis&amp;curae=turpis&amp;donec=sed&amp;pharetra=ante&amp;magna=vivamus&amp;vestibulum=tortor&amp;aliquet=duis&amp;ultrices=mattis&amp;erat=egestas&amp;tortor=metus&amp;sollicitudin=aenean&amp;mi=fermentum&amp;sit=donec&quot; 8.8.1 Steps split the url using the pattern :// extract the first element 8.8.1.1 Step 1: Split the url using the pattern ://. The HTTP protocol is the first part of the url and is separated from the rest of the url by :. Let us split the url using str_split() and using the pattern :. Since : is a special character, we will escape it using \\\\. str_split(url1, pattern = &#39;://&#39;) ## [[1]] ## [1] &quot;https&quot; ## [2] &quot;engadget.com/nascetur/ridiculus/mus/vivamus/vestibulum.jsp?eu=est&amp;tincidunt=risus&amp;in=auctor&amp;leo=sed&amp;maecenas=tristique&amp;pulvinar=in&amp;lobortis=tempus&amp;est=sit&amp;phasellus=amet&amp;sit=sem&amp;amet=fusce&amp;erat=consequat&amp;nulla=nulla&amp;tempus=nisl&amp;vivamus=nunc&amp;in=nisl&amp;felis=duis&amp;eu=bibendum&amp;sapien=felis&amp;cursus=sed&amp;vestibulum=interdum&amp;proin=venenatis&amp;eu=turpis&amp;mi=enim&amp;nulla=blandit&amp;ac=mi&amp;enim=in&amp;in=porttitor&amp;tempor=pede&amp;turpis=justo&amp;nec=eu&amp;euismod=massa&amp;scelerisque=donec&amp;quam=dapibus&amp;turpis=duis&amp;adipiscing=at&amp;lorem=velit&amp;vitae=eu&amp;mattis=est&amp;nibh=congue&amp;ligula=elementum&amp;nec=in&amp;sem=hac&amp;duis=habitasse&amp;aliquam=platea&amp;convallis=dictumst&amp;nunc=morbi&amp;proin=vestibulum&amp;at=velit&amp;turpis=id&amp;a=pretium&amp;pede=iaculis&amp;posuere=diam&amp;nonummy=erat&amp;integer=fermentum&amp;non=justo&amp;velit=nec&amp;donec=condimentum&amp;diam=neque&amp;neque=sapien&amp;vestibulum=placerat&amp;eget=ante&amp;vulputate=nulla&amp;ut=justo&amp;ultrices=aliquam&amp;vel=quis&amp;augue=turpis&amp;vestibulum=eget&amp;ante=elit&amp;ipsum=sodales&amp;primis=scelerisque&amp;in=mauris&amp;faucibus=sit&amp;orci=amet&amp;luctus=eros&amp;et=suspendisse&amp;ultrices=accumsan&amp;posuere=tortor&amp;cubilia=quis&amp;curae=turpis&amp;donec=sed&amp;pharetra=ante&amp;magna=vivamus&amp;vestibulum=tortor&amp;aliquet=duis&amp;ultrices=mattis&amp;erat=egestas&amp;tortor=metus&amp;sollicitudin=aenean&amp;mi=fermentum&amp;sit=donec&quot; 8.8.1.2 Step 2: Extract the first element. The HTTP protocol is the first value in each element of the list. As we did in the previous example, we will extact it using map_chr() and extract(). url1 %&gt;% str_split(pattern = &#39;://&#39;) %&gt;% map_chr(extract(1)) ## [1] &quot;https&quot; 8.9 Extract file type urls &lt;- mockstring %&gt;% use_series(url) %&gt;% extract(1:3) 8.9.1 Steps check if there are only 2 dots in the URL check if there is only 1 question mark in the URL detect the staritng position of file type tetect the ending position of file type use the locations to specify the index position for extracting file type 8.9.1.1 Step 1: Check if there are only 2 dots in the URL Let us locate all the dots in the url using str_locate_all() and see if any of them contain more than 2 dots. urls %&gt;% str_locate_all(pattern = &#39;\\\\.&#39;) %&gt;% map_int(nrow) %&gt;% is_greater_than(2) %&gt;% sum() ## [1] 0 8.9.1.2 Step 2: Check if there is only 1 question mark in the URL The next step is to check if there is only one ? (question mark) in the url. urls %&gt;% str_locate_all(pattern = &quot;[?]&quot;) %&gt;% map_int(nrow) %&gt;% is_greater_than(1) %&gt;% sum() ## [1] 0 8.9.1.3 Step 3: Detect the staritng position of file type Since the file type is located between the second dot and the first quesiton mark in the url, let us extract the location of the second dot and add 1 as the file type starts after the dot. d &lt;- urls %&gt;% str_locate_all(pattern = &#39;\\\\.&#39;) %&gt;% map_int(extract(2)) %&gt;% add(1) d ## [1] 64 47 48 8.9.1.4 Step 4: Detect the ending position of file type In step 2, we confirmed that the url has only one question mark. Let us locate the question mark in the url and subtract 1 (as the file type ends before the question mark) so that we get the ending chapterion of the file type. . q &lt;- urls %&gt;% str_locate_all(pattern = &quot;[?]&quot;) %&gt;% map_int(extract(1)) %&gt;% subtract(1) q ## [1] 66 50 51 8.9.1.5 Step 5: Specify the index position for extracting file type From steps 3 and 4, we have the location of the second dot and the first question mark in the url. Let us use them with str_sub() to extract the file type. str_sub(urls, start = d, end = q) ## [1] &quot;jsp&quot; &quot;json&quot; &quot;json&quot; "],
["lubridate.html", "Chapter 9 Working with Date &amp; Time 9.1 Introduction 9.2 Case Study 9.3 Date &amp; Time Classes 9.4 Date Arithmetic 9.5 Time Zones 9.6 Date &amp; Time Formats 9.7 Parse Date &amp; Time 9.8 Date &amp; Time Components 9.9 Create, Update &amp; Verify 9.10 Intervals, Duration &amp; Period 9.11 Others", " Chapter 9 Working with Date &amp; Time 9.1 Introduction Let us begin by looking at the current date and time. 9.1.1 Date Sys.Date() and today() will return the current date. Sys.Date() ## [1] &quot;2020-06-20&quot; lubridate::today() ## [1] &quot;2020-06-20&quot; 9.1.2 Time Sys.time() and now() return the date, time and the timezone. In now(), we can specify the timezone using the tzone argument. Sys.time() ## [1] &quot;2020-06-20 15:47:52 IST&quot; lubridate::now() ## [1] &quot;2020-06-20 15:47:52 IST&quot; lubridate::now(tzone = &quot;UTC&quot;) ## [1] &quot;2020-06-20 10:17:52 UTC&quot; 9.1.3 AM or PM? am() and pm() allow us to check whether date/time occur in the am or pm? They return a logical value i.e. TRUE or FALSE lubridate::am(now()) ## [1] FALSE lubridate::pm(now()) ## [1] TRUE 9.1.4 Leap Year We can also check if the current year is a leap year using leap_year(). lubridate::leap_year(Sys.Date()) ## [1] TRUE 9.1.5 Summary Function Description Sys.Date() Current Date lubridate::today() Current Date Sys.time() Current Time lubridate::now() Current Time lubridate::am() Whether time occurs in am? lubridate::pm() Whether time occurs in pm? lubridate::leap_year() Check if the year is a leap year? 9.1.6 Your Turn get current date get current time check whether the time occurs in am or pm? check whether the following years were leap years 2018 2016 9.2 Case Study Throughout the tutorial, we will work on a case study related to transactions of a imaginary company. The data set includes information about invoice and payment dates. 9.2.1 Data transact &lt;- readr::read_csv(&#39;https://raw.githubusercontent.com/rsquaredacademy/datasets/master/transact.csv&#39;) ## # A tibble: 2,466 x 3 ## Invoice Due Payment ## &lt;date&gt; &lt;date&gt; &lt;date&gt; ## 1 2013-01-02 2013-02-01 2013-01-15 ## 2 2013-01-26 2013-02-25 2013-03-03 ## 3 2013-07-03 2013-08-02 2013-07-08 ## 4 2013-02-10 2013-03-12 2013-03-17 ## 5 2012-10-25 2012-11-24 2012-11-28 ## 6 2012-01-27 2012-02-26 2012-02-22 ## 7 2013-08-13 2013-09-12 2013-09-09 ## 8 2012-12-16 2013-01-15 2013-01-12 ## 9 2012-05-14 2012-06-13 2012-07-01 ## 10 2013-07-01 2013-07-31 2013-07-26 ## # ... with 2,456 more rows 9.2.2 Data Dictionary The data set has 3 columns. All the dates are in the format (yyyy-mm-dd). Column Description Invoice Invoice Date Due Due Date Payment Payment Date In the case study, we will try to answer a few questions we have about the transact data. extract date, month and year from Due compute the number of days to settle invoice compute days over due check if due year is a leap year check when due day in february is 29, whether it is a leap year how many invoices were settled within due date how many invoices are due in each quarter 9.3 Date &amp; Time Classes 9.3.1 Introduction In this section, we will look at two things. First, how to create date/time data in R, and second, how to convert other data types to date/time. Let us begin by creating the latest R release date manually. release_date &lt;- 2019-12-12 release_date ## [1] 1995 Okay! Why do we see 1995 when we call the date? What is happening here? Let us quickly check the data type of release_date. class(release_date) ## [1] &quot;numeric&quot; The data type is numeric i.e. R has subtracted 12 twice from 2019 to return 1995. Clearly, the above method is not the right way to store date/time. Let us see if we can get some hints from the builtin R functions we used in the previous section. If you observe the output, all of them returned date/time wrapped in quotes. Hmmm… let us wrap our date in quotes and see what happens. release_date &lt;- &quot;2019-12-12&quot; release_date ## [1] &quot;2019-12-12&quot; Alright, now R does not do any arithmetic and returns the date as we specified. Great! Is this the right format to store date/time? No. Why? What is the problem if date/time is saved as character/string? The problem is the nature or type of operations done on date or time is different when compared to string/character, number or logical values. how do we add/subtract dates? how do we extract components such as year, month, day etc. To answer the above questions, we will first check the data type of Sys.Date() and now(). class(Sys.Date()) ## [1] &quot;Date&quot; class(lubridate::now()) ## [1] &quot;POSIXct&quot; &quot;POSIXt&quot; class(release_date) ## [1] &quot;character&quot; As you can see from the above output, there are 3 different classes for storing date/time in R Date POSIXct POSIXlt Let us explore each of the above classes one by one. 9.3.2 Date 9.3.2.1 Introduction The Date class represents calendar dates. Let us go back to Sys.Date(). If you check the class of Sys.Date(), it is Date. Internally, this date is a number i.e. an integer. The unclass() function will show dates are stored internally. unclass(Sys.Date()) ## [1] 18433 What does this integer represent? Why has R stored the date as an integer? Before we answer this question, we need to know something else. In R, dates are represented as the number of days since 1970-01-01. All the dates in R are internally stored in this way. Before we explore this concept further, let us learn to create Date objects in R. We will continue to use the latest R release date, 2019-12-12. Until now, we have stored the above date as character/string but now we will use as.Date() to save it as a Date object. as.Date() is the easiest and simplest way to create dates in R. release_date &lt;- as.Date(&quot;2019-12-12&quot;) release_date ## [1] &quot;2019-12-12&quot; The as_date() function from the lubridate package is similar to as.Date(). release_date &lt;- lubridate::as_date(&quot;2019-12-12&quot;) release_date ## [1] &quot;2019-12-12&quot; If you look at the difference between release_date and 1970-01-01, it will be the same as unclass(release_date). release_date - as.Date(&quot;1970-01-01&quot;) ## Time difference of 18242 days unclass(release_date) ## [1] 18242 Let us come back to 1970-01-01 i.e. the origin for dates in R. lubridate::origin ## [1] &quot;1970-01-01 UTC&quot; From the previous examples, we know that dates are internally stored as number of days since 1970-01-01. How about dates older than the origin? How are they stored? Let us look at that briefly. unclass(as.Date(&quot;1963-08-28&quot;)) ## [1] -2318 Dates older than the origin are stored as negative integers. For those who are not aware, Martin Luther King, Jr. delivered his famous I Have a Dream speech on 1963-08-28. Let us move on and learn how to convert numbers into dates. 9.3.2.2 Convert Numeric The as.Date() function can be used to convert any of the following to a Date object character/string number factor (categorical/qualitative) We have explored how to convert strings to date. How about converting numbers to date? Sure, we can create date from numbers by specifying the origin and number of days since it. as.Date(18242, origin = &quot;1970-01-01&quot;) ## [1] &quot;2019-12-12&quot; The origin can be changed to another date (while changing the number as well.) as.Date(7285, origin = &quot;2000-01-01&quot;) ## [1] &quot;2019-12-12&quot; 9.3.3 ISO 8601 If you have carefully observed, the format in which we have been specifying the dates as well as of those returned by functions such as Sys.Date() or Sys.time() is the same i.e. YYYY-MM-DD. It includes the year including the century the month the date The month and date separated by -. This default format used in R is the ISO 8601 standard for date/time. ISO 8601 is the internationally accepted way to represent dates and times and uses the 24 hour clock system. Let us create the release date using another function ISOdate(). ISOdate(year = 2019, month = 12, day = 12, hour = 8, min = 5, sec = 3, tz = &quot;UTC&quot;) ## [1] &quot;2019-12-12 08:05:03 UTC&quot; We will look at all the different weird ways in which date/time are specified in the real world in the Date &amp; Time Formats section. For the time being, let us continue exploring date/time classes in R. The next class we are going to look at is POSIXct/POSIXlt. 9.3.4 POSIX You might be wondering what is this POSIX thing? POSIX stands for Portable Operating System Interface. It is a family of standards specified f or maintaining compatibility between different operating systems. Before we learn to create POSIX objects, let us look at now() from lubridate. class(lubridate::now()) ## [1] &quot;POSIXct&quot; &quot;POSIXt&quot; now() returns current date/time as a POSIXct object. Let us look at its internal representation using unclass() unclass(lubridate::now()) ## [1] 1592648277 ## attr(,&quot;tzone&quot;) ## [1] &quot;&quot; The output you see is the number of seconds since January 1, 1970. 9.3.4.1 POSIXct POSIXct represents the number of seconds since the beginning of 1970 (UTC) and ct stands for calendar time. To store date/time as POSIXct objects, use as.POSIXct(). Let us now store the latest R release date as POSIXct as shown below release_date &lt;- as.POSIXct(&quot;2019-12-12 08:05:03&quot;) class(release_date) ## [1] &quot;POSIXct&quot; &quot;POSIXt&quot; unclass(release_date) ## [1] 1576118103 ## attr(,&quot;tzone&quot;) ## [1] &quot;&quot; 9.3.4.2 POSIXlt POSIXlt represents the following information in a list seconds minutes hour day of the month month year day of week day of year daylight saving time flag time zone offset in seconds from GMT The lt in POSIXlt stands for local time. Use as.POSIXlt() to store date/time as POSIXlt objects. Let us store the release date as a POSIXlt object as shown below release_date &lt;- as.POSIXlt(&quot;2019-12-12 08:05:03&quot;) release_date ## [1] &quot;2019-12-12 08:05:03 IST&quot; As we said earlier, POSIXlt stores date/time components in a list and these can be extracted. Let us look at the date/time components returned by POSIXlt using unclass(). release_date &lt;- as.POSIXlt(&quot;2019-12-12 08:05:03&quot;) unclass(release_date) ## $sec ## [1] 3 ## ## $min ## [1] 5 ## ## $hour ## [1] 8 ## ## $mday ## [1] 12 ## ## $mon ## [1] 11 ## ## $year ## [1] 119 ## ## $wday ## [1] 4 ## ## $yday ## [1] 345 ## ## $isdst ## [1] 0 ## ## $zone ## [1] &quot;IST&quot; ## ## $gmtoff ## [1] NA Use unlist() if you want the components returned as a vector. release_date &lt;- as.POSIXlt(&quot;2019-12-12 08:05:03&quot;) unlist(release_date) ## sec min hour mday mon year wday yday isdst zone gmtoff ## &quot;3&quot; &quot;5&quot; &quot;8&quot; &quot;12&quot; &quot;11&quot; &quot;119&quot; &quot;4&quot; &quot;345&quot; &quot;0&quot; &quot;IST&quot; NA To extract specific components, use $. release_date &lt;- as.POSIXlt(&quot;2019-12-12 08:05:03&quot;) release_date$hour ## [1] 8 release_date$mon ## [1] 11 release_date$zone ## [1] &quot;IST&quot; Now, let us look at the components returned by POSIXlt. Some of them are intuitive Component Description sec Second min Minute hour Hour of the day mon Month of the year (0-11 zone Timezone wday Day of week mday Day of month year Years since 1900 yday Day of year isdst Daylight saving flag gmtoff Offset is seconds from GMT Great! We will end this section with a few tips/suggestions on when to use Date or POSIXct/POSIXlt. use Date when there is no time component use POSIX when dealing with time and timezones use POSIXlt when you want to access/extract the different components 9.3.5 Your Turn R 1.0.0 was released on 2000-02-29 08:55:23 UTC. Save it as Date using character Date using origin and number POSIXct POSIXlt and extract month day day of year month zone ISODate 9.4 Date Arithmetic 9.4.1 Introduction Time to do some arithmetic with the dates. Let us calculate the length of a course you have enrolled for (Become a Rock Star Data Scientist in 10 Days) by subtracting the course start date from the course end date. course_start &lt;- as_date(&#39;2017-04-12&#39;) course_end &lt;- as_date(&#39;2017-04-21&#39;) course_duration &lt;- course_end - course_start course_duration ## Time difference of 9 days 9.4.2 Shift Date Time to shift the course dates. We can shift a date by days, weeks or months. Let us shift the course start date by: 2 days 3 weeks 1 year course_start + days(2) ## [1] &quot;2017-04-14&quot; course_start + weeks(3) ## [1] &quot;2017-05-03&quot; course_start + years(1) ## [1] &quot;2018-04-12&quot; 9.4.3 Case Study 9.4.3.1 Compute days to settle invoice Let us estimate the number of days to settle the invoice by subtracting the date of invoice from the date of payment. transact %&gt;% mutate( days_to_pay = Payment - Invoice ) ## # A tibble: 2,466 x 4 ## Invoice Due Payment days_to_pay ## &lt;date&gt; &lt;date&gt; &lt;date&gt; &lt;drtn&gt; ## 1 2013-01-02 2013-02-01 2013-01-15 13 days ## 2 2013-01-26 2013-02-25 2013-03-03 36 days ## 3 2013-07-03 2013-08-02 2013-07-08 5 days ## 4 2013-02-10 2013-03-12 2013-03-17 35 days ## 5 2012-10-25 2012-11-24 2012-11-28 34 days ## 6 2012-01-27 2012-02-26 2012-02-22 26 days ## 7 2013-08-13 2013-09-12 2013-09-09 27 days ## 8 2012-12-16 2013-01-15 2013-01-12 27 days ## 9 2012-05-14 2012-06-13 2012-07-01 48 days ## 10 2013-07-01 2013-07-31 2013-07-26 25 days ## # ... with 2,456 more rows 9.4.3.2 Compute days over due How many of the invoices were settled post the due date? We can find this by: subtracting the due date from the payment date counting the number of rows where delay &lt; 0 transact %&gt;% mutate( delay = Due - Payment ) %&gt;% filter(delay &lt; 0) %&gt;% mutate( delay = delay * -1 ) %&gt;% count(delay) ## # A tibble: 36 x 2 ## delay n ## * &lt;drtn&gt; &lt;int&gt; ## 1 1 days 61 ## 2 2 days 65 ## 3 3 days 51 ## 4 4 days 62 ## 5 5 days 69 ## 6 6 days 56 ## 7 7 days 55 ## 8 8 days 49 ## 9 9 days 38 ## 10 10 days 33 ## # ... with 26 more rows 9.4.4 Your Turn compute the length of a vacation which begins on 2020-04-19 and ends on 2020-04-25 recompute the length of the vacation after shifting the vacation start and end date by 10 days and 2 weeks compute the days to settle invoice and days overdue from the receivables.csv data set compute the length of employment (only for those employees who have been terminated) from the hr-data.csv data set (use date of hire and termination) 9.5 Time Zones 9.5.1 Introduction In the previous section, POSIXlt stored date/time components as a list. Among the different components it returned were gmtoff zone gmtoff is offset in seconds from GMT i.e. difference in hours and minutes from UTC. Wait.. What do UTC and GMT stand for? Coordinated Universal Time (UTC) Greenwich Meridian Time (GMT) Since we are talking about UTC, GMT etc., let us spend a little time on understanding the basics of time zones and daylight savings. 9.5.2 Time Zones Timezones exist because different parts of the Earth receive sun light at different times. If there was a single timezone, noon or morning would mean different things in different parts of the world. The timezones are based on Earth’s rotation. The Earth moves ~15 degrees every 60 minutes i.e. 360 degrees in 24 hours. The planet is divided into 24 timezones each 15 degrees of longitude width. Now, you have heard of Greenwich Meridian Time (GMT) right? We just saw GMT off set in POSIXlt and you would have come across it in other time formats as well. For example, India timezone is given as GMT +5:30. Let us explore GMT in a little more detail. Greenwich is a suburb of London and the time at Greenwich is Greenwich Mean Time. As you move West from Greenwich, every 15 degree section is one hour earlier than GMT and every 15 degree section to the East is an hour later. Alright! What is UTC then? Coordinated Universal Time (UTC) , on the other hand, is the time standard commonly used across the world. Even though they share the same current time, GMT is a timezone while UTC is a time standard. So how do we check the timezone in R? When you run Sys.timezone(), you should be able to see the timezone you are in. Sys.timezone() ## [1] &quot;Asia/Calcutta&quot; If you do not see the timezone, use Sys.getenv() to get the value of the TZ environment variable. Sys.getenv(&quot;TZ&quot;) ## [1] &quot;&quot; If nothing is returned, it means we have to set the timezone. Use Sys.setenv() to set the timezone as shown below. The author resides in India and hence the timezone is set to Asia/Calcutta. You need to set the timezone in which you reside or work. Sys.setenv(TZ = &quot;Asia/Calcutta&quot;) Another way to get the timezone is through tz() from the lubridate package. lubridate::tz(Sys.time()) ## [1] &quot;&quot; If you want to view the time in a different timezone, use with_tz(). Let us look at the current time in UTC instead of Indian Standard Time. lubridate::with_tz(Sys.time(), &quot;UTC&quot;) ## [1] &quot;2020-06-20 10:18:00 UTC&quot; 9.5.3 Daylight Savings Daylight savings also known as daylight saving time daylight savings time daylight time summer time is the practice of advancing clocks during summer months so that darkness falls later each day according to the clock. In other words advance clock by one hour in spring (spring forward) retard clocks by one hour in autumn (fall back) In R, the dst() function is an indicator for daylight savings. It returns TRUE if daylight saving is in force, FALSE if not and NA if unknown. dst(Sys.Date()) ## [1] FALSE 9.5.4 Your Turn check the timezone you live in check if daylight savings in on check the current time in UTC or a different time zone 9.6 Date &amp; Time Formats 9.6.1 Introduction After the timezones and daylight savings detour, let us get back on path and explore another important aspect, date &amp; time formats. Although it is a good practice to adher to ISO 8601 format, not all date/time data will comply with it. In real world, date/time data may come in all types of weird formats. Below is a sample Format December 12, 2019 12th Dec, 2019 Dec 12th, 19 12-Dec-19 2019 December 12.12.19 When the data is not in the default ISO 8601 format, we need to explicitly specify the format in R. We do this using conversion specifications. A conversion specification is introduced by %, usually followed by a single letter or O or E and then a single letter. 9.6.2 Conversion Specifications Specification Description Example %d Day of the month (decimal number) 12 %m Month (decimal number) 12 %b Month (abbreviated) Dec %B Month (full name) December %y Year (2 digit) 19 %Y Year (4 digit) 2019 %H Hour 8 %M Minute 5 %S Second 3 Time to work through a few examples. Let us say you are dealing with dates in the format 19/12/12. In this format, the year comes first followed by month and the date; each separated by a backslash (/). The year consists of only 2 digits i.e. it does not include the century. Let us now map each component of the date to the format table shown at the beginning. Date Specification 19 %y 12 %m 12 %d Using the format argument, we will specify the date format as a character vector i.e. enclosed in quotes. as.Date(&quot;19/12/12&quot;, format = &quot;%y/%m/%d&quot;) ## [1] &quot;2019-12-12&quot; Another way in which the release data can be written is 2019-Dec-12. We still have the year followed by the month and the date but there are a few changes here: the components are separated by a - instead of / year has 4 digits i.e. includes the century the month is specified using abbreviation instead of digits Let us map the components to the format table: Date Specification 2019 %Y Dec %b 12 %d Let us specify the format for the date using the above mapping. as.Date(&quot;2019-Dec-12&quot;, format = &quot;%Y-%b-%d&quot;) ## [1] &quot;2019-12-12&quot; In both the above examples, we have not dealt with time components. Let us include the time of the latest R release in the next one i.e. 19/12/12 08:05:03. Date Specification 19 %y 12 %m 12 %d 08 %H 05 %M 03 %S Since we are dealing with time, we will use as.POSIXct() instead of as.Date(). as.POSIXct(&quot;19/12/12 08:05:03&quot;, tz = &quot;UTC&quot;, format = &quot;%y/%m/%d %H:%M:%S&quot;) ## [1] &quot;2019-12-12 08:05:03 UTC&quot; In the below table, we look at some of the most widely used conversion specifications. You can learn more about these specifications by running ?strptime or help(strptime). Specification Description %a Abbreviated weekday %A Full weekday %C Century (00-99) %D Same as %m/%d/%y %e Day of month [1 - 31] %F Same as %Y-%m-%d %h Same as %b %I Hours as decimal [01 - 12] %j Day of year [001 - 366] %R Same as %H:%M %t Tab %T Same as %H:%M:%S %u Weekday 1 - 7 %U Week of year [00 - 53] %V Week of year [01 - 53] %w Weekday 0 - 6 %W Week of year [00 - 53] We have included a lot of practice questions for you to explore the different date/time formats. The solutions are available in the Learning Management system as well as in our GitHub repo. Try them and let us know if you have any doubts. 9.6.3 Guess Format guess_formats() from lubridate is a very useful function. It will guess the date/time format if you specify the order in which year, month, date, hour, minute and second appear. release_date_formats &lt;- c(&quot;December 12th 2019&quot;, &quot;Dec 12th 19&quot;, &quot;dec 12 2019&quot;) guess_formats(release_date_formats, orders = &quot;mdy&quot;, print_matches = TRUE) ## Omdy mdy ## [1,] &quot;December 12th 2019&quot; &quot;%Om %dth %Y&quot; &quot;%B %dth %Y&quot; ## [2,] &quot;Dec 12th 19&quot; &quot;%Om %dth %y&quot; &quot;%b %dth %y&quot; ## [3,] &quot;dec 12 2019&quot; &quot;%Om %d %Y&quot; &quot;%b %d %Y&quot; ## Omdy Omdy Omdy mdy mdy ## &quot;%Om %dth %Y&quot; &quot;%Om %dth %y&quot; &quot;%Om %d %Y&quot; &quot;%B %dth %Y&quot; &quot;%b %dth %y&quot; ## mdy ## &quot;%b %d %Y&quot; 9.6.4 Your Turn Below, we have specified July 5th, 2019 in different ways. Create the date using as.Date() while specifying the correct format for each of them. July-05-19 JUL-05-19 05.07.19 5-July 2019 July 5th, 2019 July 05, 2019 2019-July- 05 05/07/2019 07/05/2019 7/5/2019 07/5/19 2019-07-05 9.7 Parse Date &amp; Time While creating date-time objects, we specified different formats using the conversion specification but most often you will not create date/time and instead deal with data thay comes your way from a system or colleague/collaborator. In such cases, we need to be able to parse date/time from the data provided to us. In this section, we will focus on parsing date/time from character data. Both base R and the lubridate package offer functions to parse date and time and we will explore a few of them in this section. We will initially use functions from base R and later on explore those from lubridate which will give us an opportunity to compare and contrast. It will also allow us to choose the functions based on the data we are dealing with. strptime() will convert character data to POSIXlt. You will use this when converting from character data to date/time. On the other hand, if you want to convert date/time to character data, use any of the following: strftime() format() as.character() The above functions will convert POSIXct/POSIXlt to character. Let us start with a simple example. The data we have been supplied has date/time as character data and in the format YYYYMMDD i.e. nothing separates the year, month and date from each other. We will use strptime() to convert this to an object of class POSIXlt. rel_date &lt;- strptime(&quot;20191212&quot;, format = &quot;%Y%m%d&quot;) class(rel_date) ## [1] &quot;POSIXlt&quot; &quot;POSIXt&quot; If you have a basic knowledge of conversion specifications, you can use strptime() to convert character data to POSIXlt. Let us quickly explore the functions to convert date/time to character data before moving on to the functions from lubridate. rel_date_strf &lt;- strftime(rel_date) class(rel_date_strf) ## [1] &quot;character&quot; rel_date_format &lt;- format(rel_date) class(rel_date_format) ## [1] &quot;character&quot; rel_date_char &lt;- as.character(rel_date) class(rel_date_char) ## [1] &quot;character&quot; As you can see, all the 3 functions converted date/time to character. Time to move on and explore the lubridate package. We will start with an example in which the release date is formatted in 3 different ways but they have one thing in common i.e. the order in which the components appear. In all the 3 formats, the year is followed by the month and then the date. To parse the release date, we will use parse_date_time() from lubridate which parses the input into POSIXct objects. release_date &lt;- c(&quot;19-12-12&quot;, &quot;20191212&quot;, &quot;19-12 12&quot;) parse_date_time(release_date, &quot;ymd&quot;) ## [1] &quot;2019-12-12 UTC&quot; &quot;2019-12-12 UTC&quot; &quot;2019-12-12 UTC&quot; parse_date_time(release_date, &quot;y m d&quot;) ## [1] &quot;2019-12-12 UTC&quot; &quot;2019-12-12 UTC&quot; &quot;2019-12-12 UTC&quot; parse_date_time(release_date, &quot;%y%m%d&quot;) ## [1] &quot;2019-12-12 UTC&quot; &quot;2019-12-12 UTC&quot; &quot;2019-12-12 UTC&quot; Try to use strptime() in the above example and see what happens. Now, let us look at another data set. release_date &lt;- c(&quot;19-07-05&quot;, &quot;2019-07-05&quot;, &quot;05-07-2019&quot;, &quot;07-05-2019&quot;) What happens in the below case? The same date appears in multiple formats. How do we parse them? parse_date_time() allows us to specify mutiple date-time formats. Let us first map the dates to their formats. Date Specification 19-07-05 ymd 2019-07-05 ymd 05-07-2019 dmy 07-05-2019 mdy The above specifications can be supplied as a character vector. parse_date_time(release_date, c(&quot;ymd&quot;, &quot;ymd&quot;, &quot;dmy&quot;, &quot;mdy&quot;)) ## [1] &quot;2019-07-05 UTC&quot; &quot;2019-07-05 UTC&quot; &quot;2019-07-05 UTC&quot; &quot;2019-05-07 UTC&quot; Great! We have used both strptime() and parse_date_time() now. Can you tell what differentiates parse_date_time() when compared to strptime()? We summarize it in the points below: no need to include % prefix or separator specify several date/time formats There are other helper functions that can be used to parse dates with year, month, day components parse dates with year, month, day, hour, minute, seconds components parse period with hour, minute, second components and are explored in the below examples. # year/month/date ymd(&quot;2019-12-12&quot;) ## [1] &quot;2019-12-12&quot; # year/month/date ymd(&quot;19/12/12&quot;) ## [1] &quot;2019-12-12&quot; # date/month/year dmy(121219) ## [1] &quot;2019-12-12&quot; # year/month/date/hour/minute/second ymd_hms(191212080503) ## [1] &quot;2019-12-12 08:05:03 UTC&quot; # hour/minute/second hms(&quot;8, 5, 3&quot;) ## [1] &quot;8H 5M 3S&quot; # hour/minute/second hms(&quot;08:05:03&quot;) ## [1] &quot;8H 5M 3S&quot; # minute/second ms(&quot;5,3&quot;) ## [1] &quot;5M 3S&quot; # hour/minute hm(&quot;8, 5&quot;) ## [1] &quot;8H 5M 0S&quot; Note, in a couple of cases where the components are not separated by /, - or space, we have not enclosed the values in quotes. 9.7.1 Your Turn Below, we have specified July 5th, 2019 in different ways. Parse the dates using strptime() or parse_date_time() or any other helper function. July-05-19 JUL-05-19 05.07.19 5-July 2019 July 5th, 2019 July 05, 2019 2019-July- 05 05/07/2019 07/05/2019 7/5/2019 07/5/19 2019-07-05 9.8 Date &amp; Time Components In the second section, we discussed the downside of saving date/time as character/string in R. One of the points we discussed was that we can’t extract components such as year, month, day etc. In this section, we will learn to extract date/time components such as year month date week day quarter semester hour minute second timezone The below table outlines the functions we will explore in the first part of this section. Function Description year() Get year month() Get month (number) month(label = TRUE) Get month (abbreviated name) month(abbr = FALSE) Get month (full name) months() Get month week() Get week 9.8.1 Year release_date &lt;- ymd_hms(&quot;2019-12-12 08:05:03&quot;) year(release_date) ## [1] 2019 9.8.2 Month month() will return the month as a number i.e. 07 for July. month(release_date) ## [1] 12 If you want the name of the month instead, use the label argument and set it to TRUE. Now it returns Jul instead of 07. month(release_date, label = TRUE) ## [1] Dec ## 12 Levels: Jan &lt; Feb &lt; Mar &lt; Apr &lt; May &lt; Jun &lt; Jul &lt; Aug &lt; Sep &lt; ... &lt; Dec But this is the abbreviated name and not the full name. How do we get the full name of the month? Set the abbr argument to FALSE. month(release_date, label = TRUE, abbr = FALSE) ## [1] December ## 12 Levels: January &lt; February &lt; March &lt; April &lt; May &lt; June &lt; ... &lt; December Ah! now we can see the full name of the month. months() from base R will return the full name of the month by default. If you want the abbreviated name, use the abbreviate argument and set it to TRUE. months(release_date) ## [1] &quot;December&quot; 9.8.3 Week week() returns the number of complete 7 day periods between the date and 1st January plus one. week(release_date) ## [1] 50 9.8.4 Day Use day() to extract the date component. There are other variations such as Function Description day Get day mday() Day of the month wday() Day of the week qday() Day of quarter yday() Day of year weekdays() Day of week days_in_month() Days in the month day(release_date) ## [1] 12 mday(release_date) ## [1] 12 qday(release_date) ## [1] 73 yday(release_date) ## [1] 346 wday can return a number abbreviation of the weekday full name of the weekday wday(release_date) ## [1] 5 wday(release_date, label = TRUE) ## [1] Thu ## Levels: Sun &lt; Mon &lt; Tue &lt; Wed &lt; Thu &lt; Fri &lt; Sat wday(release_date, label = TRUE, abbr = FALSE) ## [1] Thursday ## 7 Levels: Sunday &lt; Monday &lt; Tuesday &lt; Wednesday &lt; Thursday &lt; ... &lt; Saturday weekdays() from base R also returns the day of the week (the name and not the number). If you want the abbreviated name, use the abbreviate argument. weekdays(release_date) ## [1] &quot;Thursday&quot; weekdays(release_date, abbreviate = TRUE) ## [1] &quot;Thu&quot; 9.8.5 Days in Month If you want to know the number of days in the month, use days_in_month(). In our example, the month is December and it has 31 days. days_in_month(release_date) ## Dec ## 31 9.8.6 Hour, Minute &amp; Seconds Function Description hour() Get hour minute() Get minute second() Get second seconds() Number of seconds since 1970-01-01 So far we have been looking at date components. Now, let us look at time components. hour(release_date) ## [1] 8 minute(release_date) ## [1] 5 second(release_date) ## [1] 3 seconds() returns the number of seconds since 1970-01-01. seconds(release_date) ## [1] &quot;1576137903S&quot; 9.8.7 Quarter &amp; Semester quarter() will return the quarter from the date. December is in the 4th quarter and hence it returns 4. quarter(release_date) ## [1] 4 If you want the year along with the quarter, set the with_year argument to TRUE. quarter(release_date, with_year = TRUE) ## [1] 2019.4 In India, the fiscal starts in April and December falls in the 3rd quarter. How can we accommodate this change? The fiscal_start argument allows us to set the month in which the fiscal begins. We will set it to 4 for April. Now it returns 3 instead of 4. quarter(release_date, fiscal_start = 4) ## [1] 3 quarters() from base R also returns the quarter. quarters(release_date) ## [1] &quot;Q4&quot; Function Description quarter() Get quarter quarter(with_year = TRUE) Quarter with year quarter(fiscal_start = 4) Fiscal starts in April quarters() Get quarter semester() Get semester 9.8.8 Case Study 9.8.8.1 Extract Date, Month &amp; Year from Due Date Let us now extract the date, month and year from the Due column. transact %&gt;% mutate( due_day = day(Due), due_month = month(Due), due_year = year(Due) ) ## # A tibble: 2,466 x 6 ## Invoice Due Payment due_day due_month due_year ## &lt;date&gt; &lt;date&gt; &lt;date&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2013-01-02 2013-02-01 2013-01-15 1 2 2013 ## 2 2013-01-26 2013-02-25 2013-03-03 25 2 2013 ## 3 2013-07-03 2013-08-02 2013-07-08 2 8 2013 ## 4 2013-02-10 2013-03-12 2013-03-17 12 3 2013 ## 5 2012-10-25 2012-11-24 2012-11-28 24 11 2012 ## 6 2012-01-27 2012-02-26 2012-02-22 26 2 2012 ## 7 2013-08-13 2013-09-12 2013-09-09 12 9 2013 ## 8 2012-12-16 2013-01-15 2013-01-12 15 1 2013 ## 9 2012-05-14 2012-06-13 2012-07-01 13 6 2012 ## 10 2013-07-01 2013-07-31 2013-07-26 31 7 2013 ## # ... with 2,456 more rows 9.8.8.2 Data Sanitization Let us do some data sanitization. If the due day happens to be February 29, let us ensure that the due year is a leap year. Below are the steps to check if the due year is a leap year: we will extract the following from the due date: day month year we will then create a new column is_leap which will have be set to TRUE if the year is a leap year else it will be set to FALSE filter all the payments due on 29th Feb select the following columns: Due is_leap transact %&gt;% mutate( due_day = day(Due), due_month = month(Due), due_year = year(Due), is_leap = leap_year(due_year) ) %&gt;% filter(due_month == 2 &amp; due_day == 29) %&gt;% select(Due, is_leap) ## # A tibble: 4 x 2 ## Due is_leap ## &lt;date&gt; &lt;lgl&gt; ## 1 2012-02-29 TRUE ## 2 2012-02-29 TRUE ## 3 2012-02-29 TRUE ## 4 2012-02-29 TRUE 9.8.8.3 Invoices Distribution by Quarter Let us count the invoices due for each quarter. transact %&gt;% mutate( quarter_due = quarter(Due) ) %&gt;% count(quarter_due) ## # A tibble: 4 x 2 ## quarter_due n ## * &lt;int&gt; &lt;int&gt; ## 1 1 521 ## 2 2 661 ## 3 3 618 ## 4 4 666 9.8.9 Your Turn Get the R release dates using r_versions() from the rversions package and tabulate the following year month with label weekday with label hour and quarter 9.9 Create, Update &amp; Verify In the second section, we learnt to create date-time objects using as.Date(), as.POSIXct() etc. In this section, we will explore a few other functions that will allow us to do the same make_date() make_datetime() 9.9.1 Create To create date without time components, use make_date() and specify the following: year month date We need to specify all the components in numbers i.e. we cannot use Jul or July for the month. It has to be 7. make_date(year = 2019, month = 12, day = 12) ## [1] &quot;2019-12-12&quot; When you need to include time components, use make_datetime(). make_datetime(year = 2019, month = 12, day = 12, hour = 08, min = 05, sec = 03, tz = &quot;UTC&quot;) ## [1] &quot;2019-12-12 08:05:03 UTC&quot; 9.9.2 Update Let us look at another scenario. You have a date-time object and want to change one of its components i.e. any of the following year month date Instead of creating another date-time object, you can change any of the components using update(). In the below example, we will start with the date of release of R version 3.6.1 and using update(), we will change it to 2019-12-12. prev_release &lt;- ymd(&quot;2019-07-05&quot;) prev_release %&gt;% update(year = 2019, month = 12, mday = 12) ## [1] &quot;2019-12-12&quot; 9.9.3 Date Sequence So far we have created a single date-time instance. How about creating a sequence of dates? We can do that using seq.Date(). We need to specify the from date as the bare minimum input. If the end date is not specified, it will create the sequence uptil the current date. The interval of the sequence can be specified in any of the following units: day week month quarter year We can add the following to the interval units integer + / - (increment or decrement) Using the integer, we can specify multiples of the units mentioned and using the sign, we can specify whether to increment or decrement. The below table displays the main arguments used in seq.Date(): Function Description from Starting date of the sequence by End date of the sequence to Date increment of the sequence length.out Length of the sequence along.with Use length of this value as length of sequence In the first example, we will create a sequence of dates from 2010-01-01 to 2019-12-31. The unit of increment should be a year i.e. the difference between the dates in the sequence should be 1 year, specified using the by argument. seq.Date(from = as.Date(&quot;2010-01-01&quot;), to = as.Date(&quot;2019-12-31&quot;), by = &quot;year&quot;) ## [1] &quot;2010-01-01&quot; &quot;2011-01-01&quot; &quot;2012-01-01&quot; &quot;2013-01-01&quot; &quot;2014-01-01&quot; ## [6] &quot;2015-01-01&quot; &quot;2016-01-01&quot; &quot;2017-01-01&quot; &quot;2018-01-01&quot; &quot;2019-01-01&quot; In the next example, we change the unit of increment to a quarter i.e. the difference between the dates in the sequence should be a quarter or 3 months. seq.Date(from = as.Date(&quot;2009-12-12&quot;), to = as.Date(&quot;2019-12-12&quot;), by = &quot;quarter&quot;) ## [1] &quot;2009-12-12&quot; &quot;2010-03-12&quot; &quot;2010-06-12&quot; &quot;2010-09-12&quot; &quot;2010-12-12&quot; ## [6] &quot;2011-03-12&quot; &quot;2011-06-12&quot; &quot;2011-09-12&quot; &quot;2011-12-12&quot; &quot;2012-03-12&quot; ## [11] &quot;2012-06-12&quot; &quot;2012-09-12&quot; &quot;2012-12-12&quot; &quot;2013-03-12&quot; &quot;2013-06-12&quot; ## [16] &quot;2013-09-12&quot; &quot;2013-12-12&quot; &quot;2014-03-12&quot; &quot;2014-06-12&quot; &quot;2014-09-12&quot; ## [21] &quot;2014-12-12&quot; &quot;2015-03-12&quot; &quot;2015-06-12&quot; &quot;2015-09-12&quot; &quot;2015-12-12&quot; ## [26] &quot;2016-03-12&quot; &quot;2016-06-12&quot; &quot;2016-09-12&quot; &quot;2016-12-12&quot; &quot;2017-03-12&quot; ## [31] &quot;2017-06-12&quot; &quot;2017-09-12&quot; &quot;2017-12-12&quot; &quot;2018-03-12&quot; &quot;2018-06-12&quot; ## [36] &quot;2018-09-12&quot; &quot;2018-12-12&quot; &quot;2019-03-12&quot; &quot;2019-06-12&quot; &quot;2019-09-12&quot; ## [41] &quot;2019-12-12&quot; We will now create a sequence of dates but instead of specifying the unit of increment, we specify the number of dates in the sequence i.e. the length of the sequence. We do this using the length.out argument which specifies the desired length of the sequence. We want the sequence to have 10 dates including the start and end date, and hence we supply the value 10 for the length.out argument. seq.Date(from = as.Date(&quot;2010-01-01&quot;), to = as.Date(&quot;2019-12-31&quot;), length.out = 10) ## [1] &quot;2010-01-01&quot; &quot;2011-02-10&quot; &quot;2012-03-22&quot; &quot;2013-05-02&quot; &quot;2014-06-11&quot; ## [6] &quot;2015-07-22&quot; &quot;2016-08-31&quot; &quot;2017-10-10&quot; &quot;2018-11-20&quot; &quot;2019-12-31&quot; In all of the previous examples, we have specified both the start and the end date. Let us look at a few examples where we create a sequence of dates where we only specify the start date. In the below example, we want to create a sequence of dates starting from 2010-01-01. The unit of increment should be 1 year i.e. the difference between the dates in the sequence should be 1 year and the length of the sequence should be 10 i.e. the number of dates including the start date should be 10. seq.Date(from = as.Date(&quot;2010-01-01&quot;), by = &quot;year&quot;, length.out = 10) ## [1] &quot;2010-01-01&quot; &quot;2011-01-01&quot; &quot;2012-01-01&quot; &quot;2013-01-01&quot; &quot;2014-01-01&quot; ## [6] &quot;2015-01-01&quot; &quot;2016-01-01&quot; &quot;2017-01-01&quot; &quot;2018-01-01&quot; &quot;2019-01-01&quot; The unit of increment can include multiples and +/- sign i.e. it can be an unit of increment or decrement. In the next example, we can increment the dates in the sequence by 2 i.e. the difference between the dates should be 2 instead of 1. This is achieved by specifying the unit of increment (multiple) first followed by a space and then the unit. In our example, it is 2 year. As you can see, the sequence now goes all the way till 2028 and the gap between the dates is 2 years. seq.Date(from = as.Date(&quot;2010-01-01&quot;), by = &quot;2 year&quot;, length.out = 10) ## [1] &quot;2010-01-01&quot; &quot;2012-01-01&quot; &quot;2014-01-01&quot; &quot;2016-01-01&quot; &quot;2018-01-01&quot; ## [6] &quot;2020-01-01&quot; &quot;2022-01-01&quot; &quot;2024-01-01&quot; &quot;2026-01-01&quot; &quot;2028-01-01&quot; Let us say instead of increment we want to decrement the dates i.e. the sequence of dates will go backwards as shown in the next example. We achieve this by using the - sign along with the unit of decrement. The sequence of dates in next example starts from 2010 and goes back upto 1992 and the difference between the dates in 2 years. seq.Date(from = as.Date(&quot;2010-01-01&quot;), by = &quot;-2 year&quot;, length.out = 10) ## [1] &quot;2010-01-01&quot; &quot;2008-01-01&quot; &quot;2006-01-01&quot; &quot;2004-01-01&quot; &quot;2002-01-01&quot; ## [6] &quot;2000-01-01&quot; &quot;1998-01-01&quot; &quot;1996-01-01&quot; &quot;1994-01-01&quot; &quot;1992-01-01&quot; In the last example, we will explore the along.with argument. Here we have supplied a vector which is a sequence of numbers from 1 to 10. The length of this vector is 10 and the same length is used as the length of the sequence i.e. the length of value supplied to along.with is also the length of the sequence. seq.Date(from = as.Date(&quot;2010-01-01&quot;), by = &quot;-2 year&quot;, along.with = 1:10) ## [1] &quot;2010-01-01&quot; &quot;2008-01-01&quot; &quot;2006-01-01&quot; &quot;2004-01-01&quot; &quot;2002-01-01&quot; ## [6] &quot;2000-01-01&quot; &quot;1998-01-01&quot; &quot;1996-01-01&quot; &quot;1994-01-01&quot; &quot;1992-01-01&quot; 9.9.4 Verify Type How do you check if the data is a date-time object? You can do that using any of the following from the lubridate package. is.Date() is.POSIXct() is.POSIXlt() is.Date(release_date) ## [1] FALSE is.POSIXct(release_date) ## [1] TRUE is.POSIXlt(release_date) ## [1] FALSE 9.9.5 Your Turn R 2.0.0 was released on 2004-10-04 14:24:38. Create this date using both make_date() and make_datetime() R 3.0.0 was released on 2013-04-03 07:12:36. Update the date created in the previous step to the above using update() 9.10 Intervals, Duration &amp; Period In this chapter, we will learn about intervals duration and period 9.10.1 Interval An interval is a timespan defined by two date-times. Let us represent the length of the course using interval. course_start &lt;- as_date(&#39;2017-04-12&#39;) course_end &lt;- as_date(&#39;2017-04-21&#39;) interval(course_start, course_end) ## [1] 2017-04-12 UTC--2017-04-21 UTC If you observe carefully, the interval is represented by the course start and end dates. We will learn how to use intervals in the case study. 9.10.1.1 Overlapping Intervals Let us say you are planning a vacation and want to check if the vacation dates overlap with the course dates. You can do this by: creating vacation and course intervals use int_overlaps() to check if two intervals overlap. It returns TRUE if the intervals overlap else FALSE. Let us use the vacation start and end dates to create vacation_interval and then check if it overlaps with course_interval. vacation_start &lt;- as_date(&#39;2017-04-19&#39;) vacation_end &lt;- as_date(&#39;2017-04-25&#39;) course_interval &lt;- interval(course_start, course_end) vacation_interval &lt;- interval(vacation_start, vacation_end) int_overlaps(course_interval, vacation_interval) ## [1] TRUE 9.10.1.2 How many invoices were settled within due date? Let us use intervals to count the number of invoices that were settled within the due date. To do this, we will: create an interval for the invoice and due date create a new column due_next by incrementing the due date by 1 day another interval for due_next and the payment date if the intervals overlap, the payment was made within the due date transact %&gt;% mutate( inv_due_interval = interval(Invoice, Due), due_next = Due + days(1), due_pay_interval = interval(due_next, Payment), overlaps = int_overlaps(inv_due_interval, due_pay_interval) ) %&gt;% select(Invoice, Due, Payment, overlaps) ## # A tibble: 2,466 x 4 ## Invoice Due Payment overlaps ## &lt;date&gt; &lt;date&gt; &lt;date&gt; &lt;lgl&gt; ## 1 2013-01-02 2013-02-01 2013-01-15 TRUE ## 2 2013-01-26 2013-02-25 2013-03-03 FALSE ## 3 2013-07-03 2013-08-02 2013-07-08 TRUE ## 4 2013-02-10 2013-03-12 2013-03-17 FALSE ## 5 2012-10-25 2012-11-24 2012-11-28 FALSE ## 6 2012-01-27 2012-02-26 2012-02-22 TRUE ## 7 2013-08-13 2013-09-12 2013-09-09 TRUE ## 8 2012-12-16 2013-01-15 2013-01-12 TRUE ## 9 2012-05-14 2012-06-13 2012-07-01 FALSE ## 10 2013-07-01 2013-07-31 2013-07-26 TRUE ## # ... with 2,456 more rows Below we show another method to count the number of invoices paid within the due date. Instead of using days to change the due date, we use int_shift to shift it by 1 day. transact %&gt;% mutate( inv_due_interval = interval(Invoice, Due), due_pay_interval = interval(Due, Payment), due_pay_next = int_shift(due_pay_interval, by = days(1)), overlaps = int_overlaps(inv_due_interval, due_pay_next) ) %&gt;% select(Invoice, Due, Payment, overlaps) ## # A tibble: 2,466 x 4 ## Invoice Due Payment overlaps ## &lt;date&gt; &lt;date&gt; &lt;date&gt; &lt;lgl&gt; ## 1 2013-01-02 2013-02-01 2013-01-15 TRUE ## 2 2013-01-26 2013-02-25 2013-03-03 FALSE ## 3 2013-07-03 2013-08-02 2013-07-08 TRUE ## 4 2013-02-10 2013-03-12 2013-03-17 FALSE ## 5 2012-10-25 2012-11-24 2012-11-28 FALSE ## 6 2012-01-27 2012-02-26 2012-02-22 TRUE ## 7 2013-08-13 2013-09-12 2013-09-09 TRUE ## 8 2012-12-16 2013-01-15 2013-01-12 TRUE ## 9 2012-05-14 2012-06-13 2012-07-01 FALSE ## 10 2013-07-01 2013-07-31 2013-07-26 TRUE ## # ... with 2,456 more rows You might be thinking why we incremented the due date by a day before creating the interval between the due day and the payment day. If we do not increment, both the intervals will share a common date i.e. the due date and they will always overlap as shown below: transact %&gt;% mutate( inv_due_interval = interval(Invoice, Due), due_pay_interval = interval(Due, Payment), overlaps = int_overlaps(inv_due_interval, due_pay_interval) ) %&gt;% select(Invoice, Due, Payment, overlaps) ## # A tibble: 2,466 x 4 ## Invoice Due Payment overlaps ## &lt;date&gt; &lt;date&gt; &lt;date&gt; &lt;lgl&gt; ## 1 2013-01-02 2013-02-01 2013-01-15 TRUE ## 2 2013-01-26 2013-02-25 2013-03-03 TRUE ## 3 2013-07-03 2013-08-02 2013-07-08 TRUE ## 4 2013-02-10 2013-03-12 2013-03-17 TRUE ## 5 2012-10-25 2012-11-24 2012-11-28 TRUE ## 6 2012-01-27 2012-02-26 2012-02-22 TRUE ## 7 2013-08-13 2013-09-12 2013-09-09 TRUE ## 8 2012-12-16 2013-01-15 2013-01-12 TRUE ## 9 2012-05-14 2012-06-13 2012-07-01 TRUE ## 10 2013-07-01 2013-07-31 2013-07-26 TRUE ## # ... with 2,456 more rows 9.10.1.3 Shift Interval Intervals can be shifted too. In the below example, we shift the course interval by: 1 day 3 weeks 1 year course_interval &lt;- interval(course_start, course_end) # shift course_interval by 1 day int_shift(course_interval, by = days(1)) ## [1] 2017-04-13 UTC--2017-04-22 UTC # shift course_interval by 3 weeks int_shift(course_interval, by = weeks(3)) ## [1] 2017-05-03 UTC--2017-05-12 UTC # shift course_interval by 1 year int_shift(course_interval, by = years(1)) ## [1] 2018-04-12 UTC--2018-04-21 UTC 9.10.2 Within Let us assume that we have to attend a conference in April 2017. Does it occur during the course duration? We can answer this using %within% which will return TRUE if a date falls within an interval. conference &lt;- as_date(&#39;2017-04-15&#39;) conference %within% course_interval ## [1] TRUE 9.10.2.1 How many invoices were settled within due date? Let us use %within% to count the number of invoices that were settled within the due date. We will do this by: creating an interval for the invoice and due date check if the payment date falls within the above interval transact %&gt;% mutate( inv_due_interval = interval(Invoice, Due), overlaps = Payment %within% inv_due_interval ) %&gt;% select(Due, Payment, overlaps) ## # A tibble: 2,466 x 3 ## Due Payment overlaps ## &lt;date&gt; &lt;date&gt; &lt;lgl&gt; ## 1 2013-02-01 2013-01-15 TRUE ## 2 2013-02-25 2013-03-03 FALSE ## 3 2013-08-02 2013-07-08 TRUE ## 4 2013-03-12 2013-03-17 FALSE ## 5 2012-11-24 2012-11-28 FALSE ## 6 2012-02-26 2012-02-22 TRUE ## 7 2013-09-12 2013-09-09 TRUE ## 8 2013-01-15 2013-01-12 TRUE ## 9 2012-06-13 2012-07-01 FALSE ## 10 2013-07-31 2013-07-26 TRUE ## # ... with 2,456 more rows 9.10.3 Duration Duration is timespan measured in seconds. To create a duration object, use duration(). The timespan can be anything from seconds to years but it will be represented as seconds. Let us begin by creating a duration object where the timespan is in seconds. duration(50, &quot;seconds&quot;) ## [1] &quot;50s&quot; Another way to specify the above timespan is shown below: duration(second = 50) ## [1] &quot;50s&quot; As you can see, the output is same in both the cases. Let us increase the timespan to 60 seconds and see what happens. duration(second = 60) ## [1] &quot;60s (~1 minutes)&quot; Although the timespan is primarily measured in seconds, it also shows ~1 minutes in the brackets. As the length of the timespan increases i.e. the number becomes large, it is represented using larger units such as hours and days. In the below examples, as the number of seconds increases, you can observe larger units being used to represent the timespan. # minutes duration(minute = 50) ## [1] &quot;3000s (~50 minutes)&quot; duration(minute = 60) ## [1] &quot;3600s (~1 hours)&quot; # hours duration(hour = 23) ## [1] &quot;82800s (~23 hours)&quot; duration(hour = 24) ## [1] &quot;86400s (~1 days)&quot; The following helper functions can be used to create duration objects as well. # default dseconds() ## [1] &quot;1s&quot; dminutes() ## [1] &quot;60s (~1 minutes)&quot; # seconds duration(second = 59) ## [1] &quot;59s&quot; dseconds(59) ## [1] &quot;59s&quot; # minutes duration(minute = 50) ## [1] &quot;3000s (~50 minutes)&quot; dminutes(50) ## [1] &quot;3000s (~50 minutes)&quot; # hours duration(hour = 36) ## [1] &quot;129600s (~1.5 days)&quot; dhours(36) ## [1] &quot;129600s (~1.5 days)&quot; # weeks duration(week = 56) ## [1] &quot;33868800s (~1.07 years)&quot; dweeks(56) ## [1] &quot;33868800s (~1.07 years)&quot; Let us use the above helper functions to get the course length in different units. # course length in seconds course_interval / dseconds() ## [1] 777600 # course length in minutes course_interval / dminutes() ## [1] 12960 # course length in hours course_interval / dhours() ## [1] 216 # course length in weeks course_interval / dweeks() ## [1] 1.285714 # course length in years course_interval / dyears() ## [1] 0.02464066 9.10.4 Period A period is a timespan defined in units such as years, months, and days. In the below examples, we use period() to represent timespan using different units. # second period(5, &quot;second&quot;) ## [1] &quot;5S&quot; period(second = 5) ## [1] &quot;5S&quot; # minute &amp; second period(c(3, 5), c(&quot;minute&quot;, &quot;second&quot;)) ## [1] &quot;3M 5S&quot; period(minute = 3, second = 5) ## [1] &quot;3M 5S&quot; # hour, minte &amp; second period(c(1, 3, 5), c(&quot;hour&quot;, &quot;minute&quot;, &quot;second&quot;)) ## [1] &quot;1H 3M 5S&quot; period(hour = 1, minute = 3, second = 5) ## [1] &quot;1H 3M 5S&quot; # day, hour, minute &amp; second period(c(3, 1, 3, 5), c(&quot;day&quot;, &quot;hour&quot;, &quot;minute&quot;, &quot;second&quot;)) ## [1] &quot;3d 1H 3M 5S&quot; period(day = 3, hour = 1, minute = 3, second = 5) ## [1] &quot;3d 1H 3M 5S&quot; Let us get the course length in different units using as.period(). # course length in second as.period(course_interval, unit = &quot;seconds&quot;) ## [1] &quot;777600S&quot; # course length in hours and minutes as.period(course_interval, unit = &quot;minutes&quot;) ## [1] &quot;12960M 0S&quot; # course length in hours, minutes and seconds as.period(course_interval, unit = &quot;hours&quot;) ## [1] &quot;216H 0M 0S&quot; time_length() computes the exact length of a timespan i.e. duration, interval or period. Let us use time_length() to compute the length of the course in different units. # course length in seconds time_length(course_interval, unit = &quot;seconds&quot;) ## [1] 777600 # course length in minutes time_length(course_interval, unit = &quot;minutes&quot;) ## [1] 12960 # course length in hours time_length(course_interval, unit = &quot;hours&quot;) ## [1] 216 9.11 Others In this section, we will learn to round date/time to the nearest unit and roll back dates. 9.11.1 Rounding Dates We will explore functions for rounding dates to the nearest value using round_dates() down using floor_date() up using ceiling_date() The unit for rounding can be any of the following: second minute hour day week month bimonth quarter season halfyear and year We will look at a few examples using round_date() and you will then practice using the other two functions. # minute round_date(release_date, unit = &quot;minute&quot;) ## [1] &quot;2019-12-12 08:05:00 UTC&quot; round_date(release_date, unit = &quot;mins&quot;) ## [1] &quot;2019-12-12 08:05:00 UTC&quot; round_date(release_date, unit = &quot;5 mins&quot;) ## [1] &quot;2019-12-12 08:05:00 UTC&quot; # hour round_date(release_date, unit = &quot;hour&quot;) ## [1] &quot;2019-12-12 08:00:00 UTC&quot; # day round_date(release_date, unit = &quot;day&quot;) ## [1] &quot;2019-12-12 UTC&quot; 9.11.2 Rollback Use rollback() if you want to change the date to the last day of the previous month or the first day of the month. rollback(release_date) ## [1] &quot;2019-11-30 08:05:03 UTC&quot; To change the date to the first day of the month, use the roll_to_first argument and set it to TRUE. rollback(release_date, roll_to_first = TRUE) ## [1] &quot;2019-12-01 08:05:03 UTC&quot; 9.11.3 Your Turn round up R release dates to hours round down R release dates to minutes rollback R release dates to the beginning of the month "],
["forcats.html", "Chapter 10 Categorical Data Analysis 10.1 Introduction 10.2 Case Study 10.3 Tabulate Referrers 10.4 Reorder Referrers 10.5 Plot Referrer Frequency (Descending Order) 10.6 Plot Referrer Frequency (Ascending Order) 10.7 Case Study 2 10.8 Tabulate Referrer 10.9 Collapse Referrer Categories 10.10 Lump Infrequent Referrer Types 10.11 Retain top 3 referrers 10.12 Lump Referrer Types with less than 10% traffic 10.13 Retain 3 Referrer Types with lowest traffic 10.14 Retain 3 Referrer Types with less than 10% traffic 10.15 Replace Levels 10.16 Drop Levels 10.17 Reorder Levels 10.18 Case Study 3 10.19 Shift Levels", " Chapter 10 Categorical Data Analysis 10.1 Introduction In this chapter, we will learn to work with categorical/qualitative data in R using forcats. We will use the following R packages: library(forcats) library(tibble) library(magrittr) library(purrr) library(dplyr) library(ggplot2) library(readr) 10.2 Case Study We will use a case study to explore the various features of the forcats package. You can download the data for the case study from here or directly import the data using the readr package. We will do the following in this case study: compute the frequency of different referrers plot average number of pages browsed for different referrers collapse referrers with low sample size into a single group club traffic from social media websites into a new category group referrers with traffic below a threshold into a single category 10.2.1 Data ecom &lt;- read_csv(&#39;https://raw.githubusercontent.com/rsquaredacademy/datasets/master/web.csv&#39;, col_types = cols_only( referrer = col_factor(levels = c(&quot;bing&quot;, &quot;direct&quot;, &quot;social&quot;, &quot;yahoo&quot;, &quot;google&quot;)), n_pages = col_double(), duration = col_double() ) ) ecom ## # A tibble: 1,000 x 3 ## referrer n_pages duration ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 google 1 693 ## 2 yahoo 1 459 ## 3 direct 1 996 ## 4 bing 18 468 ## 5 yahoo 1 955 ## 6 yahoo 5 135 ## 7 yahoo 1 75 ## 8 direct 1 908 ## 9 bing 19 209 ## 10 google 1 208 ## # ... with 990 more rows Let us extract the referrer column from the above data using use_series and save it in a new variable referrers. Instead of using ecom which is a tibble, we will use referrers which is a vector. We do this to avoid extracting the referrer column from the above data in later examples. referrers &lt;- use_series(ecom, referrer) 10.3 Tabulate Referrers Let us look at the traffic driven by different referrer types. fct_count(referrers) ## # A tibble: 5 x 2 ## f n ## &lt;fct&gt; &lt;int&gt; ## 1 bing 194 ## 2 direct 191 ## 3 social 200 ## 4 yahoo 207 ## 5 google 208 If you want to sort the output in descending order, use sort and set it to TRUE. fct_count(referrers, sort = TRUE) ## # A tibble: 5 x 2 ## f n ## &lt;fct&gt; &lt;int&gt; ## 1 google 208 ## 2 yahoo 207 ## 3 social 200 ## 4 bing 194 ## 5 direct 191 Use fct_unique to view the categories or levels of the referrer variable. fct_unique(referrers) ## [1] bing direct social yahoo google ## Levels: bing direct social yahoo google 10.4 Reorder Referrers We want to examine the average number of pages visited by each referrer type. refer_summary &lt;- ecom %&gt;% group_by(referrer) %&gt;% summarise( page = mean(n_pages), tos = mean(duration), n = n() ) ## `summarise()` ungrouping (override with `.groups` argument) refer_summary ## # A tibble: 5 x 4 ## referrer page tos n ## * &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 bing 6.13 368. 194 ## 2 direct 6.38 358. 191 ## 3 social 5.42 355. 200 ## 4 yahoo 5.99 336. 207 ## 5 google 5.73 360. 208 Let us plot the average number of pages visited by each referrer type. refer_summary %&gt;% ggplot() + geom_point(aes(page, referrer)) Use fct_reorder to reorder the referrer types by the average number of pages visited. refer_summary %&gt;% ggplot() + geom_point(aes(page, fct_reorder(referrer, page))) 10.5 Plot Referrer Frequency (Descending Order) Since we want to plot the referrers in descending order of frequency, we will use fct_infreq() to reorder by frequency. referrers %&gt;% fct_infreq() %&gt;% fct_unique() ## [1] google yahoo social bing direct ## Levels: google yahoo social bing direct Now that we know how to reorder categories/levels by frequency, let us reorder the referrers by frequency and plot them. ecom %&gt;% mutate( ref = referrer %&gt;% fct_infreq() ) %&gt;% ggplot(aes(ref)) + geom_bar() 10.6 Plot Referrer Frequency (Ascending Order) Let us look at the categories of the referrer variable. fct_unique(referrers) ## [1] bing direct social yahoo google ## Levels: bing direct social yahoo google Since we want to plot the referrers in ascending order of frequency, we will use fct_rev() to reverse the order. referrers %&gt;% fct_rev() %&gt;% fct_unique() ## [1] google yahoo social direct bing ## Levels: google yahoo social direct bing Let us reorder the referrers by frequency first and then reverse the order before plotting their frequencies. ecom %&gt;% mutate( ref = referrer %&gt;% fct_infreq() %&gt;% fct_rev() ) %&gt;% ggplot(aes(ref)) + geom_bar() 10.7 Case Study 2 In this case study, we will learn to: combine categories recategorize The data set we will use has just one column traffics i.e. the source of traffic for a imaginary website. 10.7.1 Data traffic &lt;- read_csv(&#39;https://raw.githubusercontent.com/rsquaredacademy/datasets/master/web_traffic.csv&#39;, col_types = list( col_factor(levels = c(&quot;affiliates&quot;, &quot;bing&quot;, &quot;direct&quot;, &quot;facebook&quot;, &quot;yahoo&quot;, &quot;google&quot;, &quot;instagram&quot;, &quot;twitter&quot;, &quot;unknown&quot;) ) ) ) traffic ## # A tibble: 48,232 x 1 ## traffics ## &lt;fct&gt; ## 1 google ## 2 google ## 3 google ## 4 google ## 5 google ## 6 google ## 7 google ## 8 google ## 9 google ## 10 google ## # ... with 48,222 more rows Let us extract the traffics column from the above data using use_series and save it in a new variable traffics. Instead of using traffic which is a tibble, we will use traffics which is a vector. We do this to avoid extracting the traffics column from the above data in all the examples shown below. traffics &lt;- use_series(traffic, traffics) 10.8 Tabulate Referrer Let us compute the traffic driven by different referrers using fct_count. fct_count(traffics) ## # A tibble: 9 x 2 ## f n ## &lt;fct&gt; &lt;int&gt; ## 1 affiliates 7641 ## 2 bing 5893 ## 3 direct 1350 ## 4 facebook 8135 ## 5 yahoo 4899 ## 6 google 9229 ## 7 instagram 3907 ## 8 twitter 4521 ## 9 unknown 2657 10.9 Collapse Referrer Categories We want to group some of the referrers into 2 categories: social search To group categories/levels, we will use fct_collapse(). traffics %&gt;% fct_collapse( social = c(&quot;facebook&quot;, &quot;twitter&quot;, &quot;instagram&quot;), search = c(&quot;google&quot;, &quot;bing&quot;, &quot;yahoo&quot;) ) %&gt;% fct_count() ## # A tibble: 5 x 2 ## f n ## &lt;fct&gt; &lt;int&gt; ## 1 affiliates 7641 ## 2 search 20021 ## 3 direct 1350 ## 4 social 16563 ## 5 unknown 2657 The above result can be achieved using fct_recode() as shown below: fct_recode(traffics, search = &quot;bing&quot;, search = &quot;yahoo&quot;, search = &quot;google&quot;, social = &quot;facebook&quot;, social = &quot;twitter&quot;, social = &quot;instagram&quot;) %&gt;% levels() ## [1] &quot;affiliates&quot; &quot;search&quot; &quot;direct&quot; &quot;social&quot; &quot;unknown&quot; 10.10 Lump Infrequent Referrer Types Let us group together referrer types that drive low traffic to the website. Use fct_lump() to lump together categories. fct_count(traffics) ## # A tibble: 9 x 2 ## f n ## &lt;fct&gt; &lt;int&gt; ## 1 affiliates 7641 ## 2 bing 5893 ## 3 direct 1350 ## 4 facebook 8135 ## 5 yahoo 4899 ## 6 google 9229 ## 7 instagram 3907 ## 8 twitter 4521 ## 9 unknown 2657 traffics %&gt;% fct_lump() %&gt;% table() ## . ## affiliates bing facebook yahoo google instagram twitter ## 7641 5893 8135 4899 9229 3907 4521 ## unknown Other ## 2657 1350 10.11 Retain top 3 referrers We want to retain the top 3 referrers and combine the rest of them into a single category. ## # A tibble: 9 x 2 ## f n ## &lt;fct&gt; &lt;int&gt; ## 1 google 9229 ## 2 facebook 8135 ## 3 affiliates 7641 ## 4 bing 5893 ## 5 yahoo 4899 ## 6 twitter 4521 ## 7 instagram 3907 ## 8 unknown 2657 ## 9 direct 1350 Use fct_lump() and set the argument n to 3 indicating we want to retain top 3 categories and combine the rest. traffics %&gt;% fct_lump(n = 3) %&gt;% table() ## . ## affiliates facebook google Other ## 7641 8135 9229 23227 10.12 Lump Referrer Types with less than 10% traffic Let us combine referrers that drive less than 10% traffic to the website. ## # A tibble: 9 x 3 ## f n percent ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; ## 1 affiliates 7641 15.8 ## 2 bing 5893 12.2 ## 3 direct 1350 2.8 ## 4 facebook 8135 16.9 ## 5 yahoo 4899 10.2 ## 6 google 9229 19.1 ## 7 instagram 3907 8.1 ## 8 twitter 4521 9.37 ## 9 unknown 2657 5.51 Since we are looking at proportion of traffic driven to the website and not the actual numbers, we use the prop argument and set it to 0.1, indicating that we want to retain only those categories which have a proportion of more than 10% and combine the rest. traffics %&gt;% fct_lump(prop = 0.1) %&gt;% table() ## . ## affiliates bing facebook yahoo google Other ## 7641 5893 8135 4899 9229 12435 10.13 Retain 3 Referrer Types with lowest traffic What if we want to retain 3 referrers which drive the lowest traffic to the website and combine the rest? ## # A tibble: 9 x 2 ## f n ## &lt;fct&gt; &lt;int&gt; ## 1 direct 1350 ## 2 unknown 2657 ## 3 instagram 3907 ## 4 twitter 4521 ## 5 yahoo 4899 ## 6 bing 5893 ## 7 affiliates 7641 ## 8 facebook 8135 ## 9 google 9229 We will still use the n argument but instead of specifying 3, we now specify -3. traffics %&gt;% fct_lump(n = -3) %&gt;% table() ## . ## direct instagram unknown Other ## 1350 3907 2657 40318 10.14 Retain 3 Referrer Types with less than 10% traffic Let us see how to retain referrers that drive less than 10 % traffic to the website and combine the rest into a single group. ## # A tibble: 9 x 3 ## f n percent ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; ## 1 affiliates 7641 15.8 ## 2 bing 5893 12.2 ## 3 direct 1350 2.8 ## 4 facebook 8135 16.9 ## 5 yahoo 4899 10.2 ## 6 google 9229 19.1 ## 7 instagram 3907 8.1 ## 8 twitter 4521 9.37 ## 9 unknown 2657 5.51 Instead of setting prop to 0.1, we will set it to -0.1. traffics %&gt;% fct_lump(prop = -0.1) %&gt;% table() ## . ## direct instagram twitter unknown Other ## 1350 3907 4521 2657 35797 10.15 Replace Levels Let us assume we want to retain a couple of important categories and group the rest into a single category. In the below example, we retain google and yahoo while grouping the rest as others using fct_other(). fct_other(traffics, keep = c(&quot;google&quot;, &quot;yahoo&quot;)) %&gt;% levels() ## [1] &quot;yahoo&quot; &quot;google&quot; &quot;Other&quot; 10.16 Drop Levels What if you want to drop a couple of categories instead of grouping them? Use the drop argument in fct_other() and specify the categories to be dropped. In the below example, we drop the following referrer categories: instagram twitter fct_other(traffics, drop = c(&quot;instagram&quot;, &quot;twitter&quot;)) %&gt;% levels() ## [1] &quot;affiliates&quot; &quot;bing&quot; &quot;direct&quot; &quot;facebook&quot; &quot;yahoo&quot; ## [6] &quot;google&quot; &quot;unknown&quot; &quot;Other&quot; 10.17 Reorder Levels The categories can be reordered using fct_relevel(). In the above example, we reorder the categories to ensure google appears first. Similarly in the below example, we reorder the levels to ensure twitter appears first irrespective of its frequency or order of appearance in the data. fct_relevel(traffics, &quot;twitter&quot;) %&gt;% levels() ## [1] &quot;twitter&quot; &quot;affiliates&quot; &quot;bing&quot; &quot;direct&quot; &quot;facebook&quot; ## [6] &quot;yahoo&quot; &quot;google&quot; &quot;instagram&quot; &quot;unknown&quot; If the category needs to appear at a particular position, use the after argument and specify the position after which it should appear. For example, if google should be the third category, we would specify after = 2 i.e. google should come after the 2nd position (i.e. third position). fct_relevel(traffics, &quot;google&quot;, after = 2) %&gt;% levels() ## [1] &quot;affiliates&quot; &quot;bing&quot; &quot;google&quot; &quot;direct&quot; &quot;facebook&quot; ## [6] &quot;yahoo&quot; &quot;instagram&quot; &quot;twitter&quot; &quot;unknown&quot; If the category should appear last, supply the value Inf (infinity) to the after argument as shown below. fct_relevel(traffics, &quot;facebook&quot;, after = Inf) %&gt;% levels() ## [1] &quot;affiliates&quot; &quot;bing&quot; &quot;direct&quot; &quot;yahoo&quot; &quot;google&quot; ## [6] &quot;instagram&quot; &quot;twitter&quot; &quot;unknown&quot; &quot;facebook&quot; 10.18 Case Study 3 In this case study, we deal with categorical data which is ordered and cyclical. It contains response to an imaginary survey. 10.18.1 Data response_data &lt;- read_csv(&#39;https://raw.githubusercontent.com/rsquaredacademy/datasets/master/response.csv&#39;, col_types = list(col_factor(levels = c(&quot;like&quot;, &quot;like somewhat&quot;, &quot;neutral&quot;, &quot;dislike somewhat&quot;, &quot;dislike&quot;), ordered = TRUE) ) ) Since we will be using only one column from the above data set, let us extract it using use_series() and save it as responses. responses &lt;- use_series(response_data, response) levels(responses) ## [1] &quot;like&quot; &quot;like somewhat&quot; &quot;neutral&quot; &quot;dislike somewhat&quot; ## [5] &quot;dislike&quot; 10.19 Shift Levels To shift the levels, we use fct_shift(). Use the n argument to indicate the direction of the shift. If n is positive, the levels are shifted to the left else to the right. In the below example, we shift the levels to the left by 2 positions. fct_shift(responses, 2) %&gt;% levels() ## [1] &quot;neutral&quot; &quot;dislike somewhat&quot; &quot;dislike&quot; &quot;like&quot; ## [5] &quot;like somewhat&quot; To shift the levels to the right, supply a negative value to the n argument in fct_shift(). In the below example, we shift the levels to the right by 2 positions. fct_shift(responses, -2) %&gt;% levels() ## [1] &quot;dislike somewhat&quot; &quot;dislike&quot; &quot;like&quot; &quot;like somewhat&quot; ## [5] &quot;neutral&quot; "]
]
